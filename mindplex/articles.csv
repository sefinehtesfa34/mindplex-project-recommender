timestamp,contentId,authorResidence,title,content,authorId,communtId,source
221,-6451309518266745024,SOUTH AFRICA,"Ethereum, a Virtual Currency, Enables Transactions That Rival Bitcoin's","All of this work is still very early. The first full public version of the Ethereum software was recently released, and the system could face some of the same technical and legal problems that have tarnished Bitcoin. Many Bitcoin advocates say Ethereum will face more security problems than Bitcoin because of the greater complexity of the software. Thus far, Ethereum has faced much less testing, and many fewer attacks, than Bitcoin. The novel design of Ethereum may also invite intense scrutiny by authorities given that potentially fraudulent contracts, like the Ponzi schemes, can be written directly into the Ethereum system. But the sophisticated capabilities of the system have made it fascinating to some executives in corporate America. IBM said last year that it was experimenting with Ethereum as a way to control real world objects in the so-called Internet of things. Microsoft has been working on several projects that make it easier to use Ethereum on its computing cloud, Azure. ""Ethereum is a general platform where you can solve problems in many industries using a fairly elegant solution - the most elegant solution we have seen to date,"" said Marley Gray, a director of business development and strategy at Microsoft. Mr. Gray is responsible for Microsoft's work with blockchains, the database concept that Bitcoin introduced. Blockchains are designed to store transactions and data without requiring any central authority or repository. Blockchain ledgers are generally maintained and updated by networks of computers working together - somewhat similar to the way that Wikipedia is updated and maintained by all its users. Many corporations, though, have created their own Ethereum networks with private blockchains, independent of the public system, and that could ultimately detract from the value of the individual unit in the Ethereum system - known as an Ether - that people have recently been buying. The interest in Ethereum is one sign of the corporate fascination with blockchains. Most major banks have expressed an interest in using them to make trading and money transfer faster and more efficient. On Tuesday, executives from the largest banks will gather for a conference, ""Blockchain: Tapping I nto the Real Potential , Cutting Through the Hype."" Many of these banks have recently been looking at how some version of Ethereum might be put to use. JPMorgan, for instance, has created a specific tool, Masala, that allows some of its internal databases to interact with an Ethereum blockchain. Michael Novogratz, a former top executive at the private equity firm Fortress Investing Group, who helped lead Fortress's investment in Bitcoin, has been looking at Ethereum since he left Fortress last fall. Mr. Novogratz said that he made a ""significant"" purchase of Ether in January. He has also heard how the financial industry's chatter about the virtual currency has evolved. ""A lot of the more established players were thinking, 'It's still an experiment,' "" he said. ""It feels like in the last two to three months that experiment is at least getting a lot more validation."" Since the beginning of the year, the value of an individual unit of Ether has soared as high as $12 from around $1. That has brought the value of all existing Ether to over $1 billion at times, significantly more than any virtual currency other than Bitcoin, which had over $6 billion in value outstanding last week. Since Bitcoin was invented, there have been many so-called alt-coins that have tried to improve on Bitcoin, but none have won the following of Ethereum. Unlike Bitcoin, which was released in 2009 by a mysterious creator known as Satoshi Nakamoto, Ethereum was created in a more transparent fashion by a 21-year-old Russian-Canadian, Vitalik Buterin, after he dropped out of Waterloo University in Ontario. The most basic aim of Ethereum was to make it possible to program binding agreements into the blockchain - the smart contract concept. Two people, for instance, could program a bet on a sports game directly into the Ethereum blockchain. Once the final score came in from a mutually agreed upon source - say, The Associated Press - the money would be automatically transferred to the winning party. Ether can be used as a currency in this system, but Ether are also necessary to pay for the network power needed to process the bet. The Ethereum system has sometimes been described as a single shared computer that is run by the network of users and on which resources are parceled out and paid for by Ether. A team of seven co-founders helped Mr. Buterin write up the software after he released the initial description of the system. Mr. Buterin's team raised $18 million in 2014 through a presale of Ether, which helped fund the Ethereum Foundation, which supports the software's development. Like Bitcoin, Ethereum has succeeded by attracting a dedicated network of followers who have helped support the software, partly in the hope that their Ether will increase in value if the system succeeds. Last week, there were 5,800 computers - or nodes - helping support the network around the world. The Bitcoin network had about 7,400 nodes. One of Mr. Buterin's co-founders, Joseph Lubin, has set up ConsenSys, a company based in Brooklyn that has hired over 50 developers to build applications on the Ethereum system, including one that enables music distribution and another that allows for a new kind of financial auditing. The ConsenSys offices are in an old industrial building in the Bushwick section of Brooklyn. The office is essentially one large room, with all the messy trademarks of a start-up operation, including white boards on the walls and computer parts lying around. Mr. Lubin said he had thrown himself into Ethereum after starting to think that it delivered on some of the failed promise of Bitcoin, especially when it came to allowing new kinds of online contracts and markets. ""Bitcoin presented the broad strokes vision, and Ethereum presented the crystallization of how to deliver that vision,"" he said. Joseph Bonneau, a computer science researcher at Stanford who studies so-called crypto-currencies, said Ethereum was the first system that had really caught his interest since Bitcoin. It is far from a sure thing, he cautioned. ""Bitcoin is still probably the safest bet, but Ethereum is certainly No. 2, and some folks will say it is more likely to be around in 10 years,"" Mr. Bonneau said. ""It will depend if any real markets develop around it. If there is some actual application.""",4340306774493623681,2782,Google
164,-4110354420726924665,AMERICA,"Ethereum, a Virtual Currency, Enables Transactions That Rival Bitcoin's","All of this work is still very early. The first full public version of the Ethereum software was recently released, and the system could face some of the same technical and legal problems that have tarnished Bitcoin. Many Bitcoin advocates say Ethereum will face more security problems than Bitcoin because of the greater complexity of the software. Thus far, Ethereum has faced much less testing, and many fewer attacks, than Bitcoin. The novel design of Ethereum may also invite intense scrutiny by authorities given that potentially fraudulent contracts, like the Ponzi schemes, can be written directly into the Ethereum system. But the sophisticated capabilities of the system have made it fascinating to some executives in corporate America. IBM said last year that it was experimenting with Ethereum as a way to control real world objects in the so-called Internet of things. Microsoft has been working on several projects that make it easier to use Ethereum on its computing cloud, Azure. ""Ethereum is a general platform where you can solve problems in many industries using a fairly elegant solution - the most elegant solution we have seen to date,"" said Marley Gray, a director of business development and strategy at Microsoft. Mr. Gray is responsible for Microsoft's work with blockchains, the database concept that Bitcoin introduced. Blockchains are designed to store transactions and data without requiring any central authority or repository. Blockchain ledgers are generally maintained and updated by networks of computers working together - somewhat similar to the way that Wikipedia is updated and maintained by all its users. Many corporations, though, have created their own Ethereum networks with private blockchains, independent of the public system, and that could ultimately detract from the value of the individual unit in the Ethereum system - known as an Ether - that people have recently been buying. The interest in Ethereum is one sign of the corporate fascination with blockchains. Most major banks have expressed an interest in using them to make trading and money transfer faster and more efficient. On Tuesday, executives from the largest banks will gather for a conference, ""Blockchain: Tapping I nto the Real Potential , Cutting Through the Hype."" Many of these banks have recently been looking at how some version of Ethereum might be put to use. JPMorgan, for instance, has created a specific tool, Masala, that allows some of its internal databases to interact with an Ethereum blockchain. Michael Novogratz, a former top executive at the private equity firm Fortress Investing Group, who helped lead Fortress's investment in Bitcoin, has been looking at Ethereum since he left Fortress last fall. Mr. Novogratz said that he made a ""significant"" purchase of Ether in January. He has also heard how the financial industry's chatter about the virtual currency has evolved. ""A lot of the more established players were thinking, 'It's still an experiment,' "" he said. ""It feels like in the last two to three months that experiment is at least getting a lot more validation."" Since the beginning of the year, the value of an individual unit of Ether has soared as high as $12 from around $1. That has brought the value of all existing Ether to over $1 billion at times, significantly more than any virtual currency other than Bitcoin, which had over $6 billion in value outstanding last week. Since Bitcoin was invented, there have been many so-called alt-coins that have tried to improve on Bitcoin, but none have won the following of Ethereum. Unlike Bitcoin, which was released in 2009 by a mysterious creator known as Satoshi Nakamoto, Ethereum was created in a more transparent fashion by a 21-year-old Russian-Canadian, Vitalik Buterin, after he dropped out of Waterloo University in Ontario. The most basic aim of Ethereum was to make it possible to program binding agreements into the blockchain - the smart contract concept. Two people, for instance, could program a bet on a sports game directly into the Ethereum blockchain. Once the final score came in from a mutually agreed upon source - say, The Associated Press - the money would be automatically transferred to the winning party. Ether can be used as a currency in this system, but Ether are also necessary to pay for the network power needed to process the bet. The Ethereum system has sometimes been described as a single shared computer that is run by the network of users and on which resources are parceled out and paid for by Ether. A team of seven co-founders helped Mr. Buterin write up the software after he released the initial description of the system. Mr. Buterin's team raised $18 million in 2014 through a presale of Ether, which helped fund the Ethereum Foundation, which supports the software's development. Like Bitcoin, Ethereum has succeeded by attracting a dedicated network of followers who have helped support the software, partly in the hope that their Ether will increase in value if the system succeeds. Last week, there were 5,800 computers - or nodes - helping support the network around the world. The Bitcoin network had about 7,400 nodes. One of Mr. Buterin's co-founders, Joseph Lubin, has set up ConsenSys, a company based in Brooklyn that has hired over 50 developers to build applications on the Ethereum system, including one that enables music distribution and another that allows for a new kind of financial auditing. The ConsenSys offices are in an old industrial building in the Bushwick section of Brooklyn. The office is essentially one large room, with all the messy trademarks of a start-up operation, including white boards on the walls and computer parts lying around. Mr. Lubin said he had thrown himself into Ethereum after starting to think that it delivered on some of the failed promise of Bitcoin, especially when it came to allowing new kinds of online contracts and markets. ""Bitcoin presented the broad strokes vision, and Ethereum presented the crystallization of how to deliver that vision,"" he said. Joseph Bonneau, a computer science researcher at Stanford who studies so-called crypto-currencies, said Ethereum was the first system that had really caught his interest since Bitcoin. It is far from a sure thing, he cautioned. ""Bitcoin is still probably the safest bet, but Ethereum is certainly No. 2, and some folks will say it is more likely to be around in 10 years,"" Mr. Bonneau said. ""It will depend if any real markets develop around it. If there is some actual application.""",4340306774493623681,1921,Google
198,-7292285110016212249,JAPAN,Bitcoin Future: When GBPcoin of Branson Wins Over USDcoin of Trump,"The alarm clock wakes me at 8:00 with stream of advert-free broadcasting, charged at one satoshi per second. The current BTC exchange rate makes that snooze button a costly proposition! So I get up, make coffee and go to my computer to check the overnight performance of my bots. TradeBot earns me on Trump and Branson TradeBot, which allocates funds between the main chain and various national currency side-chains, generated a lucrative 0.24 BTC return. TradeBot has been reliably profitable ever since I set it to trade USDcoin according to political prediction market data. As expected, the latest poll numbers came in as highly supportive of Trump's re-election as USDcoin CEO. Trump's resistance to de-anonymizing public spending, by moving USDcoin off the Confidential Transactions layer, continues to erode his coin's credibility. In his latest speech, Trump maintains that full CT-privacy is essential to ""combatting CNYcoin's sinister ring-signature scheming."" I make a note to increase my long position in GBPcoin. Following CEO Branson's memo to the effect that government finances and national banks be brought into compliance with the public blockchain , British corruption indices have flatlined. As the first national econmy to ""go light,"" Britain leads the global recovery from the Great Debt Default of '20. Happy with the GoatData Project I check TeachBot and note that it's performing in-line with expectations. TeachBot serves as an autonomous info-agent between various contracting AIs and data providers. The 0.5 BTC bounty it awarded to a team of Sherpas to outfit a herd of Tibetan mountain goats with full motion-sensing rigs has already been repaid...I check the latest figures... four times over! My best TeachBot strategy to date, the GoatData project provides valuable data to WinterHoof, the Artificial General Intelligence in charge of the Swiss military's quadripedal robotics program. At this rate, I'll soon have enough BTC to retire to Satoshi City on Mars!",4340306774493623681,2106,Instagram
106,-6151852268067518688,ENGLAND,Google Data Center 360° Tour,"We're excited to share the Google Data Center 360° Tour - a YouTube 360° video that gives you an unprecedented and immersive look inside one of our data centers. There are several ways to view this video: On desktop using Google Chrome use your mouse or trackpad to change your view while the video plays YouTube app on mobile - move your device around to look at all angles while the video plays And the most immersive way to view - using Google Cardboard (currently supported by the Android YouTube app only, iOS support is coming soon!) Load the video in the YouTube app and tap on the Cardboard icon when the video starts to play. Insert your phone in Cardboard and look around. A little background . . . Several months ago, those of us on the Google Cloud Developer Advocacy Team had a rare opportunity to tour the Google data center in The Dalles, Oregon. Many of us had seen other non-Google data centers in our careers, but this experience was beyond anything we ever imagined. We were blown away by the scale, the incredible attention to security and privacy, and the amazing efforts to make the data center extremely efficient and green. Additionally, we were proud to meet some of the brilliant people that design, build and maintain these data centers. If you are a Google Cloud Platform customer, then this is your data center as much as it is our data center, so we want you to experience what we experienced. We hope you enjoy it! - Posted by Greg Wilson, Head of Developer Advocacy, Google Cloud Platform",3891637997717104548,1471,Google
177,2448026894306402386,AMERICA,"IBM Wants to ""Evolve the Internet"" With Blockchain Technology","The Aite Group projects the blockchain market could be valued at $400 million by 2019. For that reason, some of the biggest names in banking, industry and technology have entered into the space to evaluate how this technology could change the financial world. IBM and Linux, for instance, have brought together some of the brightest minds in the industry and technology to work on blockchain technology through the Hyperledger Project. The Hyperledger Project is under the umbrella of the Linux Foundation, and seeks to incorporate findings by blockchain projects such as Blockstream, Ripple, Digital Asset Holdings and others in order to make blockchain technology useful for the world's biggest corporations. IBM has also contributed its own code to the project. According to John Wolpert, IBM's Global Blockchain Offering Director, when IBM and Linux began working together on the blockchain project, Linux made clear it wanted to ""disrupt the disruption,"" in part with their findings, as well as the data gathered by projects such as Ripple, Ethereum and others exploring the blockchain. The Linux foundation announced its Hyperledger project on December 17, 2015. Just one day later, 2,300 companies had requested to join. The second-largest open source foundation in the history of open source had only 450 inquiries. ""So, it's either going to be a holy mess or it's going to change the world,"" Wolpert said at The Blockchain Conference in San Francisco presented by Lighthouse Partners. As Wolford puts it, a team of IBMers is ""on a quest"" to understand and ""do something important"" with blockchain technology. ""I don't know why we got this rap in the '70s, way back, that we are not cool, that we're kind of stodgy, and that is not the IBM of my experience,"" Wolpert, who founded the taxi service Flywheel, explained. ""We're the original crazy people. The craziest people are the guys at IBM in the '60s and '70s -- you can imagine what they were doing in the '60s -- they are wild-eyed revolutionaries."" Although this is not the image IBM markets, their work in quantum computing, quantum teleportation and neuro semantic chips count among some of the ""cool stuff"" IBM does, says Wolpert. IBM also approaches projects in the spirit of open innovation, and not proprietarily. ""Our method of operations is open, and it's often our MO to back not-our-thing,"" he said of IBM since the 1990s. Wolpert cites Java as one such project: ""You would not know about Java today if it weren't for IBM."" As a ""pretty young dude,"" Wolpert was in the room when IBM made a billion-dollar decision to back Linux over its own technology. He also cites IBM's work on XML as an example of IBM's dedication to open innovation. Currently, IBM has employees working on crypto-security and distributed systems who have been working on consensus algorithms for their entire careers, some for more than 30 years. ""They're crazy smart, we're planetary, we've gone from a couple of guys in a canoe, to a platoon and approaching an army of people working on blockchain,"" Wolpert said. ""So it feels a lot like my first job at IBM which was making Java real for business."" This has led old and new friends to contact the the multinational technology and consulting corporation. ""Banks who have been calling us constantly ... saying 'What's your view on blockchain?' or 'Hey, let's do a project together on blockchain,'"" Wolpert said. ""We've been doing all these crazy projects on blockchain, every kind of blockchain, and learning a lot, been doing that for a couple years and really started to accelerate last year."" Today, there is a whole unit dedicated to blockchain technology at Linux Foundation. ""We went all-in on blockchain,"" he explained. What's it all about for Wolpert? ""It's about evolving the Internet."" Bitcoin is important for moving money around without a single authority. ""[Bitcoin] is a solution to a problem, a specific problem, of people who need to move money around in environments where you don't trust the government,"" Wolpert said. ""I think we can all agree we are approaching the end of the era where a single authority manages trust and gets compensated for the risk in doing so."" In Wolpert's view, finance does not need to go from today's system of trust -- where consumers trust institutions to handle their money -- to one of complete trustlessness, like the Bitcoin system in which a protocol, not a centralized authority, manages the movement of value. ""It doesn't follow, having one single trust authority to trustlessness on everything,"" he said. ""There is a false dichotomy between the notion of trust and trustlessness, that you have to have a walled garden on one side and Bitcoin on the other."" He doesn't compare Bitcoin to the Internet, saying that's a wrong analogy. ""It is not apt to say the Internet is like Bitcoin; the Internet is, from the perspective of Bitcoin, a permissioned walled garden,"" he said. ""Ever heard of ICANN? It's permissive, but it's permissioned."" Bitcoin, Ripple and Ethereum are ""Iteration 1"" of blockchain technology, the three time IBMer told his audience. Wolpert thinks blockchain technology will soon evolve to ""Iteration 2."" The problem with it? ""The Internet is constrained,"" Wolpert said. ""You have a fabric that allows for lots of competition on platforms and huge amounts of competition on solutions and applications on top of it, so we need to evolve the Internet to become economically aware, and that Internet isn't going to be an application, it is going to be a fabric and then lots of applications on top of that."" This doesn't mean blockchain 2.0 technologies such as Blockstream, Ethereum and Ripple are lost causes that must now completely retool. The Linux Foundation, in fact, works with Ethereum and Ripple employees. With its blockchain projects, IBM is not focused on moving a cryptocurrency. ""Lots of banks pay us lots of money and we like that, and we want to radically improve them,"" Wolpert explained. ""[Bankers] are gonna try like heck to radically improve what is going on ahead of the disruption, disrupt themselves if you will. If you look at things like [Overstock's] t0, that's happening, might happen on Bitcoin protocol, but might not, and there's plenty of ways to solve it, when you commit to it. And sure, Bitcoin has woken [banks] up to this, so it's doing its job, it's disrupting the thinking and maybe that is all that is necessary."" Wolpert wonders aloud during his speech if the world would have iTunes were it not for Napster, or Netflix if it weren't for BitTorrent. The former head of products for IBM's Watson Ecosystem says IBM currently has a ""boomtown with giant numbers of people at IBM and everywhere else"" investigating the blockchain problem and how distributed technology can be made useful for industry. He details how oftentimes IBM tinkers with open innovations itself to make the technology ready for industry. That's not the route being taken with Bitcoin. That the Bitcoin network is on par with Ireland for energy consumption, and that mining consortiums could eventually dictate that market, scares the firm away from dealing directly with Bitcoin. ""What I want to see, what we want to see, is a world full of companies who I can know, who I can identify, who are so spread out, like the Internet,"" he said. Wolpert cites the Arab Spring, and the role the Internet played in that uprising, as how a permissioned, yet permissive, structure functions. Websites, albeit technology permissioned by ICANN, remained online during the protests and played a major role. What's the risk in experimenting with blockchain? ""The risk is we go crazy, we have an unruly mess,"" Wolpert warned. ""[But] it's the Linux Foundation, we've been in this movie before. While there's a lot of really smart guys working on Bitcoin and startups, we've got equally amount of people who also have 20 or 30 years of experience with open source and open innovation, and I think we have a good shot at maintaining restraints, keeping it scoped down, keeping it simple, but getting it to something that is useful.""",4340306774493623681,1230,Google
161,-2826566343807132236,JAPAN,IEEE to Talk Blockchain at Cloud Computing Oxford-Con - CoinDesk,"One of the largest and oldest organizations for computing professionals will kick off its annual conference on the future of mobile cloud computing tomorrow, where blockchain is scheduled to be one of the attractions. With more than 421,000 members in 260 countries, the Institute of Electrical and Electronics Engineers (IEEE) holding such a high-profile event has the potential to accelerate the rate of blockchain adoption by the engineering community. At the four-day conference, beginning Tuesday, the IEEE will host five blockchain seminars at the 702-year-old Exeter College of Oxford. The conference, IEEE Mobile Cloud 2016, is the organization's fourth annual event dedicated to mobile cloud computing services and engineering. Speaking at the event, hosted at Oxford University, professor Wei-Tek Tsai of the School of Computing, Informatics and Decision Systems engineering at Arizona State University will talk about the future of blockchain technology as an academic topic of research. Computing shift While this looks to be the first IEEE conference to deal so closely with blockchain, its presence at an event dedicated to cloud computing is no surprise. 2016 is shaping up to be the year many stopped talking about bitcoin as a decentralized ledger and started talking about it as a database. Last month, IBM made a huge splash in the blockchain ecosystem when it announced among a wide range of other news, that it would be offering a wide range of blockchain-related initiatives. Further, in November, Microsoft coined the term Blockchain-as-a-Service (BaaS) to describe its sandbox environment where developers could experiment with tools hosted on the company's Azure Cloud platform. Companies including ConsenSys, Augur, BitShares and Slock.it have joined that effort. Oxford University via Shutterstock",4340306774493623681,1124,LinkedIn
106,-2148899391355011268,JAPAN,Banks Need To Collaborate With Bitcoin and Fintech Developers,"It will take time until banks come around to the idea of embracing Bitcoin or Fintech, though. Banks need to innovate at an accelerated pace, yet are unable to do so on their own. Allowing third-party developers to work together with the bank through API access would be a significant step in the right direction, as there is valuable input to be gathered from the Bitcoin and Fintech industries. Banks and other established financial players have not taken a liking to Fintech and digital currency just yet, as they see both industries as major competitors to their offerings. While it is certainly true Bitcoin and Fintech can bring significant improvements to the table, they should be seen as complementary allies who will bring success to the banking industry. Or that is what the Monetary Authority of Singapore seems to be thinking, at least. Also read: Bitcoin Price Technical Analysis For 03/28/2016 - Looking To Buy BTC? Embracing Bitcoin And Fintech As A Bank Is Necessary There is no denying the banking sector has seen very little to no real innovation for quite some time now, opening the door for other players to step in and offer something entirely different. In fact, a lot of financial experts see and as two major threats to the banking system, which would explain the vocal opposition to any solution that is not controlled by a bank or government. But the Monetary Authority of Singapore sees things different, as they feel both Fintech and Bitcoin are capable of complementing the current financial infrastructure, rather than be competing with banks. A combination of traditional banking with innovative technology and services has the potential to create a powerful and versatile financial ecosystem all over the globe. Fintech startups and entrepreneurs are renowned for thinking outside of the proverbial box when it comes to accessing financial services. Particularly where the money lending and peer-to-peer aspect is concerned, most Fintech companies have a leg up over their more traditional counterparts. But that does not mean these startups are looking to overthrow the banking system, although they might be able to in the long run. Any bank that is too slow - or simply unwilling - to innovate will be left behind in the long term, though. Collaborating with Fintech players is a far more preferable solution to this alternative, although it may involve swallowing some of the pride that are associated with the banking system. Fintech companies are, by design, more capable of embracing new and advanced technological features, which makes them invaluable allies for traditional banks. A Senior Regulator of the Monetary Authority of Singapore stated: ""MAS' approach to fintech is to use the power of technology to help banks to succeed. It's not a game of disruptors versus enablers. We see fintech as an enabler... The good news is fintech companies by design are creating far superior technology products, and where success lies is in partnering with banks, and enabling banks to succeed."" But Bitcoin has a role to play in the financial world as well, although this system is completely outside of the control of banks and governments. Despite those barriers, the popular digital currency is available in every country in the world, allowing for frictionless and real-time transfers of value, regardless of previous access to financial services by consumers and businesses. So far, most of the major banks around the world have taking a page out of the Bitcoin playbook, by trying to develop their own blockchain technology solutions. But that is not all, as several countries are considering to issue their own digital currency, rather than embracing Bitcoin. Whether or not that will be the right decision, in the end, remains to be seen. It will take time until banks come around to the idea of embracing Bitcoin or Fintech, though. Banks need to innovate at an accelerated pace, yet are unable to do so on their own. Allowing third-party developers to work together with the bank through API access would be a significant step in the right direction as there is valuable input to be gathered from the Bitcoin and Fintech industries.",4340306774493623681,1736,LinkedIn
241,4119190424078847945,ERITREA,Blockchain Technology Could Put Bank Auditors Out of Work,"When most people think about computers and robots taking jobs away from humans, the images that usually come to mind are robots moving inventory around in an Amazon warehouse or McDonald's customers placing their order via a tablet instead of a cashier . But the robots are coming for much more sophisticated jobs as well. For example, blockchain technology is out to eat the lunches of some professionals in the traditional financial system. There's a Lot of Mistrust in the Banking System At a recent blockchain-focused event in Toronto, Bitcoin Core contributor Peter Todd was asked to explain the reasoning behind Wall Street's increased interest in blockchain technology. During his initial response, Todd pointed out some of the mistrust that exists in the current financial system: ""The dirty secret is [the banks] don't actually trust [their databases]. I mean, they don't trust their own employees. ... They don't trust each other. There's so many levels of mistrust here."" Todd then discussed the massive industry built around financial audits. He noted: ""If they did trust all this stuff, why are there so many auditors? Why is there this massive infrastructure of labor-intensive human beings sitting there poring over transactions and trying to figure out where the money got created out of thin air. Where did the money disappear? Who moved what where? Was it all legit?"" Many financial institutions are interested in the concept of creating new systems for record-keeping, which would replace the current closed-ledger system with a more open alternative, similar to Bitcoin. Many believe this open system would enable more efficient and transparent auditing of financial activity. The Status Quo Is Doing All Right But It's Hard to Improve Todd also pointed out that financial institutions are already pretty good at what they do in terms of audits. He stated, ""For the most part, bank fraud is at tolerable levels, it seems."" Todd noted that maintaining a proper history of financial activity is one of the issues with increasing the speed of settlement . Because audits are labor intensive and require man hours to complete, it's difficult to essentially come to consensus on the correct version of events in a nearly instantaneous manner. He added, ""The faster money can move around, the faster you could lose it all due to some hacker."" How Does the Blockchain Help? Todd spoke on the perceived advantages of blockchains over the current way things work, which relies on placing trust in database admins and the people with the keys to the system. From this perspective, a blockchain simply looks like a strong audit log. Todd gave a specific example of how this technology can help: ""It could be something as simple as when I, as a bank employee, type something in, we really do want a cryptographic signature that's actually tied to my keycard or something. And that should go into a database. Well, what does that look like? It looks like a blockchain."" The longtime Bitcoin researcher also pointed out that this is sort of what banks were already looking at doing before blockchain technology started to receive a lot of attention. He explained: ""I think where they're thinking of going naturally looks like blockchains, so when they hear all this blockchain stuff it's like, 'Oh yeah. This is roughly what we were looking at doing anyway.'"" Replacing Humans Is the Point At one point during the recent event in Toronto, Todd was asked if the trend is that blockchains will eventually replace human auditors. Todd responded: ""All this blockchain stuff is really about: How good can we make the security to get to the point where we can imagine getting rid of human beings?"" Indeed, Todd's comments appear to fit well with Satoshi Nakamoto 's original Bitcoin white paper . In the paper, Nakamoto stated: ""What is needed is an electronic payment system based on cryptographic proof instead of trust...."" Going back further, cypherpunk Nick Szabo has written about the concept that third parties are security holes. In addition to improving security by cutting out trusted parties, financial institutions can cut costs by replacing human labor with computer code. Kyle Torpey is a freelance journalist who has been following Bitcoin since 2011. His work has been featured on VICE Motherboard, Business Insider, NASDAQ, RT's Keiser Report and many other media outlets. You can follow @kyletorpey on Twitter.",4340306774493623681,1638,LinkedIn
280,-7926018713416777892,ENGLAND,Why Decentralized Conglomerates Will Scale Better than Bitcoin - Interview with OpenLedger CEO - Bitcoin News,"Bitcoin.com spoke with the OpenLedger CEO, Ronny Boesing to get deeper insight of how decentralized conglomerates will challenge the status quo, why they will bring greater financial freedom to the public, and the disadvantages of Bitcoin when it comes to enterprise scaling compared to DC's. Also read: Deloitte: Blockchain Will 'Gain Significant Traction' by 2020 The Coming Age of 'Decentralized Conglomerates' As the OpenLedger team recently its Global Enterprise 3.0 program, its universal shared application called ""Decentralized Conglomerate"" (DC) is poised to revolutionize how we look at big business in the 21st century. For a long time, the traditional conglomerate model went unchallenged (e.g. Berkshire Hathaway, Philip Morris etc.) until the emergence of the Internet and increasing global interconnectivity. A new digital conglomerate structure began to emerge with the likes of Google (and its Alphabet Inc. parent company), which became more efficient at managing the production and distribution of their products. Nevertheless, the formal top-down structure of these new digital age conglomerates remained, including the the traditional way of how profits are distributed - that is, until now. ""Decentralized Conglomerates are the next logical step."" - Ronny Boesing Bitcoin.com (BC): Conglomerates first existed in physical form such as Berkshire Hathaway, after which we saw the advent of digital ones such as Google and Alphabet. Are decentralized autonomous conglomerates the next logical step in this evolution? Ronny Boesing (RB): Without question, Decentralized Conglomerates are the next logical step. It should be noted that there will be Autonomous and Semi-Autonomous organizations that emerge. The difference being the level of organizational influence exerted over the universal platform. In an Autonomous Conglomerate, the contracts and workers would be completely independent of having any external influence or association with another entity, but may simply be using the same platform as another organization. In a Semi-Autonomous Conglomerate, there will be some level of external influence, support, or association with an official business. This means that an organization has special interest in the associated operations of the Conglomerate, and some semblance of top-down influence may occur. BC: What are the basic advantages of decentralized conglomerate over a centralized one like Google, for example? RB: A Decentralized Conglomerate allows organizations to join the forces of their communities on a universal platform that allows cross promotion and profit sharing, but does not force it. This paradigm also allows individual brand identities to flourish within the Conglomerate without having to worry about the interests of the Universal Platform conflicting with the interests of any given brand using the platform. In a traditional or even a digital conglomerate, the mission of the parent company guides the decisions of the subsidiaries. BC: Is anyone free to join such an online conglomerate community? RB: The system is agnostic, so that anyone who wants to create an asset for their organization or commodity has the capacity to do so. All tokens do not have equal risk, and some are far less risky than others to purchase. BC: Some could argue that MLM pyramid structured companies are also ""profit sharing"" communities, though this entails a lot of risk as we all know. What kind of risk is there for people choosing to enter and buy the digital tokens of a decentralized conglomerate or its constituents? RB: The risk of purchasing such tokens depends on many things, such as the market capitalization of the DC, the number of organizations connected to the DC, the capacity to exchange tokens for other forms of currency or products, the size of the reserve backing a given token. All tokens do not have equal risk, and some are far less risky than others to purchase. Everyone should do their due diligence to know how solid the infrastructure is behind any given token or DC. BC: Can't Bitcoin be considered the world's first decentralized conglomerate? RB: Bitcoin is not a conglomerate. It is a decentralized ledger. The only purpose of Bitcoin's blockchain was to keep track of transactions. Many organizations expanded upon the original protocol to create colored coins and languages that permitted applications to be built on top of the Bitcoin blockchain. However, this was not the original intention of ""Bitcoin"" proper, and specifically talking about ""Bitcoin,"" it is apolitical, without special interest, and has no official organizational associations. [A decentralized conglomerate] was not the original intention of 'Bitcoin' proper, [...] it is apolitical, without special interest, and has no official organizational associations. Bitcoin is the world's first globally accepted currency that has no organizational associations, and that is simultaneously the best and worst aspect about it. Since there is no organization association, the problems that Bitcoin encounters rely on the community to come together and make a unilateral decision. As we have seen with the blocksize scaling issue , the transaction time debate , the recent DDoS attack, and the blockchain halt due to transaction costs; Bitcoin has some real problems that need to be addressed in order for it to be usable for enterprise scale organizations or countries alike. In that regard, since Bitcoin is apolitical, it doesn't really make logical sense for any country to switch to something which they have no control over as their main currency. So in that regard, Bitcoin is not only not a conglomerate, the apolitical aspect serves to keep organizations away from holding the currency during the fluctuations, as there is no real organization that has vested interest in keeping Bitcoin stable. BC: What exactly is OpenLedger's ""Bitcoin 3.0"" technology and what are Bitcoin's disadvantages compared to it when using it within the DC context? RB: As mentioned, Bitcoin has no associations. This means that the token can be purchased and amassed by any organization or country around the globe, leaving the token subject to wild swings due to margin trading. You see examples with organizations like Ether , Factom , or OpenLedger that have created a token that has organizational backing, and their respective tokens steadily increased in value instead of having wild unpredictable swings. Having an organization that has special interest in the Universal Ledger establishes a community and ecosystem which Bitcoin is lacking. It is the same type of culture that Apple has created using ""I"" branding that can be achieved with organizational associations. Bitcoin does not have this capacity. Having an organization that has special interest in the Universal Ledger establishes a community and ecosystem which Bitcoin is lacking. In order to get mass adoption or successful utilization, it is crucial to have a unified approach to the interface, and this becomes another major failing point of Bitcoin. Openledger has streamlined the UX/UI of digital currency transactions, and this represents a much more user-friendly approach with a smaller learning curve. Having development teams that can do research and development and then modify the UX based on user feedback is a great benefit that DC's have over Bitcoin. BC: Can you talk a little bit about the first partnerships between communities on this platform, namely OBITS and BitTeaser? RB: Every month, 70% of BitTeaser's monthly Bitcoin profits will be used to buyback BTSR and OBITS on the respective markets on OpenLedger. OBITS does it on the BTC (10%) and BTS (90%) market, and with BTSR it is done on BTS, BTC, USD and ETH markets 25% each. BTSR holders will receive 80% of this share, while OBITS holders will receive 20%. In the long term, BTSR holders may see an increase in the value per unit of their investment based on supply and demand, as well as having the tokens burned from every buyback thus reducing overall available supply. BitTeaser operates in a similar way to Google Adwords - revenue is generated based on providing advertising space for websites, companies and products. It currently serves over 1,000 webmasters, with monthly growth of around 15 to 20%. BC: Do they use smart-contracts to establish the terms? How is consensus achieved? RB: Yes, the system uses smart contracts to establish terms of service and reward. The consensus is achieved by third party confirmation that both sides have accomplished their respective tasks, whether it is putting money in escrow, or completing a contract request. BC: What was the approximate payout to the writers and bloggers using the platform? Will this amount increase as the overall platform grows? RB: At the last payment, some 11 BTC worth (currently around $4,500 USD) of BTSR and OBITS was sent to participating bloggers. That amount will increase as the network size and reach of the DC increases. OpenLedger has made it available for users to directly change tokens to USD, EUR, and CNY and have them directly deposited into their bank account. BC: How does the DC model interact with traditional fiat currency? Do users have to use an exchange to cash out their rewards? RB: ""In the same way that foreign currencies or bitcoin need to be exchanged before being used, any DC token will need to be changed to be used outside of the infrastructure. If an organization within the DC offers products or services that are desired, a direct purchase using a token could then be made without any exchange. Now, OpenLedger has made it available for users to directly change tokens to USD, EUR, and CNY and have them directly deposited into their bank account. No longer will a user have to have the hassle of going through an additional step of going to a bitcoin exchange to convert their smart money into fiat. Once a user has been validated he/she is prompted to attempt a withdrawal. Once the withdrawal request is made on OpenLedger for whatever currency is asked, the money is then sent to the user's bank account with a 3% withdrawal fee. Users can enjoy a fee-free deposit period that ends in August. [Note: Special thanks to Larry C. Bates, out of Bloomington, Indiana, the architect behind the great description of the DC, a good partner of OpenLedger and his assistance in giving answer to the questions.] Do you think decentralized conglomerates will change big business around the globe? Let us know your thoughts in the comments below! Images courtesy of OpenLedger, bitcoinwiki.co Editor, Content Creator @Bitcoin.com",4340306774493623681,1180,Google
265,3353902017498793780,ERITREA,The Rise And Growth of Ethereum Gets Mainstream Coverage,"Ethereum, considered by many to be the most promising altcoin, has grabbed the attention of The New York Times. The ""newspaper of record"" has run a big feature story on Ethereum, noting its value has spiraled 1,000% in the last three months and is attracting interest from major financial companies that are using it for private blockchains and smart contracts. The story notes the first public version of Ethereum was recently released. The story noted there are numerous applications built on Ethereum that allow new ways to place bets, pay bills and even launch Ponzi schemes. Some cryptocurrency observers claim Ethereum will face more security issues than bitcoin due to its more complex software. The alt. currency system has faced less testing and suffered fewer attacks than bitcoin. The unusual design could also invite more regulatory scrutiny considering some potentially fraudulent contracts can be written into the system. Capabilities Draw Interest However, the system's capabilities have drawn interest from major corporations. Last year, IBM announced it was testing the alt.currency system as a way to manage real-world objects in the Internet of Things. Microsoft has been working on various projects that make it easier to use Ethereum on its Azure computing cloud. Marley Gray, a business development and strategy director at Microsoft, said Ethereum is a platform for solving problems in different industries. He called it the most ""elegant"" solution seen to date. Private Blockchains Expand Many companies have created private blockchains using Ethereum. These private blockchains could eventually undermine the value of the Ether, the individual unit in the alt.currency system that people have been buying. Major banks have shown interest in using blockchains to make transferring and trading money more efficient. Several banks have examined how to use Ethereum. JPMorgan has developed a tool called Masala that enables its internal databases to interact with an Ethereum-blockchain. Michael Novogratz, who helped lead Fortress Investing Group invest in bitcoin, has been examining Ethereum since leaving Fortress last fall. He said he has made a significant purchase of Ether. Novogratz said in the last few months that the alt.currency is getting more validation. Ethereum Value Soars Ether's value has increased to as high as $12 from $1 since the beginning of the year, taking the value of all Ether currency to above $1 billion at times. This surpasses the value of any virtual currency besides bitcoin, which claimed more than $6 billion in value last week. Ethereum leads all altcoins in the size of its following One difference between Ethereum and bitcoin is that the latter came into being in a more transparent fashion. Where bitcoin was created by Satoshi Nakamoto, whose identity remains a mystery, Ethereum was developed by Vitalik Buterin , a 21-year-old Russian, who created it after dropping out of Waterloo University in Ontario. Aim Was Smart Contracts Ethereum's basic aim was to make it possible to program agreements into the blockchain. This is the smart contract concept. Two people could, for instance, program a sport bet directly into the Ethereum blockchain. Once a mutually-accepted source announced the final score (such as the Associated Press), the winnings would transfer automatically to the winner. Ether can be used as currency. Ether is also necessary to pay for the network power required to process the bet. Ethereum has been characterized as a single-shared computer operated by the network of users on which resources are doled out and paid for by Ether. Buterin's team raised $18 million through an Ether presale in 2014 to help fund the Ethereum Foundation, which supports the development of the software. Also read: Ether co-founder Vitalik Buterin on public and private blockchains Ethereum's Following Ethereum has attracted followers who have supported the software and hope that Ether will rise in value. There were 5,800 nodes, or computers, last week supporting the network. The bitcoin network had about 7,400 nodes. Joseph Lubin, a co-founder, established ConsenSys, which has hired more than 50 developers to create applications on the alt.currency system. One enables music distribution while another permits a new type of financial auditing. Lubin said he became interested in Ethereum after determining it delivered on some of bitcoin's failed promise, particularly when it came to permitting new types of online markets and contracts. He said Ethereum presented the crystallization of how to deliver on the broad strokes vision that bitcoin presented. Joseph Bonneau, a Stanford computer science researcher, said Ethereum is the first system that caught his interest since bitcoin. Ethereum is nonetheless far from a certainty, Bonneau said. He said bitcoin is still most likely the safest bet, with Ethereum number two. He said Ethereum's longevity will depend on real markets developing around it. Featured image from Shutterstock.",4340306774493623681,2416,Instagram
234,-9157338616628196758,ENGLAND,Situação financeira ruim de varejistas pressiona shoppings e eleva renegociações - Home - iG,"A queda nas vendas e a deterioração na situação financeira de varejistas têm forçado shoppings a renegociar algumas das obrigações dos lojistas nos custos de ocupação de pontos de venda. O cenário se intensificou para além da negociação de descontos pontuais, com parcelamentos que miram evitar a inadimplência e até mesmo o aumento das taxas de vacância. Arquivo/Agência Brasil Shopping centers estão entre os estabelecimentos que mais contratam no fim de ano O diretor de expansão da varejista Hope, Sylvio Korytowski, relata que muitos shoppings aceitaram congelar o vencimento de parcelas das chamadas ""luvas"", tarifas cobradas de novos locatários para garantir o ""direito"" de utilização de um determinado ponto em um shopping. As quantias, que variam de acordo com a demanda, normalmente são parceladas em 12 ou 24 meses, mas agora lojistas têm conseguido uma espécie de ""perdão temporário"", passando algum tempo sem pagar e conseguindo mais meses para quitar os valores. O preço pode ser negociado entre um lojista que sai do local e um que está chegando, mas geralmente parte do montante é destinada ao operador do shopping. ""Às vezes, quando um shopping vai mal, tem empreendedor que oferece um aluguel quase zerado para o lojista porque, se o espaço fica vago, quem paga o custo de energia, segurança, limpeza e outras coisas do dia a dia é o shopping"", explica um executivo do setor, que preferiu não se identificar. ""Nesse casos, não existe nenhuma possibilidade de o shopping cobrar luvas"", acrescenta. O esforço dos shoppings é impedir que lojas importantes fechem em meio a alta de custos e venda fraca, mas a renegociação também ofusca eventuais riscos de um estouro de inadimplência. A CEO da GS&AGR Consultores, Ana Paula Tozzi, avalia que os efeitos do quadro financeiro ruim dos varejistas ainda não aparecem totalmente nas provisões para inadimplência dos operadores de shopping, mas considera que muitos lojistas estão em dificuldade para pagar obrigações e pedindo refinanciamentos e renegociações. Fato é que a inadimplência tem aumentado em alguns centros de compras. Nos empreendimentos da BRMalls, a expectativa é que a taxa tenha um ""aumento pequeno"" no primeiro trimestre em comparação com igual intervalo do ano passado, quando estava em 4,4%. Já os atrasos superiores a 25 dias no pagamento de alugueis nos shopping centers da Multiplan subiram para 1,9% no quarto trimestre de 2015, de 1,7% em igual período do ano anterior. No mesmo período, a taxa de perda de aluguel aumentou para 1,2%, de 0,6%. O momento de enfraquecimento das vendas tem levado algumas lojas a dificuldades financeiras e, em alguns casos, culminado em processos de recuperação judicial. É o que ocorreu com o grupo GEP, dono das marcas Luigi Bertolli, Cori e franqueado da marca norte-americana GAP no Brasil. O grupo, que opera 97 lojas no Brasil, a maioria em shoppings, afirmou no pedido de recuperação judicial que ""o volume de vendas diminuiu consideravelmente"" em meio a crise macroeconômica. Varejistas relatam ainda que concessões estão sendo feitas mesmo em shoppings considerados ""de primeira linha"", ou seja, os que têm maior fluxo de clientes. Um exemplo citado por representantes de grandes redes do varejo é o da Iguatemi. Segundo eles, a companhia está concedendo descontos em shoppings da capital paulista em troca de impedir que as redes fechem lojas em centros comerciais da grande São Paulo e interior, os quais os varejistas veem com menos interesse. Ao Broadcast, serviço em tempo real da Agência Estaado, a diretora financeira e de relações com investidores da Iguatemi, Cristina Betts, diz que não há uma política única em relação aos descontos. ""Depende muito dos grupos de varejo de que estamos falando e dos locais onde temos lojas com eles. Não dá para dizer que existe uma lógica apenas"", afirma. Por ter concedido mais descontos, a companhia conseguiu reduzir sua taxa de inadimplência para 1,1% no quarto trimestre de 2015, saindo de 1 8% em igual período do ano anterior. ""Tivemos em 2015 um esforço de antecipar movimentos de descontos, principalmente em shoppings em maturação"", diz. ""Preferimos que o lojista fique bem e sobreviva, diminuindo a inadimplência. Houve um aumento de desconto em 2015, mas esperamos que fique estável em 2016"", acrescenta. Para Korytowski, o desafio das negociações hoje é que há uma massa muito grande de lojistas em dificuldades e pedindo ajuda. Isso torna difícil a tarefa do shopping de identificar quem de fato pode sobreviver com algum tipo de renegociação, já que em alguns casos isso não é suficiente para evitar o fechamento das lojas. ""No cenário atual, todo mundo pede ajuda, quem precisa e quem não precisa. Faz parte do nosso trabalho identificar quem precisa e quem queremos dentro dos shoppings. Não adianta tentar ajudar um lojista que não tem futuro"", afirma a diretora do Iguatemi, ao lembrar o fechamento da Nôa Nôa no Iguatemi São Paulo, em 2008, que abriu espaço para a chegada da Channel. ""Em épocas de mais dificuldades, tem um movimento mais acelerado de substituição de marcas que não mostram um desempenho tão bom. São poucas as marcas eternas"", acrescenta. Veja fotos dos Mall of the World, o maior shopping do mundo",5206835909720479405,1876,LinkedIn
223,1805789466376069146,JAPAN,Setting Up HTTP(S) Load Balancing,"HTTP(S) load balancing provides global load balancing for HTTP(S) requests destined for your instances. You can configure URL rules that route some URLs to one set of instances and route other URLs to other instances. Requests are always routed to the instance group that is closest to the user, provided that group has enough capacity and is appropriate for the request. If the closest group does not have enough capacity, the request is sent to the closest group that does have capacity. HTTP requests can be load balanced based on port 80 or port 8080. HTTPS requests can be load balanced on port 443. The load balancer acts as an HTTP/2 to HTTP/1.1 translation layer, which means that the web servers always see and respond to HTTP/1.1 requests, but that requests from the browser can be HTTP/1.0, HTTP/1.1, or HTTP/2. HTTP(S) load balancing does not support WebSocket. You can use WebSocket traffic with Network load balancing . Contents Before you begin HTTP(S) load balancing uses instance groups to organize instances. Make sure you are familiar with instance groups before you use load balancing. Fundamentals Overview An HTTP(S) load balancer is composed of several components. The following diagram illustrates the architecture of a complete HTTP(S) load balancer: The following sections describe how each component works together to make up each type of load balancer. For a detailed description of each component, see Components below. HTTP load balancing A complete HTTP load balancer is structured as follows: A global forwarding rule directs incoming requests to a target HTTP proxy . The target HTTP proxy checks each request against a URL map to determine the appropriate backend service for the request. The backend service directs each request to an appropriate backend based on serving capacity, zone, and instance health of its attached backends. The health of each backend instance is verified using either an HTTP health check or an HTTPS health check. If the backend service is configured to use the latter, the request will be encrypted on its way to the backend instance. In addition, you must create a firewall rule to enable traffic to your HTTP load balancer. The rule should enable traffic on the port your global forwarding rule has been configured to use (either 80 or 8080). HTTPS load balancing An HTTPS load balancer shares the same basic structure as an HTTP load balancer (described above), but differs in the following ways: Uses a target HTTPS proxy instead of a target HTTP proxy Requires a signed SSL certificate for the load balancer Requires a firewall rule that enables traffic on port 443 The client SSL session terminates at the load balancer. Sessions between the load balancer and the instance can either be HTTPS (recommended) or HTTP. If HTTPS, each instance must have a certificate. Components Global forwarding rules and addresses Global forwarding rules route traffic by IP address, port, and protocol to a load balancing configuration consisting of a target proxy, URL map, and one or more backend services. Each global forwarding rule provides a single global IP address that can be used in DNS records for your application. No DNS-based load balancing is required. You can either specify the IP address to be used or let Google Compute Engine assign one for you. Target proxies Target proxies terminate HTTP(S) connections from clients, and are referenced by one or more global forwarding rules and route the incoming requests to a URL map. The proxies set HTTP request/response headers as follows: Via: 1.1 google (requests and responses) X-Forwarded-Proto: [http | https] (requests only) X-Forwarded-For: <client IP(s)>, <global forwarding rule external IP> (requests only) Can be a comma-separated list of IP addresses depending on the X-Forwarded-For entries appended by the intermediaries the client is traveling through. The first element in the <client IP(s)> section shows the origin address. X-Cloud-Trace-Context: <trace-id>/<span-id>;<trace-options> (requests only) Parameters for Stackdriver Trace . URL maps URL maps define matching patterns for URL-based routing of requests to the appropriate backend services. A default service is defined to handle any requests that do not match a specified host rule or path matching rule. In some situations, such as the cross-region load balancing example , you might not define any URL rules and rely only on the default service. For content-based routing of traffic, the URL map allows you to divide your traffic by examining the URL components to send requests to different sets of backends. SSL certificates SSL certificates are used by target HTTPS proxies to securely route incoming HTTPS requests to backend services defined in a URL map. Backend services Backend services direct incoming traffic to one or more attached backends. Each backend is composed of an instance group and additional serving capacity metadata. Backend serving capacity can be based on CPU or requests per second (RPS) . Each backend service also specifies which health checks will be performed against the available instances. HTTP(S) load balancing supports Compute Engine Autoscaler , which allows users to perform autoscaling on the instance groups in a backend service. For more information, see Scaling Based on HTTP load balancing serving capacity . Load distribution algorithm HTTP(S) load balancing provides two methods of determining instance load. Within the backend service object, the balancingMode property selects between the requests per second (RPS) and CPU utilization modes. Both modes allow a maximum value to be specified; the HTTP load balancer will try to ensure that load remains under the limit, but short bursts above the limit can occur during failover or load spike events. Incoming requests are sent to the region closest to the user that has remaining capacity. If more than one zone is configured with backends in a region, the traffic is distributed across the instance groups in each zone according to each group's capacity. Within the zone, the requests are spread evenly over the instances using a round-robin algorithm. Round-robin distribution can be overridden by configuring session affinity . Session affinity Alpha This is an Alpha release of HTTP(S) Load Balancing Session Affinity. This feature might be changed in backward-incompatible ways and is not recommended for production use. It is not subject to any SLA or deprecation policy. Request to be whitelisted to use this feature . Session affinity sends all request from the same client to the same virtual machine instance as long as the instance stays healthy and has capacity. Google Cloud HTTP(S) Load Balancing offers two types of session affinity: Interfaces Your HTTP(S) load balancing service can be configured and updated through the following interfaces: The gcloud tool : gcloud is a command-line tool included in the Cloud SDK . The HTTP(S) load balancing documentation calls on this tool frequently to accomplish tasks. For a complete overview of gcloud documentation, see the gcloud Tool Guide . You can find commands related to load balancing in the gcloud compute and gcloud preview command groups. You can also get detailed help for any gcloud command by using the --help flag: gcloud compute http-health-checks create --help The Google Cloud Platform Console : Load balancing tasks can be accomplished through the Google Cloud Platform Console . The REST API : All load balancing tasks can be accomplished using the Google Compute Engine API. The API reference docs describe the resources and methods available to you. TLS support A HTTPS target proxy accepts only TLS 1.0 and up when terminating client SSL requests. It speaks only TLS 1.0 and up to the backend service when the backend protocol is HTTPS. Logging Alpha This is an Alpha release of Google Cloud HTTP(S) Load Balancing Logging. This feature might be changed in backward-incompatible ways and is not recommended for production use. It is not subject to any SLA or deprecation policy. Request to be whitelisted to use this feature . Each HTTP(S) request is logged via Google Cloud Logging . If you have been accepted into the Alpha testing phase, logging is automatic and does not need to be enabled. How to view logs To view logs, go to the Logs Viewer in the Cloud Platform Console. HTTP(S) logs are indexed first by forwarding rule , then by URL map . To see all logs, in the first pull-down menu select Load Balancing > All forwarding rules . To see logs for just one forwarding rule, select a single forwarding rule name from the list. To see logs for just one URL map used by a forwarding rule, select Load Balancing and choose the forwarding rule and URL map of interest. What is logged In addition to general information contained in most logs, such as severity, project ID, project number, and timestamp, HTTP(S) load balancing logs contain HttpRequest log fields. Log fields of type boolean typically only appear if they have a value of true . If a boolean field has a value of false , that field is omitted from the log. UTF-8 encoding is enforced for these fields. Characters that are not UTF-8 characters are replaced with question marks. Next steps The following guides demonstrate two different scenarios using the HTTP(S) load balancing service. These scenarios provide a practical context for HTTP(S) load balancing and demonstrate how you might set up load balancing for your specific needs. Cross-region load balancing You can use a global IP address that can intelligently route users based on proximity. For example, if you set up instances in North America, Europe, and Asia, users around the world will be automatically sent to the backends closest to them, assuming those instances have enough capacity. If the closest instances do not have enough capacity, cross-region load balancing automatically forwards users to the next closest region. Get started with cross-region load balancing Content-based load balancing Content-based or content-aware load balancing uses HTTP(S) load balancing to distribute traffic to different instances based on the incoming HTTP(S) URL. For example, you can set up some instances to handle your video content and another set to handle everything else. You can configure your load balancer to direct traffic for example.com/video to the video servers and example.com/ to the default servers. Get started with content-based load balancing Content-based and cross-region load-balancing can work together by using multiple backend services and multiple regions. You can build on top of the scenarios above to configure your own load balancing configuration that meets your needs. Notes and Restrictions HTTP(S) load balancing does not support HTTP/1.1 100 Continue response. This might affect multipart POST. The load balancing configuration automatically creates firewall rules if the instance operating system is a Compute Engine image. If not, you have to create the firewall rules manually. Load balancing does not keep instances in sync. You must set up your own mechanisms, such as using Deployment Manager , for ensuring that your instances have consistent configurations and data. Troubleshooting Traffic from the load balancer to your instances has an IP address in the range of 130.211.0.0/22. When viewing logs on your load balanced instances, you will not see the source address of the original client. Instead, you will see source addresses from this range.",-1032019229384696495,2885,LinkedIn
156,-2081760549863309770,ERITREA,Setting Up SSL proxy for Google Cloud Load Balancing,"Alpha This is an Alpha release of Setting Up SSL proxy for Google Cloud Load Balancing. This feature might be changed in backward-incompatible ways and is not recommended for production use. It is not subject to any SLA or deprecation policy. Request to be whitelisted to use this feature . Google Cloud SSL proxy terminates user SSL (TLS) connections at the global load balancing layer, then balances the connections across your instances via SSL or TCP. Cloud SSL proxy is intended for non-HTTP(S) traffic. For HTTP(S) traffic, HTTP(S) load balancing is recommended instead. Contents Google Cloud Load Balancing with SSL proxy Setting up SSL load balancing Configure instance groups and backend services Configure frontend services Additional Commands Listing target SSL proxies Describe target SSL proxies Delete target SSL proxy Update a backend service for the target SSL proxy Update the SSL certificates for the target SSL proxy Update PROXY protocol header for the proxy PROXY protocol for retaining client connection information Recommendations Troubleshooting Pages fail to load from load balancer IP Alpha Limitations FAQ When should I use HTTPS load balancing instead of SSL proxy load balancing? Can I view the original IP address of the connection to the global load balancing layer? Overview We are excited to introduce SSL (TLS) proxying for your SSL traffic. With SSL proxy, you can terminate your customers' SSL sessions at the global load balancing layer, then forward the traffic to your virtual machine instances using SSL (recommended) or TCP. SSL proxy is a global load balancing service. You can deploy your instances in multiple regions, and global load balancing will automatically direct traffic to the region closest to the user. If a region is at capacity, the load balancer automatically directs new connections to another region with available capacity. Existing user connections to remain in the current region. We recommend use of end-to-end encryption for your SSL proxy deployment by configuring your backend service to accept traffic over SSL ( backend-services --protocol SSL ). This ensures that the client traffic decrypted at the SSL proxy layer is encrypted again before being sent to the backend instances. This end-to-end encryption requires you to provision certificates and keys on your instances so they can perform SSL processing. The advantages of managed SSL proxy are as follows: Intelligent routing - the load balancer can route requests to backend locations where there is capacity. In contrast, an L3/L4 load balancer must route to regional backends without paying attention to capacity. Use of smarter routing allows provisioning at N+1 or N+2 instead of x*N. Better utilization of the back-end servers - SSL processing can be very CPU intensive if the ciphers used are not CPU efficient. In order to maximize CPU performance, use ECDSA SSL certs,TLS1.2 and prefer the ECDHE-ECDSA-AES128-GCM-SHA256 cipher suite for SSL between the load balancer and your instances. Certificate management - You only need to update your customer facing certificate in one place when you need to switch certs. You can also reduce the management overhead for your instances by using self-signed certificates. Security patching - If vulnerabilities arise in the SSL or TCP stack, we will apply patches at the load balancer automatically in order to keep your instances safe. Notes: While choosing to send the traffic over unencrypted TCP ( backend-services --protocol TCP ) between the global load balancing layer and instances enables you to manage your SSL certificates at one place and offload SSL processing from your instances, it also comes with reduced security between your global load balancing layer and instances and is therefore not recommended. SSL proxy can handle HTTPS, but this is not recommended. You should instead use HTTP(S) load balancing for HTTPS traffic. See the FAQ for details. We now describe how SSL proxy works and walk you through configuring an SSL proxy for load balancing traffic to some instances. Google Cloud Load Balancing with SSL proxy With SSL proxy at the global load balancing layer, traffic coming over an SSL connection is terminated at the global layer then proxied to the closest available instance group. In this example, the traffic from the users in Iowa and Boston is terminated at the global load balancing layer, and a separate connection is established to the selected backend instance. Google Cloud Load Balancing with SSL termination (click to enlarge) Setting up SSL load balancing This example demonstrates setting up global SSL load balancing for a simple service that exists in two regions: us-central1 and us-east1 . We will configure the following: Instance groups for holding the instances A pair of instances for each instance group A health check for verifying instance health A backend service, which monitors instances in groups and prevents them from exceeding configured usage The SSL proxy itself with its SSL certificate A public static IP address and forwarding rule that sends user traffic to the proxy A firewall rule for the load balancer IP address After that, we'll test our configuration. Configure instance groups and backend services This section shows how to create simple instance groups, add instances to them, then add those instances to a backend service with a health check. A production system would normally use managed instance groups based on instance templates , but this setup is quicker for initial testing. Backend configuration (click to enlarge) Create an instance group for each zone gcloud compute instance-groups unmanaged create us-ig1 --zone us-central1-b Created [ [PROJECT_ID]/zones/us-central1-b/instanceGroups/us-ig1]. NAME ZONE NETWORK MANAGED INSTANCES us-ig1 us-central1-b 0 gcloud compute instance-groups unmanaged create us-ig2 --zone us-east1-b Created [ [PROJECT_ID]/zones/us-east1-b/instanceGroups/us-ig2]. NAME ZONE NETWORK MANAGED INSTANCES us-ig2 us-east1-b 0 Create two instances in each zone For testing purposes, we'll install Apache on each instance. Normally, you wouldn't use SSL load balancing for HTTP traffic, but Apache is commonly used and is easy to set up for testing. These instance are all being created with a tag of ssl-lb . This tag is used later by the firewall rule. Add instances to the instance groups Add ig-us-central1-1 and ig-us-central1-2 to us-ig1 gcloud compute instance-groups unmanaged add-instances us-ig1 \ --instances ig-us-central1-1,ig-us-central1-2 \ --zone us-central1-b Updated [ [PROJECT_ID]/zones/us-central1-b/instanceGroups/us-ig1]. Add ig-us-east1-1 and ig-us-east1-2 to us-ig2 gcloud compute instance-groups unmanaged add-instances us-ig2 \ --instances ig-us-east1-1,ig-us-east1-2 \ --zone us-east1-b Updated [ [PROJECT_ID]/zones/us-east1-b/instanceGroups/us-ig2]. You now have an instance group in two different regions, each with two instances. When we create a backend service, we have to specify a health check, so we'll create the health check next. Create a health check You can configure either an SSL or TCP health check for determining the health of your instances. If you are using SSL between the load balancer and the instances, use an SSL health check. If you are using plain TCP between the load balancer and the instances, use a TCP health check. Once configured, health checks are sent on a regular basis to the specified port on all the instances in the configured instance groups. If the health check fails, the instance is marked as UNHEALTHY and the load balancer stops sending new connections to that instance until the instance becomes healthy again. Existing connections are allowed to continue. In this example, we're creating a simple SSL health check that we'll use with our backend service. This health check does a simple SSL handshake with each instance on port 443 to determine health. If the handshake succeeds twice in a row (the default), the new instance is marked HEALTHY . If the handshake fails twice in a row (the default) on a HEALTHY instance, the instance is marked UNHEALTHY . If the handshake is again successful twice in a row, the instance is marked as HEALTHY again. See the Health checks section for more information and options. gcloud alpha compute health-checks create ssl my-ssl-health-check --port 443 Created [ [PROJECT_ID]/global/healthChecks/my-ssl-health-check]. NAME PROTOCOL my-ssl-health-check SSL Create a backend service A backend service defines the capacity, max utilization, and health check of the instance groups it contains. Backend services direct incoming traffic to one or more attached backends (depending on the load balancing mode, discussed later). Each backend consists of an instance group and additional configuration to balance traffic among the instances in the instance group. Each instance group is composed of one or more instances. Each backend service also specifies which health checks will be performed for the instances in an all the instance groups added to the backend service. The duration of idle SSL proxy connections through the load balancer is limited by the backend service timeout. In this example we'll add a backend service that connects to instances over SSL. This only governs connections between the load balancer and the instance, not the connections between users and the load balancer. gcloud alpha compute backend-services create my-backend-service \ --protocol SSL \ --health-check my-ssl-health-check \ --timeout 5m Created [ [PROJECT_ID]/global/backendServices/my-backend-service]. NAME BACKENDS PROTOCOL my-backend-service SSL Alternatively you could configure unencrypted communication between from the load balancer to the instances with --protocol TCP . Configure your backend service When you configure a backend service, you must add instance groups and specify a balancing mode that determines how much traffic the load balancer can send to instances in each instance group. Once the limit is reached for a particular instance group, additional requests are sent to an instance group that is next closest to the user, as long as it has capacity. SSL proxy supports the following balancing mode: UTILIZATION (default): instances can accept traffic as long as the average current CPU utilization of the instance group is below an indicated value. To set this value, use the --max-utilization parameter and pass a value between 0.0 (0%) and 1.0 (100%). Default is 0.8 (80%). For this example, we'll add both instance groups to the same backend service and set the balancing mode to send traffic to instance groups that have not reached 80% utilization. gcloud alpha compute backend-services add-backend my-backend-service \ --instance-group us-ig1 \ --zone us-central1-b \ --balancing-mode UTILIZATION \ --max-utilization 0.8 Updated [ [PROJECT_ID]/global/backendServices/my-backend-service]. gcloud alpha compute backend-services add-backend my-backend-service \ --instance-group us-ig2 \ --zone us-east1-b \ --balancing-mode UTILIZATION \ --max-utilization 0.8 Updated [ [PROJECT_ID]/global/backendServices/my-backend-service]. Configure frontend services This section shows how to create the following frontend resources: an SslCertificate resource to use with the load balancer an SSL proxy load balancer a static external IP address and a forwarding rule to use with that address a firewall rule that allows traffic from the load balancer and the health checker to reach the instances Frontend configuration (click to enlarge) Configure an SSL certificate and key If you don't have a private key and signed certificate, you can create and use a self-signed certificate for testing purposes, or get real certificate from an authority. See SSL Certificates for further information. You should not use a self-signed certificate on the load balancer for production purposes. This step takes your certificate and key and creates an SSL certificate resource that you will assign to your SSL proxy in the next step. gcloud compute ssl-certificates create my-ssl-cert \ --certificate [CRT_FILE_PATH] \ --private-key [KEY_FILE_PATH] Created [ [PROJECT_ID]/global/sslCertificates/ssl-cert1]. NAME CREATION_TIMESTAMP ssl-cert1 2016-02-20T20:53:33.584-08:00 Configure a target SSL proxy The target SSL proxy receives the packets from the user and sends them to the backend service. When you create the target SSL proxy, you associate your backend service and SSL certificate with that resource. If you want to enable insertion of PROXY protocol version 1 header, you can configure the command above with --proxy-header PROXY_V1 . For more information on PROXY protocol, see Update proxy protocol header for the proxy . gcloud alpha compute target-ssl-proxies create my-target-ssl-proxy \ --backend-service my-backend-service \ --ssl-certificate my-ssl-cert \ --proxy-header NONE Created [ [PROJECT_ID]/global/targetSslProxies/my-target-ssl-proxy]. NAME PROXY_HEADER SERVICE SSL_CERTIFICATES my-target-ssl-proxy NONE my-backend-service ssl-cert1 Reserve a global static IP address Now we need to create a global reserved static IP address for your service. This IP address is the one your customers will use to access your load balanced service. gcloud compute addresses create ssl-lb-static-ip --global Created [ [PROJECT_ID]/global/addresses/ssl-lb-static-ip]. NAME REGION ADDRESS STATUS ssl-lb-static-ip [LB_STATIC_IP] RESERVED Configure a global forwarding rule Create a global forwarding rule to forward specific IPs and ports to the target SSL proxy. When customer traffic arrives at your external IP address, this forwarding rule tells the network to send that traffic to your SSL proxy. To create a global forwarding rule associated with the target proxy, replace LB_STATIC_IP with the IP address you generated in the prior step. gcloud alpha compute forwarding-rules create my-global-forwarding-rule \ --global \ --target-ssl-proxy my-target-ssl-proxy \ --address [LB_STATIC_IP] \ --port-range 443 Created [ [PROJECT_ID]/global/forwardingRules/my-global-forwarding-rule]. NAME REGION IP_ADDRESS IP_PROTOCOL TARGET my-global-forwarding-rule [LB_STATIC_IP] TCP my-target-ssl-proxy Create a firewall rule for the SSL load balancer Configure the firewall to allow traffic from the load balancer and health checker to the instances. gcloud compute firewall-rules create allow-ssl-130-211-0-0-22 \ --source-ranges 130.211.0.0/22 \ --target-tags ssl-lb \ --allow tcp:443 Created [ [PROJECT_ID]/global/firewalls/allow-ssl-130-211-0-0-22]. NAME NETWORK SRC_RANGES RULES SRC_TAGS TARGET_TAGS allow-ssl-130-211-0-0-22 default 130.211.0.0/22 tcp:443 ssl-lb Test your load balancer In your web browser, connect to your static IP address via HTTPS. In this test setup, the instances are using self-signed certificates. Therefore, you will see a warning in your browser the first time you access a page. Click through the warning to see the actual page. You should see one of the hosts from the region closest to you. Reload the page until you see the other instance in that region. To see instances from the other region, either stop the instances in the closest region or disable Apache on those instances. Alternatively, you can use curl from the your local machine's command line. If you are using a self-signed certificate on the SSL proxy, you must also specify -k . curl -k [LB_STATIC_IP] Additional Commands Listing target SSL proxies gcloud alpha compute target-ssl-proxies list NAME PROXY_HEADER SERVICE SSL_CERTIFICATES my-target-ssl-proxy NONE my-backend-service ssl-cert1 Describe target SSL proxies gcloud alpha compute target-ssl-proxies describe my-target-ssl-proxy creationTimestamp: '2016-02-20T20:55:17.633-08:00' id: '9208913598676794842' kind: compute#targetSslProxy name: my-target-ssl-proxy proxyHeader: NONE selfLink: [PROJECT_ID]/global/targetSslProxies/my-target-ssl-proxy service: [PROJECT_ID]/global/backendServices/my-backend-service sslCertificates: - [PROJECT_ID]/global/sslCertificates/ssl-cert1 Delete target SSL proxy To delete a target proxy, you must first delete any global forwarding rules that reference it. You can use the update command to point your SSL proxy at a different backend service. In this example, we'll create a new backend service and point the proxy at it. We'll then point back to the original proxy. Use this command to replace the SSL certificate on the SSL proxy. You must already have created a second SSL certificate resource. gcloud alpha compute target-ssl-proxies update my-target-ssl-proxy \ --ssl-certificate ssl-cert2 Updated [ [PROJECT_ID]/global/targetSslProxies/my-target-ssl-proxy]. Use this command to change the PROXY protocol header for an existing target SSL proxy. gcloud alpha compute target-ssl-proxies update my-target-ssl-proxy \ --proxy-header [NONE | PROXY_V1] Updated [ [PROJECT_ID]/global/targetSslProxies/my-target-ssl-proxy]. PROXY protocol for retaining client connection information Google Cloud Load Balancing with SSL proxy terminates SSL connections from the client and creates new connections to the instances, hence the original client IP and port information is not preserved by default. If you would like to preserve and send this information to your instances, then you will need to enable PROXY protocol (version 1) where an additional header containing the original connection information including source IP address, destination IP address, and port numbers is added and sent to the instance as a part of the request. The PROXY protocol header will typically be a single line of user-readable text with the following format: PROXY TCP4 <client IP> <load balancing IP> <source port> <dest port>\r\n An example of the PROXY protocol is show below: PROXY TCP4 192.0.2.1 198.51.100.1 15221 443\r\n Where client IP is 192.0.2.1 , load balancing IP is 198.51.100.1 , client port is 15221 and the destination port is 443 . In cases where the client IP is not known, the load balancer will generate a PROXY protocol header in the following format: PROXY UNKNOWN\r\n Health checks Health checks determine which instances can receive new connections. The health checker polls instances at specified intervals. Instances that fail the check are marked as UNHEALTHY . However, the health checker continues to poll unhealthy instances. If an instance passes its health check, it is marked HEALTHY . You can configure either an SSL or TCP health check to determine the health of your backend instances. If you are using SSL between the load balancer and the instances, use an SSL health check. If you are using plain TCP between the load balancer and the instances, use a TCP health check. Once configured, health checks will be sent on a regular basis on the specified port to all the instances in the configured instance groups. When you configure the health check to be of type SSL , an SSL connection is opened to each of your instances. When you configure the health check to be of type TCP , a TCP connection is opened. The health check itself can use one of the following checks: Simple handshake health check (default): the health checker attempts a simple TCP or SSL handshake. If it is successful, the instance passes. Request/response health check : you provide a request string for the health checker to send after completing the TCP or SSL handshake. If the instance returns the response string you've configured, the instance is marked as HEALTHY . Both the request and response strings can be up to 1024 bytes. If the check succeeds twice in a row (the default) on a new instance, the instance is marked HEALTHY . If the check fails twice in a row (the default) on a HEALTHY instance, the instance is marked UNHEALTHY . If the check is again successful twice in a row (default), the instance is marked as HEALTHY again. Existing connections are allowed to continue on instances that have failed their health check. Create a health check gcloud alpha compute health-checks create [tcp | ssl] my-ssl-health-check \ [--port PORT ] \ ...other options If you are encrypting traffic between the load balancer and your instances, use an SSL health check. If the traffic is unencrypted, use a TCP health check. Health check create options --check-interval [CHECK_INTERVAL]; default= 5s How often to perform a health check for an instance. For example, specifying 10s will run the check every 10 seconds. Valid units for this flag are s for seconds and m for minutes. --description [DESCRIPTION] An optional textual description for the health check. Must be surrounded by quotes if the string contains spaces. --healthy-threshold [HEALTHY_THRESHOLD]; default= 2 The number of consecutive successful health checks before an unhealthy instance is marked as HEALTHY . --port [PORT]; default= 80 for TCP, 443 for SSL The TCP port number that this health check monitors. --request An optional string of up to 1024 characters that the health checker can send to the instance. The health checker then looks for a reply from the instance of the string provided in the --response field. If --response is not configured, the health checker does not wait for a response and regards the check as successful if the TCP or SSL handshake was successful. --response An optional string of up to 1024 characters that the health checker expects to receive from the instance. If the response is not received exactly, the health check fails. If --response is configured, but not --request , the health checker will wait for a response anyway. Unless your system automatically sends out a message in response to a successful handshake, always configure --response to match an explicit --request . --timeout [TIMEOUT]; default= 5s If the health checker doesn't receive valid response from the instance within this interval, the check is considered a failure. For example, specifying 10s will cause the check to wait for 10 seconds before considering the request a failure. Valid units for this flag are s for seconds and m for minutes. --unhealthy-threshold [UNHEALTHY_THRESHOLD]; default= 2 The number of consecutive health check failures before a healthy instance is marked as UNHEALTHY . List health checks Lists all the health checks in the current project. gcloud alpha compute health-checks list NAME PROTOCOL my-ssl-health-check SSL Describe a health check Provides detailed information about a specific health check. gcloud alpha compute health-checks describe my-ssl-health-check checkIntervalSec: 5 creationTimestamp: '2016-02-20T20:47:26.034-08:00' description: '' healthyThreshold: 2 id: '1423984233044836273' kind: compute#healthCheck name: my-ssl-health-check selfLink: [PROJECT_ID]/global/healthChecks/my-ssl-health-check sslHealthCheck: port: 443 timeoutSec: 5 type: SSL unhealthyThreshold: 2 To modify a parameter in a health check, run the following command and pass in any of the create parameters. Any specified parameters will be changed. All unspecified parameters will be left the same. gcloud alpha compute health-checks [tcp|ssl] update [--options] Example: gcloud alpha compute health-checks update ssl my-ssl-health-check \ --description ""SSL health check"" Updated [ [PROJECT_ID]/global/healthChecks/my-ssl-health-check]. Recommendations You should configure the load balancer to prepend a PROXY protocol version 1 header if you need to retain the client connection information. If your traffic is HTTPS, then you should use HTTPS Load Balancing and not SSL proxy for load balancing. Troubleshooting Pages fail to load from load balancer IP Verify the health of instances Verify that the instances are HEALTHY. gcloud alpha compute backend-services get-health my-backend-service --- backend: [PROJECT_ID]/zones/us-central1-b/resourceViews/us-ig1 status: kind: compute#backendServiceGroupHealth --- backend: [PROJECT_ID]/zones/us-east1-b/instanceGroups/us-ig2 status: kind: compute#backendServiceGroupHealth Confirm that your firewall rule is correct Both the health checker and the load balancer need 130.211.0.0/22 to be open If you are doing SSL between the load balancer and the instances, you should do an SSL health check. In that case, tcp:443 must be allowed by the firewall from 130.211.0.0/22 . If you are doing TCP to the instances, do a TCP health check and open tcp:80 from 130.211.0.0/22 instead. If you are leveraging instance tags, make sure the tag is listed as under TARGET_TAGS in the firewall rule, and make sure all your instances have that tag. In this example, instances are tagged with ssl-lb . gcloud compute firewall-rules list NAME NETWORK SRC_RANGES RULES SRC_TAGS TARGET_TAGS allow-ssl-130-211-0-0-22 default 130.211.0.0/22 tcp:443 ssl-lb Try to reach individual instances Temporarily set a firewall rule that allows you to access your instances individually, then try to load a page from a specific instance. Then access one or more of your instances directly from your browser. [EXTERNAL_IP] Alpha Limitations The PROXY protocol header is currently only allowed if the protocol between the load balancer and the instance is set to TCP. This will be fixed for protocol SSL by Beta timeframe. FAQ When should I use HTTPS load balancing instead of SSL proxy load balancing? Though SSL proxy can handle HTTPS traffic, HTTPS Load Balancing has additional features that make it a better choice in most cases. HTTPS load balancing has the following additional functionality: Negotiates HTTP/2 and SPDY/3.1 Rejects invalid HTTP requests or responses Forwards requests to different instance groups based on URL host and path Integrates with Cloud CDN . Spreads the request load more evenly among instances, providing better instance utilization. HTTPS load balances each request separately, whereas SSL proxy sends all bytes from the same SSL or TCP connection to the same instance. SSL proxy for Google Cloud Load Balancing can be used for other protocols that use SSL, such as Websockets and IMAP over SSL. Can I view the original IP address of the connection to the global load balancing layer? Yes. You can configure the load balancer to prepend a PROXY protocol version 1 header to retain the original connection information. See Update proxy protocol header for the proxy for details.",-1032019229384696495,2022,LinkedIn
109,-5170198873410718233,ERITREA,NTT to buy Dell's services division for $3.05 billion,"You may know Dell as a computer and server maker, but Dell also operates a substantial IT services division - at least it did until today. NTT Data , the IT services company of NTT, is acquiring Dell Systems for $3.05 billion. The main reason why Dell sold off its division is that the company needs cash, and quickly. When Dell acquired EMC for $67 billion , the company promised that it would find ways to help finance the debt needed for the EMC acquisition. But $3.05 billion doesn't seem much. First, Dell acquired Perot Systems (which later became Dell Systems) for $3.9 billion in 2009. Second, Dell wanted more than $3.05 billion. Rumor has it that Dell was asking for $5 or $6 billion. So it looks like Dell didn't have enough time to find another potential buyer to outbid NTT. On the other side of the equation, NTT wants Dell Systems because Japan-based NTT is trying to expand its client base to new regions. Dell Systems is mostly operating in North America in the health-sector. NTT has already acquired Dimension Data in South Africa and Keane Inc. in the U.S. The company is just iterating on its strategy of expanding its foreign presence. Many Japanese companies have been looking at foreign markets when it comes to growth. In short, Dell is offloading side businesses in order to finance the EMC acquisition. Nobody knows if the EMC acquisition was a smart move, but at least Dell is committed to this strategy. Meanwhile, NTT is slowly but surely expanding to new markets. Featured Image: dokmai / Shutterstock",-1032019229384696495,1120,Google
150,-3367778232969996503,ENGLAND,"Good riddance, gig economy: Uber, Ayn Rand and the awesome collapse of Silicon Valley's dream of destroying your job","The Uber model just doesn't work for other industries. The price points always fail -- and that's a good thing The New York Times' Farhad Manjoo recently wrote an oddly lamenting piece about how ""the Uber model, it turns out, doesn't translate."" Manjoo describes how so many of the ""Uber-of-X"" companies that have sprung up as part of the so-called sharing economy have become just another way to deliver more expensively priced conveniences to those with enough money to pay. Ironically many of these Ayn Rand-inspired startups have been kept alive by subsidies of the venture capital kind which, for various reasons, are starting to dry up. Without that kind of ""VC welfare,"" these companies are having to raise their prices, and are finding it increasingly difficult to retain enough customers at the higher price point. Consequently, some of these startups are faltering; others are outright failing. Witness the recent collapse of SpoonRocket , an on-demand pre-made meal delivery service. Like Uber wanting to replace your car, SpoonRocket wanted to get you out of your kitchen by trying to be cheaper and faster than cooking. Its chefs mass-produced its limited menu of meals, and cars equipped with warming cases delivered the goods, aiming for ""sub-10 minute delivery of sub-$10 meals."" But it didn't work out as planned. And once the VC welfare started backing away, SpoonRocket could not maintain its low price point. The same has been happening with other on-demand services such as the valet-parking app Luxe, which has degraded to the point where Manjoo notes that "" prices are rising , service is declining, business models are shifting, and in some cases, companies are closing down."" Yet the telltale signs of the many problems with this heavily subsidized startup business model have been prevalent for quite some time, for those who wanted to see. In July 2014, media darling TaskRabbit, which had been hailed as a revolutionary for the way it allowed vulnerable workers to auction themselves to the lowest bidders for short-term gigs, underwent a major ""pivot."" That's Silicon Valley-speak for acknowledging that its business model wasn't working. It was losing too much money, and so it had to shake things up. TaskRabbit revamped how its platform worked, particularly how jobs are priced. CEO Leah Busque defended the changes as necessary to help TaskRabbit keep up with ""explosive demand growth,"" but published reports said the company was responding to a decline in the number of completed tasks. Too many of the Rabbits, it turns out, were not happy bunnies - they were underpaid and did a poor job, despite company rhetoric to the contrary. An increasing number of them simply failed to show up for their tasks . As a results, customers also failed to return. A contagion of pivots began happening among other sharing economy startups. Companies like Cherry (car washes), Prim (laundry), SnapGoods (gear rental), Rewinery (wine), HomeJoy (home cleaning) all went bust, some of them quietly and others with more headlines. Historical experience shows that three out of four startups fail , and more than nine out of 10 never earn a return. My favorite example is SnapGoods, which is still cited today by many journalists who are pumping up the sharing economy (and haven't done their homework) as a fitting example of a cool, hip company that allows people to rent out their spare equipment, like that drill you never use, or your backpack or spare bicycle-even though SnapGoods went out of business in August 2012. It just disappeared, poof, without a trace, yet goes on living in the imagination of sharing economy boosters. I conducted a Twitter interview with its former CEO, Ron J. Williams, as well as with whatever wizard currently lurks behind the faux curtain of the SnapGoods Twitter account, and the only comment they would make is that ""we pivoted and communicated to our 50,000 users that we had bigger fish to try."" Getting even more vague, they insisted ""we decided to build tech to strengthen social relationships and facilitate trust"" -classic sharing-economy speak for producing vaporware instead of substance from a company that had vanished with barely a trace. Zaarly, in its prime, was another sharing-economy darling of the venture capital set, with notable investors including Steve Jobs, hotshot VC firm Kleiner Perkins and former eBay CEO Meg Whitman on its board. It positioned itself in the marketplace as a competitor to TaskRabbit and similar services, with its brash founder and CEO, Bo Fishback, explaining his company's mission to a conference audience: "" If you've ever said, 'I'd pay X amount for Y,' then Zaarly is for you."" Fishback once spectacularly illustrated his brand by bringing on stage a cow being towed by a man in a baseball cap and carrying a jug of milk-""If I'm willing to pay $100 for someone to bring me a glass of fresh milk from an Omaha dairy cow right now, there might very well be a guy who would be super happy to do that,"" he said. That kind of bravado is what gave these companies their electric juice, as media outlets like the Economist lionized them as the ""on-demand"" economy. Like so many of the sharing-economy evangelicals, Fishback brandished a libertarian Ayn Randianism which saw Zaarly as creating ""the ultimate opt-in employment market, where there is no excuse for people who say, 'I don't know how to get a job, I don't know how to get started.'"" But alas, those were the heady, early years, when Zaarly was flush with VC cash. Flash forward to today and Fishback is more humble, as is his company, having gone through several ""pivots."" The ""request anything"" model is gone, as are Fishback's lofty sermons to American workers. Instead, Zaarly has become more narrowly focused on four comparatively mundane markets: house cleaning, handyman services, lawn care and maid service. And then there's Exec. Like Zaarly and TaskRabbit, Exec also started with great fanfare as a broader errand-running business, this one focused on hiring a personal assistant for busy Masters of the Universe. Like other sharing startups, initially it had grand ambitions about the on-demand economy and fomenting a revolution over how we work: connecting those with more money than time with those 1099 indies who desperately needed the money. But eventually this company too was forced by its market failures to narrow its focus, in this case to housekeeping exclusively. Finally the company flamed out and was sold to another housekeeping startup, Handybook. Exec's former CEO, Justin Kan, wrote a self-reflective farewell blog post about what he thought went wrong with his company . His observations are illuminating. His company had charged customers $25 per hour (which later rose to $30) to hire one of their personal assistants, and the worker received 80 percent, or about $20 per hour. That seemed like a high wage to Kan, but much to his surprise he discovered that, when his errand runners made their own personal calculation, factoring in the unsteadiness of the work, the frequency of downtime, hustling from gig to gig, the on-call nature of the work as well as their own expenses, it wasn't such a great deal. Wrote Kan, ""It turns out that $20 per hour does not provide enough economic incentive to dictate when our errand runners had to be available, leading to large supply gaps at times of spiky demand . . . it was impossible to ensure that we had consistent availability. Kan says the company also acquired a ""false sense that the quality of service for our customers was better than it was"" because the quality of the ""average recruitable errand runner""-at the low pay and on-call demands that Exec wanted-did not result in hiring the self-motivated personality types like those that start Silicon Valley companies. (Surprise, surprise.) That in turn led to too many negative experiences for too many customers, especially since, like with TaskRabbit, a too-high percentage of its on-demand workers simply failed to show up to their gigs. (Surprise, surprise.) It turns out, he discovered, that ""most competent people are not looking for part-time work."" (Surprise, surprise.) Indeed, the reality that the sharing economy visionaries can't seem to grasp is that not everyone is cut out to be a gig-preneur, or to ""build out their own businesses,"" as Leah Busque likes to say. Being an entrepreneur takes a uniquely wired brand of individual with a distinctive skill set, including being ""psychotically optimistic,"" as one business consultant put it. Simply being jobless is not a sufficient qualification. In addition, apparently nobody in Silicon Valley ever shared with Kan or Busque the old business secret that ""you get what you pay for."" That's a lesson that Uber's Travis Kalanick seems determined to learn the hard way as well. Kan, like Leah Busque, Bo Fishman and so many of the wide-eyed visionaries of Silicon Valley, had completely underestimated the human factor. To so many of these hyperactive venture entrepreneurs, workers are just another ore to be fed into their machine. They forget that the quality of the ore is crucial to their success, and that quality was dependent on how well the workers were treated and rewarded. The low pay and uncertain nature of the work keeps the employees wondering if there isn't a better deal somewhere else. Moreover, a degree of tunnel vision has prevented startup entrepreneurs from seeing that their business model often is not scalable or sustainable at the billionaire unicorn leve without ongoing VC welfare subsidies. Silicon Valley has an expression, ""That works on Sand Hill Road"" -referring to the upper-crust boulevard in Menlo Park, California, where much of the world's venture capital makes its home. Some things that seem like great ideas-like paying low wages to personal assistants to shuffle around at your every whim, or lowballing wages for someone to hustle around parking cars for yuppies-only make sense inside the VC bubble that has lost all contact with the realities of everyday Americans. A pattern has emerged about the ""white dwarf"" fate of many of these once-luminous sharing startups: after launching with much fanfare and tens of millions of VC capital behind them, vowing to enact a revolution in how people work and how society organizes peer-to-peer economic transactions, in the end many of these companies morphed into the equivalent of old-fashioned temp agencies (and others have simply imploded into black hole nothingness). Market forces have resulted in a convergence of companies on a few services which had been the most used on their platforms. In a real sense, even the startup king itself, Uber, is merely a temp agency, where workers do only one task: drive cars. Rebecca Smith, deputy director of the National Employment Law Project, compares the businesses of the gig economy to old-fashioned labor brokers. Companies like Instacart, Postmates and Uber, she says, talk as if they are different from old-style employers simply because they operate online. ""But in fact,"" she says, ""they are operating just like farm labor contractors, garment jobbers and day labor centers of old. "" Tech enthusiasts like the Times' Manjoo seem to be waking up to the smell of the coffee. ""The uneven service and increased prices,"" writes Manjoo, ""raise larger questions about on-demand apps"" which he says ""now often feel like just another luxury for people who have more money than time."" Yet that strikes me as too black-and-white, as overly gloomy as Manjoo once was excessively optimistic. The sharing economy apps have proven to be extremely fluid at connecting someone who needs work with someone willing to pay for that work. Some workers have praised the flexibility of the platforms, which allow labor market outsiders - young people, immigrants, minorities and seniors especially - who have difficulty finding work to access additional options. It's better than sitting at home as a couch potato with no income. And by narrowing the scope of their services, these companies stand a better chance of contracting with quality people, and developing real relationships with them. I suspect that, properly pivoted in the right direction, these app-based services will continue to play a role in the economy. Eventually many traditional economy companies may adapt an app-based labor market in ways that we can't yet anticipate. But that means we need to figure out a way to launch a universal, portable safety net for all U.S. workers (hint: we can do it at the local and state levels, we don't need to wait for a dysfunctional Congress). At the end of the day, the sharing economy startups have been hamstrung by the quality of the workers they hire. If they want good workers, they need to offer decent jobs. Otherwise, this sharing economy is not about sharing at all, and not very revolutionary. The current startup model destroys the social connection between businesses and those they employ, and these companies have failed to thrive because they provide crummy jobs that most people only want to do as a very last resort. These platforms show their workforce no allegiance or loyalty, and they engender none in return.",-1032019229384696495,1343,LinkedIn
172,4988225165850707692,AMERICA,The internet of a billion things | ET CIO,"Industrial adoption of IoT dubbed as Industrial Internet of Things (IIoT) is on ascend. Yet, near-term adoption of IIoT remains limited to achieving operational efficiency as most companies are unable to leverage IIoT for predictive capabilities that create new business opportunity. This article sheds light on the probable future of IIoT adoption and the imperative for businesses to form IIoT investment strategies. During Digital India Week in July, Prime Minister Modi announced the creation of a Centre of Excellence for Internet of Things. This is a timely intervention, as the Internet of Things, or IoT, will is set to transform technological and economic landscape over the next decade. The IoT will directly alter the Indian economy's industrial sectors, including manufacturing, energy, agriculture and transportation Together, these sectors account for close to half of India's GDP. It will also affect India's consumers, as their everyday devices connect to the Internet through tiny embedded sensors and computing power. The total potential payoff is enormous. The most conservative independent estimates place spending on the IoT worldwide at $500 billion by 2020. More optimistic forecasts peg the market as high as $15 trillion of global GDP by 2030. The Government of India estimates India's share to be between five and six percent of the worldwide IoT market through 2020. While consumer adoption of connected technology will be gradual in the short term, it will increase rapidly over the next decade. However, many industrial companies are already embracing what has been dubbed the Industrial Internet of Things, or IIoT. For example, the number of sensors shipped globally has increased more than five-fold from 4.2 billion units in 2012 to 23.6 billion units in 2014. Operational efficiency is one of the key attractions of the IIoT. Companies are seeking productivity gains that could reach 30 percent. For example, a big area for operational gains is in predictive maintenance of assets. This use of the IIoT could help reduce breakdowns by 70 percent, overall maintenance costs by 30 per cent, and repair costs by 12 per cent, according to estimates. However, there is more to the story. The IIoT can also be an important driver of breakthrough innovation and new growth opportunities. The key is to combine use of big data analytics with IIoT capabilities and technologies. For example, a major automobile manufacturer is pursuing a unique approach to increase value for customers: a flexible, convenient pay-per-use model for city dwellers needing cars. Customers can use an app to find the car that is parked nearest to them. They open the door with a membership card, drive to their destination, and simply park the car on the street and lock it up. This service competes with conventional taxis and hourly car rental services. Customers can choose to pay by the mile, the hour or the day. The rates are lower than for taxis, and there is no need to reserve, return or order a car; the cars can be parked and found anywhere use location sensors. Chart: Business benefits for driving near-term adoption of IIoT Source : Industrial Internet of Things: Unleashing the Potential of Connected Products and Services, WEF, January 2015 But are most companies up for the challenge? Survey results indicate that only 40 per cent can predict outcomes based on existing data, and fewer still (36 percent) can optimize operations from that data. Understandably, initial investments are focused on improving the ability to react and repair and move toward zero unplanned downtime. However, when asked about future plans, respondents stated their priorities in more ambitious plans for IIoT: increasing profitability (60 per cent), gaining a competitive advantage (57 per cent) and improving environmental safety and emissions (55 per cent) represent more pervasive, cross-functional and sophisticated uses of it. At Accenture, we believe that the transition from efficiency to value-enhanced growth is likely to follow four distinct phases. (See Figure) Phases one and two will represent immediate opportunities that drive the near-term adoption, starting with operational efficiency. Phases three and four will include long-term structural changes that are roughly three years away from mainstream adoption. These last two phases will lead to disruptive changes in businesses and industries. They will manifest themselves in the form of the outcome economy and an integrated human-machine workforce. Figure: The adoption and impact path of IIoT Source : Industrial Internet of Things: Unleashing the Potential of Connected Products and Services, WEF, January 2015 The outcome economy will be built on the automated quantification capabilities of the Industrial Internet. The large-scale shift from selling products or services to selling measurable outcomes is a significant change that will redefine the base of competition and industry structures. As the Industrial Internet becomes more ingrained in every industry, it will ultimately lead to a pull-based economy characterized by real-time demand sensing and highly automated, flexible production and fulfilment networks. This development will call for a pervasive use of automation and intelligent machines to complement human labour (machine augmentation). As a result, the face of the future workforce will change dramatically, along with the skill sets required to succeed in a much more automated economy. Yet, it is still early. Numerous technology challenges and important hurdles remain to be overcome. Not all products can or need to be connected. But amid the new, an old truth remains: business customers need products and services that create more value for them than those on offer today. The time to push is now. (Anindya Basu is Country Managing Director for Accenture in India & Prabhjit Didyala, Managing Director, Strategy for Accenture in India)",4670267857749552625,2184,Wikipedia
236,-5917314377186856799,SOUTH AFRICA,Artigos e Palestras - Programa Agricultura de Precisão do SENAR,"Artigos e Palestras ARTIGOS / 2015 12/08/2015 Perspectivas para o agronegócio demandam tecnologias para uma produtividade sustentável 23/06/2015 - Agricultura de precisão para alimentar 9 bilhões ARTIGOS / 2014 16/05/2014 - Adoção da Agricultura de Precisão No Brasil Por: Alberto C. de Campos Bernardi, da Embrapa Pecuária Sudeste ; Ricardo Y. Inamasu - Embrapa Instrumentação ARTIGOS / 2013 01/07/2013 - Agricultura de precisão - uma ferramenta ao alcance de todos (*) Alberto Bernardi é pesquisador da Embrapa Pecuária Sudeste, formado em Agronomia (1992); Mestrado: Solos e Nutrição de Plantas (ESALQ/USP, 1996); Doutorado: Solos e Nutrição de Plantas (ESALQ/USP, 1999).Atua na área de Fertilidade do solo e adubação, integração lavoura-pecuária, agricultura de precisão. (*) Ricardo Inamasu é pesquisador da Embrapa Instrumentação, fez graduação (1984), mestrado (1987) e doutorado (1995) em Engenharia Mecânica na Escola de Engenharia de São Carlos USP e pós-doutorado em Biological Systems Engineering na University of Nebraska - Lincoln. Tem experiência na área de Engenharia Mecânica e Mecatrônica, com ênfase em Instrumentação e Automação Agropecuária. 06/03/2013 - Tecnologia e Inovação na agropecuária brasileira (*) Renato Roscoe é engenheiro agrônomo (UFV, 2005), mestre em ciência do solo (UFLA, 1997) e doutor em environmental sciences (Wageningen University And Research Centre, 2002). Atua como diretor executivo e pesquisador no setor fertilidade do solo na Fundação MS. *Por José Luis da Silva Nunes - Dr. em Fitotecnia pela Universidade Federal do Rio Grande do Sul *Por João Guilherme Sabino Ometto é 2º vice-Presidente da FIESP. Artigo publicado no Jornal O Estado de S. Paulo em 04/10/2012 *Antônio Luis Santi1, Jackson Ernani Fiorin2, Kassia Luiza Teixeira Cocco3, Maurício Roberto Cherubin3, Mateus Tonini Eitelwein3, Telmo Jorge Carneiro Amado4, Fabio Evandro Grub Hauschild5 *Vitor Cauduro Girardello(2), Telmo Jorge Carneiro Amado(3), Rodrigo da Silveira Nicoloso(4), Tiago de Andrade Neves Hörbe(5), Ademir de Oliveira Ferreira(6), Fabiano Mauricio Tabaldi(7) & Mastrângello Enívar Lanzanova(8) *João Augusto Telles Presidente da Comissão de Irrigantes do Sistema FARSUL, Coordenador do Clube da Irrigação, Chefe da Divisão Técnica - SENAR-RS *De Telmo Amado e Antônio Santi, professores do Mestrado Profissional em Agricultura de Precisão da UFSM, Tiago Horbe, tiagohorbe@hotmail.com , e Ademir Ferreira, doutorandos em Ciência do Solo PPGCS-UFSM *José Paulo Molin, é um dos palestrantes dos seminários de Agricultura de Precisão do SENAR. Professor associado do Departamento de Engenharia de Biossistemas - Área de Mecânica e Máquinas Agrícolas da ESALQ/USP. *Ricardo Inamasu, é um dos palestrantes dos seminários sobre Agricultura de Precisão do SENAR. Pesquisador da Embrapa Instrumentação e professor colaborador da Escola de Engenharia de São Carlos, vinculada à Universidade de São Paulo (USP). PALESTRAS / 2013 - Agricultura de Precisão - situação e tendências: Jósé P. Molin - ESALQ/USP - Agricultura de Precisão - Base Conceitual: Ricardo Inamasu - Embrapa - Manejo da lavoura para altas produtividades com base na agricultura de precisão: Telmo Amado",4670267857749552625,1432,Google
231,6157037646878010131,SOUTH AFRICA,Rede Agricultura de Precisão II,"Do Se Te Qu Qu Se Sa 27 28 29 30 31 Faça download gratuito do novo livro: Você quer conhecer a Rede Agricultura de Precisão da Embrapa? Veja o Folder_Rede-AP Agricultura de Precisão Planejamento e gerenciamento de todos os processos da produção A Agricultura de Precisão é um tema abrangente, sistêmico e multidisciplinar. Não se limita a algumas culturas nem a algumas regiões. Trata-se de um sistema de manejo integrado de informações e tecnologias, fundamentado nos conceitos de que as variabilidades de espaço e tempo influenciam nos rendimentos dos cultivos. A agricultura de precisão visa o gerenciamento mais detalhado do sistema de produção agrícola como um todo, não somente das aplicações de insumos ou de mapeamentos diversos, mas de todo os processos envolvidos na produção. Esse conjunto de ferramentas para a agricultura pode fazer uso do GNSS (Global Navigation Satelite System), do SIG (Sistema de Informações Geográficas), de instrumentos e de sensores para medidas ou detecção de parâmetros ou de alvos de interesse no agroecossistema (solo, planta, insetos e doenças), de geoestatística e da mecatrônica. Mas a AP não está relacionada somente ao uso de ferramentas de alta tecnologia, pois os seus fundamentos podem ser empregados no dia-a-dia das propriedades pela maior organização e controle das atividades, dos gastos e produtividade em cada área. O emprego da diferenciação já ocorre na divisão e localização das lavouras dentro das propriedades, na divisão dos talhões ou piquetes, ou simplesmente, na identificação de ""manchas"" que diferem do padrão geral. A partir dessa divisão, o tratamento diferenciado de cada área é a aplicação do conceito de AP. Revolução gerencial, recursos tecnológicos e agregação de valores A Agricultura de Precisão é representada por estes três pontos que convergem em excelência de resultados: * Revolução gerencial; * Tecnologia de Informação; * Agregação de valor à produção. É fator determinante que estes três pontos sejam trabalhados em conjunto para que se estabeleça o aprimoramento da produtividade, da qualidade, do volume a ser produzido e da redução de preço dos produtos para competir no mercado interno e externo. Portanto, tecnologia, planejamento e gerenciamento são os fundamentos da Agricultura de Precisão. Quer saber mais? Os primeiros fundamentos teóricos da Agricultura de Precisão surgiram em 1929, nos Estados Unidos, porém tornou-se mais conhecida na década de 80, devido aos avanços e à difusão dos sistemas de posicionamento geográfico, sistemas de informações geográficas, monitoramento de colheita e também à informática. Além de destacar-se nos EUA, ganhou grande notoriedade em países como Alemanha, Argentina, Austrália, Inglaterra e Brasil. No país, as primeiras pesquisas na área foram realizadas na década de 90. No primeiro momento, a Agricultura de Precisão foi direcionada pelas máquinas agrícolas, como colheitadeiras e semeadeiras, embarcando-se a elas receptores GNSS (Global Navigation Satelite System), sofisticados computadores de bordo e sistemas que possibilitam a geração de mapas de produtividade. Aprimorou-se o mapeamento da variabilidade do solo, plantas e outros parâmetros, resultando numa aplicação otimizada de insumos, diminuindo custos e impactos ambientais negativos, consecutivamente, aumentando o retorno econômico, social e ambiental. Algumas iniciativas em pesquisa e desenvolvimento vêm sendo implementadas, colaborando para a inovação em Agricultura de Precisão no país. Atualmente são 53 grupos de pesquisas registrados no sistema Lattes do CNPq (Conselho Nacional de Desenvolvimento Científico e Tecnológico). No Brasil, o tema vem sendo divulgado em vários eventos importantes onde pesquisadores, empresas e produtores são reunidos: o SIAP (Simpósio Internacional de Agricultura de Precisão) e o ConBAP (Congresso Brasileiro de Agricultura de Precisão). No SIAP de 2007, coordenado pelo Mapa (Ministério da Agricultura, Pecuária e Abastecimento) foi instalado o Comitê Brasileiro de Agricultura de Precisão, um grande avanço para o setor. Nele foram reunidos os principais atores da Agricultura de Precisão no País, fornecendo importantes subsídios para que as políticas públicas possam ser contempladas. As ferramentas no mercado também avançaram, surgiram novos sensores e equipamentos, tornando a prática da AP cada vez mais acessível, com custos mais compatíveis e integráveis ao dia-a-dia de uma propriedade agrícola. No entanto, a adoção da Agricultura de Precisão nos diversos setores do agronegócio brasileiro está ocorrendo em um ritmo inferior ao previsto. Aumentar a taxa de utilização da AP no País, oferecendo tecnologias e conhecimentos para isso, é o papel que a Rede de Agricultura de Precisão da Embrapa pretende cumprir. O conhecimento gerado pela Embrapa, desde a criação da empresa em 1973, tem sido decisivo para o negócio agrícola brasileiro e para a posição de destaque que o Brasil hoje ocupa no cenário agrícola mundial. O Brasil e a Embrapa são referências em tecnologias para a agricultura tropical. O país é um dos líderes mundiais na produção e exportação de vários produtos agropecuários. Graças à essa posição no cenário mundial, o país passou a influir decisivamente no preço e no fluxo de alimentos e outras commodities agrícolas. A visão de futuro, o forte investimento na formação de recursos humanos e a capacidade de estar em sintonia com o avanço da ciência fazem com que a Embrapa possa contribuir para que o Brasil esteja posicionado na fronteira do conhecimento, em temas emergentes como agroenergia, créditos de carbono e biossegurança e em áreas como biotecnologia, nanotecnologia e agricultura de precisão. No caso específico da agricultura de precisão, a atuação da Embrapa e dos parceiros será fundamental para gerar conhecimentos, ferramentas e inovações tecnológicas para aumentar a eficiência dos sistemas produtivos. Estamos traçando o perfil do usuário de Agricultura de Precisão no Brasil. Por favor, nos auxilie respondendo aos questionários abaixo. Existe um para produtores e outro para consultores. PC1 - Gestão do Projeto PC2 - Desenvolvimento e validação de instrumentos e de tecnologias de informação PC3 - Caracterização, monitoramento e manejo da variabilidade espaço temporal em sistemas de culturas anuais PC4 - Caracterização, manejo e monitoramento de atributos do solo e da planta em sistemas de produção de plantas perenes PC5 - Inovação tecnológica em agricultura de precisão Assista aos videos da Rede Agricultura de Precisão da Embrapa! Association of Equipment Manufactures (Associação Norte Americana dos Fabricantes de Equipamentos, USA) Massey Ferguson / Valtra Cooperativa Agrária Agroindustrial Agrosystem Comércio, Imp. e Exp. Ltda APagri Consultoria Agronômica Auteq Computadores e Sistemas Ltda. Agri-Tillage do Brasil Indústria e Comércio de Máquinas e Implementos Agrícolas LTDA Batistella Florestal Fundação AGRISUS - Agricultura Sustentável Campo Agricultura e Meio Ambiente Confederação Nacional de Agricultura Case New Holland Cooperativa Agroindustrial dos Produtores Rurais do Sudoeste Goiano Cooperativa Agropecuária e Industrial Laboratório Nacional de Ciência e Tecnologia do Bioetanol Centro de Tecnologia da Informação Renato Archer Escola de Engenharia de São Carlos - Universidade de São Paulo Enalta Inovações Tecnológicas Empresa de Pesquisa Agropecuária e Extensão Rural de Santa Catarina Escola Superior de Agricultura ""Luiz de Queiroz"" - Universidade de São Paulo Fundação de Amparo à Ciência e Tecnologia do Estado de Pernambuco Mogi Mirim - SP Planaltina de Goiás - GO Castelândia - GO Petrolina - PE Faculdade de Ciências Agronômicas - Universidade Estadual Paulista ""Júlio De Mesquita Filho"" - Campus de Botucatu Florestalle Assessoria e Consultoria Florestal S. S. Agricultura Sustentável FISCHER S/A COM. IND. E AGRICULTURA SLC Agrícola - Fazenda Pamplona, Cristalina - GO Instituto Agronômico de Campinas Instituto Brasileiro de Geografia e Estatística Instituto CNA Máquinas Agrícolas Jacto S/A John Deere Brasil Kuhn do Brasil LOHR Sistemas Eletrônicos Ltda Marchesan Implementos e Máquinas Agrícolas TATU S/A Vinícola Miolo Original Indústria Eletrônica Ltda Escola Politécnica - Universidade de São Paulo Schio Agropecuária Serviço Nacional de Aprendizagem Rural SOMAFÉRTIL LTDA Stara S.A. Indústria de Implementos Agrícolas Terrasul Vinhos Finos Ltda Universidade de Caxias do Sul Universidade Federal de Lavras Faculdade de Agronomia Eliseu Maciel - Universidade Federal de Pelotas Universidade Federal do Rio Grande do Sul Universidade Federal de Santa Maria Programa de Pós-Graduação em Engenharia Agrícola Verion Agricultura A Embrapa está criando o Laboratório de Referência Nacional em Agricultura de Precisão (LANAPRE) , que funcionará como agente integrador das várias dimensões da Agricultura de Precisão, oferecendo uma área de integração para desenvolver padrões, realizar testes, validações e certificações de sistemas. Os recursos para a implantação do laboratório,da ordem de R$ 7,1 milhões, foram conquistados em 2010 com o apoio do Congresso Nacional por meio de emendas da Comissão de Desenvolvimento Econômico , Indústria e Comércio -CDEIC, presidida à época pelo Dep. Fed. Dr. Ubiali, e emendas parlamentares individuais do Dep. Fed. Duarte Nogueira e Dep. Fed. Lobbe Neto. O LANAPRE está localizado na região central do Estado de São Paulo, em São Carlos nas coordenadas 21 o 57'14""S e 47 o 51'08,45""O. Este município possui duas Unidades da Embrapa (Instrumentação e Pecuária Sudeste) e exitem outros três Centros próximos - na região de Campinas, que são de extrema importância para o tema (Monitoramento por Satélite, Informática e Meio Ambiente). O Laboratório deverá contar com uma infra-estrutura para abrigar máquinas, realizar testes de conexão entre diferentes fabricantes, tanto laboratorial como em campo, promover eventos para compatibilizar conexão e integrar diferentes sistemas, instalar sistema de suporte de informática e geoinformática para desenvolvedores, realizar testes de desempenho de campo com sistema integrado, ter lavouras de culturas mais importantes para o país como soja, milho, mandioca, pastagem (Integração Lavoura-Pecuária-Floresta), café, cana-de-açúcar, entre outros. Para isso, será construído um galpão limpo de 3.000 m 2 , com pátio/pista pavimentada de 2.000 m 2 , salas anexas de informática e eletrônica de 200 m 2 . O modelo proposto é de uma gestão compartilhada entre a Embrapa Pecuária Sudeste - localizada na Fazenda Canchim - onde o Laboratório ficará instalado e a Embrapa Instrumentação, que há anos tem se dedicado ao tema e, atualmente, coordena a Rede Nacional de Agricultura de Precisão da Embrapa. O LANAPRE terá condições de: Realizar pesquisa e desenvolvimento de máquinas e equipamentos para AP; Realizar testes de conexão entre diferentes fabricantes, tanto laboratorial como em campo; Promover eventos para compatibilizar conexão e integrar diferentes sistemas; Instalar sistema de suporte de informática e geoinformática para desenvolvedores; Realizar testes de desempenho de campo com sistema integrado. Acompanhe as etapas da construção: setembro 2012 Veja as notícias sobre o LANAPRE: Embrapa terá Laboratório de Referência Nacional em Agricultura de Precisão , Noite de celebração para comemorar os 40 anos da Embrapa Veja o LANAPRE no Google Earth: Local_LANAPRE (para visualizar é necessário ter instalado o programa Google Earth ) Grupo de São Carlos constrói equipamento robótico com laser para análise de elementos químicos presentes no solo e nas folhas CSIRO's Precision Agriculture group has developed a range of software tools to help manage and analyse spatial information. Carine Ferreira | Valor Técnica, usada principalmente por produtores mais jovens, consiste em mapear cada ponto do terreno para aumentar a produtividade. A Rede de Agricultura de Precisão da Empresa Brasileira de Pesquisa Agropecuária (Embrapa) realiza, no dia 15 de maio de 2014, um seminário no Laboratório de Referência Nacional em Agricultura de Precisão (Lanapre), em São Carlos (SP). Estudo envolveu mais de 200 profissionais de 44 unidades da empresa e mais de 30 instituições parceiras Modelos podem ser adaptados às necessidades do produtor Segundo o pesquisador Lúcio André de Castro Jorge, da Embrapa Instrumentação, já são vendidos modelos de drones no País equipados com câmeras básicas. Veículos aéreos não tripulados (VANTs) podem custar de R$ 5.000 a R$ 120.000 e são uma evolução para a agricultura de precisão. Embrapa também está criando softwares para a nova tecnologia, que poderão prever o aparecimento de pragas Pesquisadores aprimoram tecnologias usadas nas áreas espacial e militar para a produção agrícola After starting to operate the first exclusive laboratory for precision agriculture in the country, Embrapa presents the main solutions already developed in that place No Rádio UFSCar Convida de hoje, o tema é Agricultura de Precisão. E para conversar sobre esses recursos tecnológicos, nós recebemos os pesquisadores da Embrapa, Ricardo Inamasu e Alberto Bernardi. Entidade também está criando softwares para a nova tecnologia, que poderão prever o aparecimento de pragas Os chamados drones permitem monitorar a plantação e a propriedade de forma mais rápida e eficiente. Programa Revista do Campo 21/05/14 - RIT TV (9:45min até 14:05min) Tecnologia do Campo/Canal Rural (31/05) Evento reúne, até esta sexta (6), 110 participantes, dos quais 95 vão apresentar trabalhos científicos englobando 12 temas - agricultura de precisão, agroenergia, biotecnologia, genética e melhoramento animal, genética e melhoramento vegetal, instrumentação agropecuária, meio ambiente, manejo e conservação do solo e da água, novos materiais e nanotecnologia, produção animal, produção vegetal, pós-colheita e qualidade de produtos agropecuários, sanidade animal. Conexão Ciência: EXIBIDO EM 01/07/2014 - A agricultura de precisão é uma forma de manejo integrado de informações e tecnologias que visa o gerenciamento mais detalhado do sistema de produção agrícola como um todo. A prática tem por objetivo reduzir os custos de produção, diminuir a contaminação da natureza pelos agrotóxicos utilizados e aumentar a produtividade. O pesquisador da Embrapa, Ricardo Inamasu, fala sobre o assunto no Conexão Ciência. A agricultura de precisão é uma forma de manejo integrado de informações e tecnologias que visa o gerenciamento mais detalhado do sistema de produção agrícola como um todo. A prática tem por objetivo reduzir os custos de produção, diminuir a contaminação da natureza pelos agrotóxicos utilizados e aumentar a produtividade. O pesquisador da Embrapa, Ricardo Inamasu, fala sobre o assunto no Conexão Ciência, Entrevista com o presidente da Embrapa, Maurício Lopes, que falou sobre o importante papel da entidade no desenvolvimento técnico e científico para o setor pecuário. A técnica de agricultura de precisão possibilita a produção sustentável, além do aumento da produtividade, por meio do monitoramento de aspectos essenciais nas lavouras, bem como o uso racional dos insumos agrícolas, reduzindo os custos de produção A Embrapa Instrumentação apresenta ao público um drone, veículo aéreo não tripulado, que poderá ser utilizado por pequenos produtores na gestão da lavoura. Tecnologia de informação é aplicada para o aumento da produção rural e se torna um dos destaques de evento anual promovido por federação Do clima à energia e o turismo (incluindo a agricultura de precisão), o presidente da Embrapa listou oportunidades que podem alavancar o crescimento do país em um futuro próximo. Agricultura de precisão, drones e outras inovações agrícolas, feira de máquinas e comercialização de produtos e serviços fazem parte das atrações Com aporte de R$ 7 milhões, entidade envolveu 20 de seus Centros de Pesquisa e empresas, com adesão de 214 pesquisadores e 15 unidades experimentais Noticias relacionadas ao tema do projeto Conheça as publicações da Equipe da Rede AP A Rede AP tem 15 áreas experimentais distribuídas no Nordeste, Centro-oeste, Sudeste e Sul do país, cobrindo culturas anuais (milho, soja, trigo, arroz irrigado e algodão) e culturas perenes (eucalipto, pinus, uva, pastagem, cana-de-açúcar, laranja, maçã e pêssego). Mapa esquemático da localização das UPs no Brasil Cristalina - GO Pelotas - RS Matão - SP Mogi Mirim - SP Bagé - RS Dourados - MS Vacaria - RS São Carlos - SP Morro Redondo - RS Rio Negrinho e Doutor Pedrinho - SC Planaltina de Goiás - GO Castelândia - GO Guarapuava - PR Não-Me-Toque - RS Petrolina - PE Bento Gonçalves - RS University of Nebraska-Lincoln Este é o espaço de trabalho criado pela comunidade gvSIG Brasil. The International Society of Precision Agriculture (ISPA) is a non-profit professional scientific organization. The mission of ISPA is to advance the science of precision agriculture globally. As melhores soluções agropecuárias em TI desenvolvidas pela Embrapa, reunidas aqui. Agricultura sustentável: alimentando o presente para garantir o futuro Uma iniciativa para inserir o Brasil no esforço internacional de padronização de comunicação entre tratores e implementos agrícolas. Agricultura de Precisão Embrapa Instrumentação Agropecuária Rua XV de Novembro, 1452 São Carlos-SP Telefone (16) 2107-2804 - Fax (16) 2107-2902",4670267857749552625,2966,Wikipedia
120,6023609667389715259,ERITREA,Five Bitcoin and Ethereum Based Projects to Watch in 2016 - Blockchain Investment Vehicles,"Five Bitcoin and Ethereum Based Projects to Watch in 2016 Git Money (Bitcoin) (Crowdsourcing) Git Money allows anyone to earn money by solving open issues on GitHub. Repository owners put up bounties for tasks and the reward is automatically paid to whoever submits the first successfully merged pull request. No need for interviews, contracts, or trust. So far 44 bounties have been claimed, for translating web pages, creating graphics/videos, writing a Medium article, and various programming tasks. Of the projects on this list, Git Money is the only production ready application with high potential for near term adoption. Amazon's Mechanical Turk has shown that crowdsourcing platforms are useful, and the model is a great fit for GitHub, the home of many cryptocurrency based projects. Git Money was designed using a 21 Bitcoin Computer . ""We've said it before and we'll say it again - it doesn't matter where you're from or who you are. We believe in the right to work anywhere on whatever you want. It doesn't matter your gender, your education, your social or spiritual beliefs or your skin color."" Augur (Ethereum) (Prediction markets/forecasting) Augur is a decentralized prediction market where users wager on the outcome of future events. There is plenty of research( 1 , 2 ) demonstrating the value of centralized markets as forecasting tools and Augur aims to address their shortcomings. Check out the Beta to browse example markets and test the platform with play money. We're unlikely to see a final product before the year's end, but the team has been making steady progress since crowdfunding over $5 million last fall. Microsoft will be offering a solution for companies to run private markets through their Azure Blockchain as a Service (BaaS) platform. TransActive Grid (Ethereum) (Utilities/renewable energy) TransActive Grid is allowing neighbors to purchase and sell renewable energy among each other, offering communities with microgrids a way to create a local energy market while reducing emissions and pollution. This is enabled by attaching solar panels to computers that track and log the creation of energy onto Ethereum's blockchain. The first implementation is being tested with participants of the Brooklyn City Microgrid project. For details see recent features in The New Scientist , Vice , CleanTechnica , and the team's video presentation . Yours (Bitcoin) (Content sharing/social media) Yours is a decentralized content sharing application where users will earn money for their submissions. When you ""endorse"" (think Reddit upvote or Facebook like) a post, you send the author a small microtip from your account's Bitcoin wallet. Endorsing a post also gives you the chance to be rewarded with a portion of the author's following tips, this incentivizes the early support of quality content. Zapchain is an existing site demonstrating demand for Bitcoin powered content monetization, and Medium is rolling out features that will allow authors to get paid. Here's a very early stage demo of Yours. ""We're going to enable anyone to get paid for the things they create, and allow the world as a whole to decide who deserves recognition, fame, and fortune."" Slock.it (Ethereum) (Internet of Things) Slock.it GmbH is a German startup working on slocks; software based ""smart locks"" that can be controlled with a smart phone application. In their introductory video , you can see a glimpse of how they hope to disrupt the sharing economy, enabling the renting and selling of anything without a middleman. Microsoft will provide testing and integration tools for businesses through their Azure BaaS. ""If your name is GE, Pearson or 3M - it's the ideal way to get started - with a one-click deployment."" Slock.it is also working with RWE (Germany's second largest utilities provider) on a smart contract powered electric vehicle charging platform, BlockCharge. They recently showcased a working prototype and will be testing the technology with real vehicles and stations over the next year. Slock.it's most ambitious venture is a Decentralized Autonomous Organization (DAO), a company that lives on Ethereum's blockchain whose rules are enforced by software . Imagine a ""super-Kickstarter"" which allowed backers to invest and become permanent stakeholders in projects instead of receiving one time rewards. Anyone can become a shareholder in the DAO by purchasing tokens (with ether) in the upcoming token sale. Shareholders vote on the DAO's business decisions and receive a portion of transaction fees whenever slocks are used. The DAO's existence is supposed to ensure the slock network can continue operating even if Slock.it GmbH goes out of business. Slock.it will submit a proposal to the DAO for development of the Ethereum computer , the device responsible for controlling slocks. If approved by shareholders, the DAO will pay Slock.it using the ether it raised during the token sale. There are nearly 3000 people in Slock.it's rapidly growing Slack channel, many of them eagerly waiting to participate in the DAO experiment.",4340306774493623681,2589,Wikipedia
228,7905485530310717815,SOUTH AFRICA,Blockchain Smart Contracts Startup Selected By BNP Paribas Accelerator - CoinDesk,"CommonAccord, a blockchain-based startup for legal documentation, is one of eight startups selected for BNP Paribas' new FinTech accelerator, L'Atelier. CommonAccord 's goal is to create global codes for transferring legal documents like contracts, consents and permits. The startup wants to develop a distributed network of participants that synchronizes files with each other, using blockchain, GitHub or email transfer. According to CommonAccord, however, blockchain is particularly important for it's ability to automate routine functions using smart contracts and provide an immutable ledger that can be used for legal enforcement. A parser program developed by Primavera De Filippi, an expert in the bitcoin legal space at Harvard's Berkman Center is being used by CommonAccord to support peer-to-peer transactions. BNP Paribas, French multinational bank, set up the accelerator in December to support startups and develop prototype solutions. Cryptocurrency and blockchain-related startups accounted for 3% of the 142 total startups that applied to be a part of L'Atelier's season one. Startups focusing on payments; cybersecurity, compliance and anti-fraud; and portfolio management led the number of applications. Image via Shutterstock Banking BNP Paribas Smart Contracts",4340306774493623681,1448,Wikipedia
251,8194079557551008273,ENGLAND,Using Gamified Hacking Challenges To Attract New Blockchain Developers,"The blockchain ecosystem is always in need of more developers who want to push a fresh spin on the concept of distributed ledgers. Despite several conferences and development workshops around the world, it is rather difficult to attract coders on a larger scale. A new solution by Uber, which gamifies the aspect of hacking challenges, might be something to take note of by Bitcoin and blockchain companies. Also read: Sollywood Aims to Disrupt Traditional Cable TV With Blockchain Hacking Challenges Through Games Scouting for new developer talent is not an easy task, but there are certain tools to make life a bit easier for companies. Hacking challenges are an excellent way to test someone's coding skills in a more relaxed environment, as it gives them a problem for which they can use out-of-the-box thinking to solve it. has been looking at the concept of hacking challenges and decided to experiment with this solution to scout potential hires. These challenges will appear in the Uber app, and are labelled "" Code on the Road "". Considering how this format is presented as a mobile game, it is an excellent way to kill time while being driven around, and it could land users a job. Although people with previous coding experience - especially engineers - will have somewhat of an advantage during these hacking challenges, the gamified concept is open to anyone willing to give it a try. Uber also claims they are not targeting specific users with this game, although the hacking challenges will be rolled out in US cities with a higher concentration of tech jobs. What is of particular interest is how Uber is advertising the hacking challenges as a tool for [aspiring] developers to showcase their skills. It is positive to see such a big company tackle things in a different view, rather than relying on just job interviews, which is not environment catering towards the strengths of developers. All in all, there are three different coding challenges in the game, all of which has to be completed within sixty seconds. Users who score above average can contact Uber directly through the app itself. Doing so will result in them receiving an email with a link to the job application. Interesting Solution For Future Blockchain Development the aspect of blockchain development could be an interesting way for companies to attract potential job candidates. Considering how most consumer son the ""hotter' blockchain startup areas have access to a smartphone, it could be a good fit to integrate such a solution within the existing mobile apps themselves. Granted, companies will be ""fishing in the pool"" of enthusiasts, but that might be for the better as well. There are a lot of bright developer sin the world of blockchain and digital currency, most of whom do coding on the side. Perhaps some of them would be elated to turn their coding skills into a full-time job. What are your thoughts on attracting new blockchain developers through a coding ""game""? Let us know in the comments below! Images courtesy of Shutterstock, Uber Jp Buntinx JP Buntinx is a freelance Bitcoin writer and Bitcoin journalist for various digital currency news outlets around the world. In other notes, Jean-Pierre is an active member of the Belgian Bitcoin Association, and occasionally attends various Bitcoin Meetups in Ghent and Brussels Mobile security for Android users seems to be tough to achieve these days, especially when considering how the sour... Sollywood believes it can achieve its goals of disrupting the $200 billion Cable TV industry by leveraging blockcha... Up-and-Coming dApps platform Lisk has closed its crowdfunding campaign, successfully raising 14,000 bitcoins. This ...",4340306774493623681,2145,Instagram
164,-1672166631728511207,ERITREA,O potencial do bitcoin na América Latina,"28/03/2016 | por Safiri Felix | em Economia As projeções para a economia na América Latina em 2016 não são nada animadoras. Além de agudas crises políticas em alguns dos principais países, como Brasil e Venezuela, a forte queda no preço das commodities que compõem a maior parte das exportações da região graças à desaceleração da economia chinesa, o primeiro parceiro comercial de muitas nações latino-americanas, está provocando fortes recessões. Devido a esse quadro de pessimismo econômico, cada vez mais pessoas e empresas na América Latina passaram a entender e tirar proveito dos grandes benefícios do bitcoin como uma alternativa interessante de investimento e pagamento, especialmente para despesas em moeda estrangeira. No ano passado, a adoção da moeda digital bateu recordes na América Latina. No Brasil, por exemplo, o ano de 2015 movimentou mais de R$ 113 milhões em negócios envolvendo bitcoin, um aumento de 158% em relação a 2014. A cotação da moeda teve 102% de valorização no ano. No México, os negócios aumentarem 600% em 2015. A empresa de processamento de pagamentos BitPay chegou a registrar crescimento de 510% em suas transações no meio do ano que seguiu forte até o final de 2015. Em toda região, o percentual de crescimento nas transações comerciais ultrapassou 1700% ao longo do ano. Os indicadores referentes a 2015 mostram que quem manteve capital alocado em bitcoin no ano passado viu seu investimento performar 92% melhor que o real, 65% superior que o peso mexicano, 41% em peso argentino e impressionantes 400% em relação a moeda venezuelana bolivar. É importante lembrar que a inflação em vários países atingiu níveis recordes no ano passado e a situação não deve melhorar em 2016. Na Venezuela, o índice inflacionário bateu 275% em 2015, enquanto que na Argentina chegou a 30% e no Brasil pouco mais de 10%. O grande expoente da região quando o assunto é bitcoin continua sendo a Argentina, que possui o maior número per capita de entusiastas da tecnologia. O novo presidente argentino, Mauricio Macri, parece ser um entusiasta da tecnologia. Recentemente, o mandatário argentino reuniu-se com o bilionário Richard Branson, acionista do Bitpay, e discutiu aspectos relacionados à moeda digital. No entanto, o caso brasileiro é especialmente interessante para os investidores. Com o crescimento da inflação, diversificar reservas com bitcoin é uma importante alternativa pra retenção do poder de compra e a forma mais ágil de obter moeda estrangeira. O mercado também apresenta avanços importantes em termos de liquidez e casos de uso como os desenvolvidos pela coinBR. O volume negociado diariamente cresce de forma sustentada no primeiro trimestre do ano, apontando perspectivas de que o mercado brasileiro movimentará algo em torno de R$ 300 milhões em 2016. Outro importante motivador para adoção de bitcoins na região tem sido o aumento do controle de capitais. Enviar dinheiro para o exterior utilizando serviços financeiros tradicionais incorre em um custo médio superior a 10%, além da burocracia e tributação crescente. Os usuários de bitcoin latino-americanos não estão utilizando bitcoins apenas para reservar valor ou escapar dos fortes controles de capitais. Muitos também estão aproveitando sua intrínseca eficiência para fazer compras online nos vários e-commerces que já aceitam a moeda digital, também utilizando cartões de crédito pré-pago com recarga em bitcoin, como as soluções do ContaSuper e ADVcash disponibilizadas aos clientes da coinBR. Apesar do forte crescimento na adoção de bitcoins na América Latina, a tecnologia ainda enfrenta alguns obstáculos para decolar. O setor de e-commerce, por exemplo, ainda não ganhou tração parecida como ocorreu na América do Norte e na Europa. Na Venezuela, por exemplo, o governo mandou fechar mineradoras de bitcoin e recentemente utilizou seu canal de televisão estatal para criar uma propaganda contrária ao bitcoin, dizendo que esta é a moeda da deep web e uma forma de evadir divisas do país. Na Bolívia, em 2014, o governo central baniu a utilização de quaisquer moedas que não sejam emitidas pelo banco central do país, incluindo o bitcoin. O mesmo aconteceu no Equador, quando também em 2014, o governo baniu a utilização de moedas descentralizadas. Apesar do cenário desafiador, definitivamente a América Latina figura como uma terra de oportunidades para o bitcoin. Brasil, Venezuela, México e Argentina certamente serão países que irão liderar esse crescimento do uso da tecnologia.",4340306774493623681,2441,Instagram
183,-4374331682165863764,ETHIOPIA,From fine wine to lotteries: Blockchain tech takes off - BBC News,"Imagine a world where you can vote in an election with your phone, where your buy a house in a matter of hours, or where cash simply doesn't exist. These are some of the scenarios being mooted by an increasingly excited blockchain community. The technology that underpins the cryptocurrency Bitcoin is nothing new - it's been around for decades. It's just an encrypted database that that's distributed across a computer network. But what makes it different is it can only be updated when everyone on that network agrees, and once entered the information can't be overwritten, making it extremely secure and reliable. And trust, as we know, underpins most business transactions. ""Blockchain, for perhaps the first time, presents a legitimate threat to the status quo,"" says Terry Roche, head of financial technology research at financial advisory firm, Tabb Group. The tech has spawned a new generation of start-ups looking to find new applications, from peer-to-peer lending to smart contracts. No restrictions For example, OpenBazaar is a way people can sell anything to anyone, anywhere in the world using bitcoins. Unlike with eBay or Amazon, users don't visit a website but download a programme that directly connects them with other potential buyers and sellers. ""Our goal is to unbundle the incumbent marketplaces around the world by offering a more private, secure and flexible option that isn't controlled by any one corporate interest, but rather, by the users themselves,"" developer Brian Hoffman tells the BBC. According to OpenBazaar, cutting out the middleman means there are no fees, no restrictions, no accounts to create, and you only reveal the personal information you feel comfortable sharing. The software is now in its testing phase and has been downloaded nearly 20,000 times in the last three weeks. Smart contracts Another major development exciting the industry are smart contracts, programmes that can automatically verify that contract terms have been met, and, once that has been done, authorise payment - all in real time without any need for middlemen. The results are then indelibly recorded in the blockchain database. Some believe - perhaps fancifully - that such contracts could remove the need for lawyers one day. Companies like San Francisco-based SmartContract and Hedgy are already building businesses based on the concept, which could have applications in the financial, property and commerce markets. By incorporating smart contracts with the ""internet of things"" (IoT)- smart devices hooked up to the internet - blockchain tech could have uses far beyond the financial sector, says Emmanuel Viale, a managing director of Accenture's Technology Labs. ""You could have a wearable fitness tracker that would send the number of calories or steps taken to the blockchain. The data is encrypted and my identity is anonymised. The same with home medical devices,"" he says. ""The Blockchain would create a link with health professionals - whether coaches, doctors or healthcare institutions - and the smart contract could trigger needed services - whether it's a fitness regime or treatment for a chronic disease."" In another example, start-up Hellosent thinks smart contracts and IoT devices could be used to monitor deliveries of fine wines. Sensors would continuously measure the temperature and humidity of the wine in transit, then if either fell below agreed levels as recorded in the smart contract, the purchase would be automatically cancelled. Blockchain platform Some firms are even developing their own version of blockchain. Ethereum , for example, is a blockchain-based decentralised platform that developers can use to create their own applications, cryptocurrencies and virtual organisations. Co-founder Mihai Alisie says giants such as IBM, Microsoft and Samsung have already started researching its possible uses. ""These new applications put users in full control over their data, privacy and funds, making possible an ecosystem of web apps that can be trusted... since the code powering these applications is transparently running on the Ethereum blockchain,"" he says. There are developers on Ethereum creating all sorts of new projects, from lotteries to financial options exchanges, peer-to-peer home-produced energy trading platforms to games. ""From financial systems to governance systems and everything in-between, there is a brave new world awaiting to be imagined and built. I call this the crypto renaissance,"" says Mr Alisie. Predicting the future? One intriguing piece of software based on the Ethereum platform is Augur. Sounding somewhat reminiscent of the Borg from Star Trek, it harnesses ""the wisdom of crowds"" - collective intelligence - to predict the outcome of future events. ""The goal is to create a global prediction market platform that would enable the expertise of anyone to be utilised in markets, creating what we believe will be the most accurate forecasting tool in history,"" says Tony Sakich, Augur's director of marketing. Still in its development phase, it has now been released for Beta testing. Of course, many of these blockchain start-ups may never get off the ground and some of the predictions for the technology seem far-fetched, to say the least. At Davos earlier this year, one head of an European bank thought blockchain could sound the death-knell for cash. As venture capitalists pile in, increasing investment in blockchain-related firms from $3m to $474m (£335m) from 2011 to 2015, there is the risk of a dotcom-style bubble. There is also the issue of diminishing network capacity - blockchain database management takes up a lot of computing power. Blockchain boss Peter Smith has warned that the technology could ""run out of rocket fuel before the ship reaches orbit"", as complaints about slowing transaction speeds reach record levels. That said, there is a lot of innovation going on and Peter Loop, head of innovation and disruptive technologies at Infosys Finacle, concludes: ""Blockchain as a commercially viable technology remains on an unclear path. ""But it's coming quickly. It's not 10 years away. It's coming within a few years."" Follow Technology of Business editor @matthew_wall on Twitter .",4340306774493623681,1615,Instagram
177,5714314286511882372,GERMAN,Como Startups e Grandes Empresas podem colaborar para Inovar mais e melhor,"Muito tem se falado sobre como grandes empresas podem aproveitar as startups para inovar, mas pouco tem se discutido como essas startups podem se alavancar, aproveitando o que as grandes empresas tem de melhor. Participe do evento e descubra como inovar mais e melhor. Participe dodiretor de Marketing e Inovação da Tecnisa evento gratuito de Maximiliano Carlomagno , sócio-fundador da Innoscience com participação de Romeo Busarello, Egon Barbosa e Igor Piquet, em parceria com o Com participação de: -Maximiliano Carlomagno, sócio fundador da Innoscience -Romeo Busarello, Cubo Network e descubra Como startups e grandes empresas podem colaborar para inovar mais e melhor . Potencialize sua startup no relacionamento com as grandes empresas!",1895326251577378793,2955,Facebook
190,-8141818108252244664,ERITREA,Pimp My Pitch: como fazer um Pitch poderoso,"Descrição do evento Qual horário? 14h00min às 18h30min ""Google provides access to the world's information in one click Larry Page e Sergey Brin: pitch para a Sequoia Um pitch de sucesso depende muito mais do que conhecimento sobre a sua startup. A maneira como uma informação é transmitida tem um impacto profundo na compreensão da mensagem e pode ser a diferença entre um investimento de sucesso e ser obrigado a continuar no bootstrapping. Nestes 2 dias de imersão você aprenderá os elementos essenciais que tornam as apresentações excelentes. Com uma dinâmica constante entre teoria, técnica e prática, iremos analisar pitches e apresentações de sucesso. Além de demonstrar através de casos reais, quais são as abordagens mais eficazes para conquistar o seu público. Do roteiro, passando por storytelling, design dos slides até o treinamento do pitch, você aprenderá o passo a passo para estruturar um pitch de alto nível, preparar um deck impactante e se apresentar com desenvoltura e confiança. Você também será convidado a entender um pouco dos segredos das grandes apresentações do TED e de lições do , além de descobrir o que Shark Tank , Steve Jobs e podem te ensinar sobre como criar apresentações inspiradoras e memoráveis. Aristóteles Disney TÉCNICAS DE PALCO - Como os gestos, seu tom de voz, seu olhar e sua postura podem fortalecer a comunicação da sua mensagem? Ao longo do curso, você terá a oportunidade de trabalhar o seu pitch e levá-lo a um novo patamar com base nos conhecimentos aprendidos e nos feedbacks das atividades. E principalmente: você poderá aplicar essa nova habilidade em situações variadas do cotidiano empreendedor que vão muito além do investimento, como prospecção de clientes, contratação de funcionários, negociação com parceiros e apresentações internas. Sua comunicação pode transformar sua startup.",1895326251577378793,2229,LinkedIn
254,-8939172344092554931,ERITREA,Google DeepMind Is Now Analysing Magic And Hearthstone Cards,"With retro games and Go well-conquered , where is an artificial intelligence like Google DeepMind meant to turn next? Magic: The Gathering and Hearthstone , obviously. Before you get too excited (or maybe insanely depressed as you imagine a toaster holding aloft the Magic World Championship trophy on its ejection lever), there are no plans to set the AI loose on playing these popular card games. At least not yet. For now, the folks over at Oxford University are happy enough for DeepMind to analyse card data and transform it into code . Essentially, the task it is being set is one of translating the data from human to machine speak and while the cards have their own game ""language"" and structure, they can certainly throw some curveballs. Here it is explained in their words: Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. For example, elements such as a card's resource cost never really change in nature and are easily deciphered, however, a card's text might specify that the cost is increased or reduced based on another condition. As you can imagine, writing a program that can analyse and account for these changes in card logic and translate them into arbitrary code is no trivial task. Rather than write the mother of all if/else statements, they've resorted to the use of DeepMind instead. By giving it enough data - all eleventy billion or so Magic cards, say - the AI can learn the ""language"" of card text to produce more accurate results. Apparently, it does a decent job on Hearthstone , though it still stuffs up: The card itself is on the left. On the right, the code DeepMind has generated based on its text. It handled the (relatively speaking) straightforward effect of the Madder Bomber fine, but the more specialised Preparation confused it. Which is fair enough - going by the number of times professional players have screwed up the cast order of Preparation , we can forgive DeepMind for getting it wrong. The researchers mention that the reason ""Madder Bomber"" was treated correctly was because it had ""captured"" the difference between a similar card: The ""Madder Bomber"" card is generated correctly as there is a similar card ""Mad Bomber"" in the training set, which implements the same effect, except that it deals 3 damage instead of 6. Yet, it is a promising result that the model was able to capture this difference. Yep, it's getting the hang of it all right. At this stage I'd be worried about DeepMind getting addicted to Hearthstone and blowing all of Google's cash on packs. That would be a lot of packs. This story originally appeared on Kotaku Australia .",-1443636648652872475,2027,Facebook
166,-1591454024897803197,ERITREA,How OKCupid Changed Hiring Forever.,"Twenty years ago, if you wanted to find a specific person - whether it was the perfect romantic counterpart or an ideally suited future employee - you could expect to spend months searching for anyone who simply qualified. The advent of the internet changed that completely. Suddenly, a tsunami of potential new candidates opened up for both personal and professional searches, and both lovers and recruiters had more options than they'd ever dreamed existed. The introduction of the web meant that people from anywhere in the world could connect with millions of new people from anywhere else, and conversations or relationships that would have been unfathomable before were suddenly possible. Dating sites started to spring up left and right to help connect potential matches, and traditional recruiting practices were entirely upended (think: physical job boards going digital). But this rush of new options created an entirely new-but equally problematic-challenge: too much information. The Problem With Too Much Data. In hiring, and also in love, there can be too much of a good thing. The average job listing elicits 250 applicants , while Tinder, for example, generates 1.4 billion swipes (potential matches) per day. With volume that high, quality control becomes nearly impossible, and people end up discarding candidates (either romantic or professional) without any real review, or else giving up entirely. For dating, you're left sorting through a lot of... less-than-desirable options. For hiring it means that dozens of qualified, potentially incredible workers get overlooked while recruiters are forced to waste hours digging through irrelevant resumes or following false leads. Most recruiters end up either relying on referrals or just seizing upon the first people who somewhat conform to the job description , even if they aren't truly the best fit. The system is incredibly inefficient, and the cost to an organization is substantial. So how do you contend with this overwhelming influx of potential new matches? That's where OKCupid's technical breakthroughs set the stage for a new way of managing massive amounts of information. There were other popular dating sites at the time of its rise to popularity, but OKCupid was the one that developed a reputation for its innovative use of data science and groundbreaking algorithm . The company also ran a notorious (now defunct) data blog in which it used its datasets to analyze romantic trends. Using user-sourced data and a single simple formula, OKCupid was able to pair each user with his or her most compatible dating prospects. Data science enabled it to turn that data into incredibly valuable (and interesting!) insights about the landscape of modern love-which is, after all, its operational industry. It was the one of the first and most public uses of smart new technologies and information as a way of helping people find people. What Automation and Data Science Do For Hiring. Just like dating, hiring is an incredibly specific process, with several similar steps. Hiring managers need to find candidates that are not only ""appealing"" (experienced, highly qualified, etc.), but also a fit for that particular company. Do the candidate's strengths round out the team's weaknesses? Could he or she promote the values of the company? Does the hire promote a higher diversity of experience ? All valid considerations, but difficult to balance in an extremely competitive hiring market with a flood of prospects. Recruiters need to operate quickly and with precision. They need to be confident in their candidate choices and equipt with the information to engage with them and draw them in as quickly as possible. Automation , like the kind used by OKCupid, helps them do that. There's enough publicly available information about any potential candidate on the internet to make strategic sourcing exceptionally easy. If software can combine data from social networks, blogs, work portfolios, and other sources, it can generate a pretty thorough picture of a person in nanoseconds. Then, it can present recruiters with their ""top matches"" before they even start the sourcing process. That type of automation has played a key role in defining the modern hiring market. Sourcing, engaging, onboarding, growth... Everything has gotten much, much, much faster as a result. Hiring competition has reached an all-time high, and it's largely because organizations can operate that much more efficiently. The second component of OKCupid's major contributions - analytics and data science - has proven equally critical. That's because speed on its own isn't enough. In a world where the average cost of a misplaced hire is 2.5 times that employee's annual salary , regular mistakes aren't sustainable. Recruiters have to completely understand the industry landscape, the candidates, the needs of their own organization, and how each candidate would fit in. The right strategically-designed analytics solution can help them do that. We're living in a brave new world with more connections, more interactions and more potential relationships. It can feel overwhelming at times, but ideally what it ultimately means is that we're all that much closer to finding our perfect match - whether that means meeting a partner or finally landing that one hire who takes your whole business to the next level. Robert Carroll currently serves as the Senior Vice President of Marketing for Gild , where he is responsible for crafting and executing Gild's marketing strategy including brand, sales enablement, press and analyst relations, events and demand generation programs.",-8606085472606356565,2843,LinkedIn
262,-6874540813378776198,ENGLAND,The Hierarchy of Engagement - Greylock Perspectives,"The Hierarchy of Engagement The Fuel to Build an Enduring, Billion Dollar Business Building an enduring, multi-billion dollar consumer technology company is hard. As an investor, knowing which startups have the potential to be massive and long-lasting is also hard. From both perspectives, identifying companies with this potential is a combination of ""art"" and ""science"" - the art is understanding how products work, and the science is knowing how to measure it. At the earliest stages of a company, it comes down to understanding how a product is built to maximize and leverage user engagement. I think of user engagement as the fuel powering products. The best products take that fuel and propel the product (and with it, the company) forward. Just how products do that is something I've been thinking about for most of my career. At Nir Eyal 's Habit Summit this week, I presented a framework for how I evaluate non-transactional consumer companies I'm looking to invest in that synthesizes some of this thinking - I call it the Hierarchy of Engagement. The hierarchy has three levels: 1) Growing engaged users, 2) Retaining users, and 3) Self-perpetuating. As companies move up the hierarchy, their products become better, harder to leave, and ultimately create virtuous loops that make the product self-perpetuating. Companies that scale the hierarchy are incredibly well positioned to demonstrate growth and retention that investors are looking to see. I encourage you to use the Hierarchy of Engagement framework when thinking about your own product, and building out your product roadmap. But, like most frameworks, I am continuously improving the hierarchy and would love to hear your thoughts. Let me know what you think. Follow me on Twitter @sarahtavel .",-108842214936804958,2358,Wikipedia
230,-3173020603774823976,AMERICA,Welcome Google Cloud Platform!,"Google Cloud Platform joined the Node.js Foundation today. This news comes on the heels of the Node.js runtime going into beta on Google App Engine , a platform that makes it easy to build scalable web applications and mobile backends across a variety of programming languages. In the industry, there's been a lot of conversations around a third wave of cloud computing that focuses less on infrastructure and more on microservices and container architectures. Node.js, which is a cross-platform runtime environment that consists of open source modules, is a perfect platform for these types of environments. It's incredibly resource-efficient, high performing and well-suited to scalability. This is one of the main reasons why Node.js is heavily used by IoT developers who are working with microservices environments. ""Node.js is emerging as the platform in the center of a broad full stack, consisting of front end, back end, devices and the cloud,"" said Mikeal Rogers, community manager of the Node.js Foundation. ""By joining the Node.js Foundation, Google is increasing its investment in Node.js and deepening its involvement in a vibrant community. Having more companies join the Node.js Foundation helps solidify Node.js as a leading universal development environment."" Along with joining the Node.js Foundation, Google develops the V8 JavaScript engine which powers Chrome and Node.js. The V8 team is working on infrastructural changes to improve the Node.js development workflow, including making it easier to build and test Node.js on V8's continuous integration system. Google V8 contributors are also involved in the Core Technical Committee. The Node.js Foundation is very excited to have Google Cloud Platform join our community and look forward to helping developers continue to use Node.js everywhere.",-1032019229384696495,1533,LinkedIn
149,-9107331682787867601,GERMAN,Hopper raises $16 million for a travel app that tells you the best time to fly,"Hopper , the makers of a handy travel application that tells you the best time to fly in order to find the best deals, has now raised additional capital to continue to grow its business. It has also scored a partnership with American Airlines which allows it to sell AA's tickets through its app. In terms of the new investment, the company announced $16 million in a growth funding round, led by BDC Capital IT Venture Fund. Existing investors, including OMERS Ventures, Accomplice (formerly Atlas Venture) and Brightspark Ventures, also participated. That brings the startup's total raise to date to $38 million. Though the travel space is rife with competition, Hopper has developed a useful application for those looking for an easier way to figure out when to fly in order to save money on airfare. Its prediction engine uses data and analysis from ""billions"" of tracked flight prices to suggest when travelers should buy tickets. The company claims that it's capable of saving customers up to 40% on their next flight, thanks to its service. While ever-changing airfare ticket prices are a complex thing to track, this data is presented to Hopper's end users in an easy-to-read format by way of Hopper's app. Here, customers can view color-coded calendars which shows which days are affordable (green) to travel, all the way up to expensive days (red.) You're also able to watch trips, get detailed forecast data, receive alerts when fares drop and more. The end result for customers is being able to snag a ticket without overpaying - though having more flexible travel dates, of course, helps. What's also interesting about Hopper is that it's entirely focused on delivering its product by way of a mobile app, not a website. According to Hopper's CEO and founder, Frederic Lalonde, previously a VP at Expedia, the company isn't a ""mobile-first"" startup - it's ""mobile- only ."" ""We fundamentally believe that the mobile app experience in one shape or form is going to take over all commerce,"" he says. ""A hundred percent of what we do is based on mobile. And beyond that, we're very much a conversational company in the sense that 90 percent of what we sell comes from a push notification."" In other words, when Hopper alerts its users that now is the time to buy, they do. While the company doesn't disclose its revenue, Lalonde would say that everything related to its service is growing at roughly 40 percent month-over-month, and he believes the app will reach 1 million downloads per month this summer. Hopper is still a relatively young company. It first launched in early 2015, and was named the #7 best app by Apple that same year. It's also a #4 travel app in the U.S. on the App Store, and #1 in dozens of countries. To date, the app has been downloaded over 3 million times, and has monitored fares for over 5 million watched trips worth over $4.5 billion in gross booking value. It has also sent over 38 million push notifications to users, with 7.3 million being sent last month alone, the company reports. Hopper has increased its accuracy since its debut, too, and now claims it's 95 percent accurate in terms of predicting ticket prices within $5 dollars of the actual fare up to a year ahead. But Lalonde point out that, while Hopper has improved its performance 3 percent since last year, it has done so in areas that matter - like holiday travel, for example. In addition to the new funding, Hopper has also now signed a deal with American Airlines that will make American Airlines and American Eagle fares available in its app. This includes around 6,700 daily flights to nearly 350 destinations in over 50 countries. This is notable because AA was one of the few holdouts to yet offer its fares in Hopper The app today supports almost all major airlines' fares. The company's business model involves a combination of compensation from airlines, and a $5 convenience fee charged to consumers. With the new capital, Hopper plans to grow its team of 25 in Montreal and Cambridge. It's soon rolling out a number of new features that will allow it to push recommendations to users and offer a more personalized experience. For instance, users will be able to specify things like a desire to avoid long layovers or the ultra-low cost fares in favor of regular tickets, and the app will suggest things users may not know to look for - like alternative airports, for instance. The startup also plans to increase its focus on international travel by year-end, with a focus on more accurate pricing, no matter where users book.",-1032019229384696495,1118,LinkedIn
179,-1021685224930603833,ETHIOPIA,Indústria 4.0: desafios e oportunidades,"*Igor Schiewig 25/03/2016 - A Indústria 4.0 é muito mais do que o uso de tecnologias como Internet das Coisas (IoT) e Computação em Nuvem nos processos de produção. Ela é caracterizada por modelos de negócio realmente inovadores, que gerem vantagens competitivas significativas e sustentáveis para as empresas. O que realmente vai assegurar diferenciais competitivos sustentáveis às empresas não é a simples adoção da Indústria 4.0, mas a capacidade de utilizar seus recursos para criar experiências únicas e inovadoras para seus clientes (Manufatura na Era da Experiência). Estas experiências, que poderão ser aceleradas e levadas a novos patamares, serão o segredo do sucesso das empresas nesse novo ambiente de negócios. *Igor Schiewig, Diretor de Business Transformation da Dassault Systèmes para a América Latina",4670267857749552625,1368,Facebook
240,1392715980907132808,JAPAN,MedStar Washington Potentially Affected By Bitcoin Ransomware,"There are rumors circulating this healthcare institution is affected by Bitcoin ransomware, as one staffer mentioned how she saw a pop-up on two different computer screens. In this pop-up windows, there was information about the infection, and instructions to pay a ransom through ""some form of Internet currency"". Those details have not been officially confirmed at the time of publication, though. Another hospital in the United States has fallen victim to a virus bringing their services to a halt. MedStar Hospital Center, located in Washington, noticed the virus intrusion early Monday morning, resulting in email services being shut down, and having no access to their vast database of patient records. This matter has piqued the interest of the FBI, who are currently investigating the matter to determine whether or not Bitcoin ransomware is involved in this attack. Also read: Antonopoulos and Uphold Get into a Spat over Trademarked Saying MedStar Washington Hospital Center Faces Virus Threat Given the number of hospitals being by Bitcoin ransomware over the past few months, it only seems to make sense this issue plaguing the MedStar Washington Hospital Center should be classified in the same category. However, it remains unclear as to whether or not Bitcoin ransomware is the cause of this attack, as there has not been an official statement just yet. What we do know is how no information appears to be stolen from the hospital database, which is cause for a sigh of relief. Healthcare institutions have a vast amount of personal details about their patients, and all of these details could be of high value to individual internet criminals. Luckily, this is not the case here, or so it would seem. MedStar has to be applauded for their course of action, as the decision was made to shut down all system interfaces. Doing so effectively all access paths for this virus to spread itself through the entire hospital IT system. That being said, all of the clinical facilities will remain open although there will be some delays due to staff reverting to paper records for the time being. Keeping in mind how MedStar operates ten different hospitals, as well as over 250 other facilities in the Washington region, this virus attack appears to be made deliberately against such a prominent healthcare provider. With lab results delayed by quite a margin, this issue needs to be sorted sooner rather than later, so that healthcare services can be restored to normal. Rumors are circulating this healthcare institution is affected by Bitcoin ransomware, as one staffer mentioned how she saw a pop-up on two different computer screens. In this pop-up windows, there was information about the infection, and instructions to pay a ransom through ""some form of Internet currency"". Those details have not been officially confirmed at the time of publication, though. Avast Mobile Enterprise General Manager Sinan Eren stated: ""There's a lack of budget, a lack of talent to handle these issues. Sometimes the human capital might not be there. All these things are an incremental cost to their systems. Therefore, they kind of push the can down the road to deal with technical updates later."" Regardless of what type of virus has infected the MedStar systems, it is evident the healthcare industry still has no answer to lackluster staff awareness regarding software security threats. The combination of staffers being unaware of what they should be doing, and outdated computer systems, result in these types of issues occurring far more often that people would like.",4340306774493623681,2248,Google
212,-6142462826726347616,ENGLAND,CoinFest 2016: Uniting the World's Bitcoiners,"Following the success of its international events spread across three continents and 15 countries, CoinFest, the world's first decentralized currency convention, has announced the launch of CoinFest 2016, claiming more than 20 cities already planning to participate. Founded by a small group of 100 Bitcoiners in Vancouver circa 2013, CoinFest has collaborated with startups , enthusiasts and experts to spur the awareness and mainstream adoption of cryptocurrency internationally. The organization was established to target the growth of the Bitcoin industry and community in Canada . Bitcoin community-focused events Thanks to its success in 2014, CoinFest began to gain popularity for its friendlier top-tier events. Instead of gearing towards a corporate-style conference , CoinFest has been hosting Bitcoin community-focused events, introducing Bitcoin to conventional merchants and the general population. The founder of eMunie, a major sponsor, says to CoinTelegraph in a Skype interview: ""It focuses more on regions where Bitcoin and similar technologies can have a greater impact and really improve the quality of life for a lot of people.It's more friendly and has a real 'by the community, for the community' feel to it, which we like. I've found other conferences and events to be very 'corporate' which can be a little intimidating for newcomers to Bitcoin and other similar technologies."" In-depth discussions and technical applications In 2015, CoinFest hosted its events across 16 cities, growing exponentially in size since 2013, when the organization started by hosting one annual event in Canada. Today, global Bitcoin conferences and events hosted by CoinFest are segregated into sub organizations. They cover local cities or even entire regions, such as newcomer CoinFest Midwest. CoinFest Midwest sponsor phintech.io tells CoinTelegraph: ""Coinfest Midwest is responsible for curating CoinFest along the central corridor of the United States . Our goals are to raise awareness about Bitcoin by hosting events and then educating attendees. Our content is relevant for Bitcoin newcomers just curious about the digital currency and existing fans who want to have in-depth discussions and explore more technical applications . Hopefully this year we'll get to buy and sell goods via Bitcoin, install more Bitcoin wallets for people and send them their first BTC."" A Global Network of High-Profile Startups and Experts with Bitcoin-focused Initiatives Sponsors and hosts of CoinFest Midwest have engaged in exciting events in the greater Omaha area, incorporating a network of high profile startups and experts with bitcoin-focused initiatives. Digital Economy 2015 was one of the most successful CoinFest Midwest events in Omaha last year. The collaboration and partnerships with regional fintech leaders, entrepreneurs, speakers, and panels gathered more than 130 attendees, allowing both entrepreneurs and consumers to explore the innovative nature of new financial technologies and their potential to transform the traditional finance sector. Making it Big Continuing its momentum, 30 cities announced they are planning to host simultaneous events across major cities from April 5-10, including Amsterdam , Copenhagen, Washington DC, Omaha, Toronto , Vancouver, Valdivia (Chile) and Manchester. Currently, CoinFest Amsterdam and Manchester are the most popular conferences out of all regions. Across its events, CoinFest is planning to giveaway 1 BTC in Grabbit prizes, over 40K HUC in Huntercoin prizes and all kinds of prizes on the Wheel of Bitcoin. AirBitz , the official wallet of CoinFest 2016, has announced their support, and will provide mBTC to every Bitcoin beginner who sends out a tweet. Phintech.io says: ""I feel that Asia and Africa will likely become as big, if not bigger markets than the western counterparts. There are huge opportunities for Bitcoin and other crypto currencies in these regions due to the general lack of financial infrastructure available, especially to those living in remote or poor areas.""",4340306774493623681,1149,LinkedIn
236,2255603060224026824,ERITREA,French Senate Will Debate on Bitcoin Regulation,"While the efforts by the French Senate to combat terrorism and organized crime are commendable, the decision to make Bitcoin regulation more strict could end up hurting the local economy more than anything The regulation of Bitcoin and digital currency will be debated upon in France, as the government is taking the risk of money laundering and terrorist funding very seriously. Although there is little to no proof to back up these claims, authorities feel the need to debate further on Bitcoin regulation. Bitcoin's public image of being a currency used primarily by Internet criminals is rearing its ugly head once again. Also read: Bitcoin Price Watch; Sellers Take Control French Senate Ponders Over Bitcoin Regulation Moreover, exchange platforms dealing with Bitcoin and other digital currencies will need to report to - an entity which has always been in favor of Bitcoin regulation - and keep more detailed logs of all customers. Doing so would allow government officials and Tracfin to identify financial fraud and money laundering attempts a lot quicker.",4340306774493623681,2512,LinkedIn
133,3037840448416371691,GERMAN,Bitcoin Wallets as Swiss Bank Accounts: The Developer's Perspective,"Bitcoin was seemingly dragged into the very public debate on privacy and encryption recently. Specifically, President Barack Obama warned that if the government can't access phones, ""...everybody is walking around with a Swiss bank account in their pocket,"" which appeared to refer to cryptocurrency. Last week, Bitcoin Magazine reported on Bitcoin's industry representatives and their positions on encryption, privacy, Bitcoin's role in tax evasion and money laundering and more. In part two of our coverage: What do the actual builders of these pocket-sized ""Swiss bank accounts"" think? Bitcoin Magazine reached out to Electrum developer Thomas Voegtlin, Breadwallet CEO Aaron Voisine, Mycelium developer Leo Wandersleb and Ledger CTO Nicolas Bacca to see where they stand. Broken The debate on encryption and privacy caused by the ongoing dispute between Apple and the FBI took a sharp turn this week. The United States Department of Justice had long claimed it was unable to access encrypted iPhones without help from Apple, but this turned out not to be true. Although the Department of Justice did not explain how they got access to the phone, the Bitcoin wallet developers Bitcoin Magazine spoke to were not surprised that they could. Ledger CTO Nicolas Bacca speculated: ""To get access to the data, the FBI probably relied on some kind of physical attack involving the flash memory. Swapping the flash memory or disabling writes could get you infinite retries, so you can brute force the access code. The operating system doesn't handle that securely."" Bacca also pointed out that Bitcoin itself is much more secure than a typical iPhone. ""I believe Bitcoin is less at risk against physical attacks compared to other cryptosystems, because you always get a way to invalidate a possibly compromised key - just send the coins to a different address if you notice quickly enough. The issue is properly qualifying how long is that."" Balance While the FBI demands Apple help the government agency access encrypted iPhones, the tech company maintains that weakening encryption could result in a privacy disaster. Obama, explaining his position last week, argued it's important to find the right balance between privacy and security, suggesting weakened encryption should be an option. But this option was firmly rejected by all wallet developers Bitcoin Magazine spoke to. Electrum developer Thomas Voegtlin explained: ""In the physical world, you can design a door that is difficult to break. This means that someone may be able to force that door, but not covertly, and that is why we have a balance between privacy and security. But computers are devices that tend to make things binary. In the world of computing, you either do have the key, and opening the door is very easy, or you don't, and it is impossible. If we give a special key to the government, they will be able to open millions of doors with that key, with no effort, and without attracting attention. Nothing will prevent someone from misusing that key, and eventually the key will be leaked and fall into the wrong hands. A technological backdoor is the modern equivalent of the Ring of Gyges ."" Ledger's Nicolas Bacca agreed. ""As a society I believe we should be extremely worried about calls to weaken encryption,"" Bacca said. ""Practically, it cannot be done for a single target, as any ' NOBUS ' backdoor turns into a global risk when it's discovered. Ideologically, we already had a clear demonstration that letting agencies run loose with that kind of absolute power was a pretty bad idea. Politically, I believe it can lead to important economic collateral damages, which is another good reason to avoid doing it."" Voisine agreed with this assesment as well. Moreover, the Breadwallet developer argued that strong encryption is itself a balancing factor against the widespread data monitoring, not a factor that itself requires balance. ""Privacy is core to the human experience. Imagine if your landlord or your extended family knew exactly how much money you had at any given time, and how much you spent and when. It would be a disaster. Privacy is a leveler that allows parties with otherwise unequal bargaining power to negotiate on equal footing. It's even required by law in many situations, such as with the finances of publicly traded firms. Intentionally weakened encryption is absolutely something that we should all be worrying about. In a future world with the potential for ubiquitous surveillance, strong encryption available to individuals will be the counterbalancing force,"" Voisine said. Taxation Perhaps the main reason Obama cited the Swiss bank account example was to point out that strong encryption could allow citizens an easy escape from certain types of taxation. More specifically, Bitcoin users can potentially store significant amounts of wealth on their phones without government agencies knowing about it, or even being able to touch it. Mycelium developer Leo Wandersleb, however, questioned whether that should be considered a problem at all. Wandersleb: ""So Obama is worried that government might not have ultimate power over its citizens' assets? Help me, why again does he assume the right to have that power? I'm not a U.S. citizen, so excuse me if I'm not too firm with regard to the Constitution and its amendments ... but I know of nothing that would say 'all property is yours unless the government doesn't agree.'"" Electrum's Voegtlin took a slightly more moderate position. ""I am not an anarchist, and my involvement with Bitcoin is not motivated by anti-government ideology. I believe in a society with government, with taxes and law enforcement. I write Bitcoin software because I believe that the benefits of cryptocurrency, for society and for our economies, far outweigh the risks. However, we should not be denying there are risks. New technologies always carry new risks."" But the answer to combating these risks is not to encroach on encryption, Voegtlin pointed out. Rather, he believes the risks should be mitigated through alternative means. Voegtlin: ""I think that law enforcement and taxation will need to adapt to cryptocurrency. In 2011, Pirate Party founder Rick Falkvinge proposed that, in a world with Bitcoin, governments should tax consumption, rather than wealth or income. I believe that level of thinking is appropriate."" Voisine, too, believes the answer will eventually be looked for in alternative methods of taxation. ""There are many tax revenue streams that are difficult to avoid even with the leveling power of privacy putting the individual on a more equal footing with the state. Two examples are use taxes and property taxes. As the industry grows and the world moves their wealth into Bitcoin, I think we will see a gradual shift toward more heavy reliance on these types of income streams by the state. This has the added benefit of making the true cost of state services and programs more transparent. Privacy for individuals and transparency for the state is a wonderful thing."" Security Perhaps unsurprisingly, Bitcoin wallet developers have no intention of weakening the security or decreasing the privacy they offer. Rather, most intend to increase both the security and privacy of their products where possible. Mycelium currently uses a server-based model, which means governments could potentially pressure the wallet provider to give up certain privacy sensitive information or provide false transaction data to users. But Wandersleb explained the wallet intends to improve this: ""We are working on removing ourselves from the equation. Our new wallet will not depend on our servers, so there will be no single point of failure. It will also be open source, so even if we were forced to weaken our product, others could choose to distribute reliable versions. Lastly, Mycelium works with hardware wallets that provide a very good protection against broken operating systems."" Bacca's Ledger is one of the companies working with Mycelium to realize this solution. Bacca explained: ""We are building additional security layers directly on the phones when we can, and we're also building a new hardware wallet device, Ledger Blue. This provides open applications development on a Secure Element, which a phone can use over Bluetooth low-energy. That would be close to the hypothetical doomsday device referred to by Obama."" And Voisine, too, emphasized that privacy and security remain top priorities for Breadwallet. Voisine: ""A Swiss bank in every pocket empowers the individual in incredible ways we've never seen before. It's time that this option becomes available to the whole world, not just the wealthy and politically connected, and we are going to give it to them.""",4340306774493623681,1071,Wikipedia
113,7767869406844505704,JAPAN,Slice Labs raises $3.9M to insure on-demand workers when they're on the job,"Tim Attia, co-founder and CEO of Slice Labs , said there's ""a ticking time bomb"" threatening the on-demand economy - namely, insurance and liability. Attia's tackling that problem with Slice, which will offer insurance for on-demand workers and providers, starting with rideshare drivers and then homeshare hosts. The startup is announcing today that it has raised $3.9 million in seed funding from Horizon Ventures and XL Innovate. Don't companies like Uber and Airbnb provide insurance already? They do, but Attia (who's spent more than a decade in the insurance industry ) argued that there's still a significant amount of risk. For example, Uber's coverage changes depending on where the driver is in the process (going online versus accepting the trip versus starting the ride), with the expectation that the driver will have their own personal or commercial policy. He also said Uber's policy is in Uber's name, not the driver's, which could lead to legal complications. ""I'm not picking on Uber and Airbnb - it's the only thing they can do,"" Attia said. Slice, on the other hand, aims to offer new kinds of insurance products designed for on-demand workers. These products will be available on a transactional basis - so a ridesharing driver should be covered from the moment they start driving or get into the car, but they're only paying for coverage during the time that they're working (making it more affordable than just taking out a pricey commercial insurance policy). Technically, another firm is providing the actual insurance, but it sounds like Slice is doing most of the actual work. ""Our product - we price it, we issue it, we bill, we manage claims, but we're not taking risk,"" Attia said. ""We're writing on somebody else's policy, on somebody else's paper. We're doing everything an insurance company does minus the investing."" Attia plans to launch Slice's first products in June. Even though Slice is based in New York City, Attia said he's still deciding where to offer coverage first. He also hopes to work with on-demand services to offer Slice insurance through the service's sign-up process. He added that over time, Slice can use data to improve its products and pricing, for example determining whether ""an uberX behaves more like a person because it's their car, or if they behave more like a cab."" Featured Image: MikeDotta / Shutterstock",-1032019229384696495,1490,Facebook
166,-7202774608580336956,GERMAN,Enable the new Google Contacts for your users from the Admin console,"Last year, we launched the new Google Contacts preview to consumer users with several time-saving improvements, such as a rebuilt ""Find duplicates"" feature, automatic updating of contacts with shared Google profile information, as well as a fresh look and feel. Now, we're pleased to announce that several new improvements have been added at the request of Google Apps users and we're making it possible for Apps admins to enable the Google Contacts preview for their users from the Admin console. Getting started with the new Google Contacts preview This new Admin console setting is off by default, giving admins full flexibility to enable the new Contacts preview on their own schedule. To enable this feature in the Admin console, click on Apps > Google Apps > Contacts> Advanced settings. Once enabled, all users will see the new Google Contacts by default and will have the ability to switch back to the old Google Contacts by using the ""Leave the Contacts preview"" link from the More section of the left-hand navigation menu. After enabling this feature, when users next visit Google Contacts they will see the new UI and a ""warm welcome"" splash page will guide them through the new features. Easily get rid of duplicates: No one likes having duplicate contacts, but they inevitably crop up. So we've rebuilt our ""Find duplicates"" feature from the ground up to provide a quick and painless way to clean up your duplicates. Improved Domain Directory: The directory is easier to navigate through a more intuitive and faster ""infinite"" scroll. Automatically Updated Contacts: Users can keep their contacts up to date automatically with shared information from the domain directory, Google+ (if enabled), and more. In addition, Google Contacts also now supports high resolution photographs. Limitations and switching back to the old Contacts The following features are not yet available in the new Google Contacts: When people try to use these features, they will be prompted with a message that these features are not supported as of yet and presented with a link to go back to the old Google Contacts. To opt-out and switch back to the old Google Contacts, users can click More > Leave the Contacts preview from the left-hand navigation of the Google Contacts preview. Launch Details Release track: Launching to both Rapid release and Scheduled release Rollout pace: Full rollout (1-3 days for feature visibility) Impact: All end users Action: Admin action suggested/FYI More Information Note: all launches are applicable to all Google Apps editions unless otherwise noted FAQ: Contacts preview Launch release calendar Launch detail categories Get these product update alerts by email Subscribe to the RSS feed of these updates",-1387464358334758758,2573,LinkedIn
286,5822211783543822544,ENGLAND,Study Shows Women and Minorities Are Punished for Speaking Up About Workplace Diversity,"A new study finds that people love to hear about workplace diversity-just as long as it's not coming from minorities or women. In fact, if you're not a white male, you're more likely to be punished for speaking up. An Academy of Management Journal study entitled ""Does diversity-valuing behavior result in diminished performance ratings for nonwhite and female leaders?"" explored who really gets to champion diversity without consequences. Stefanie K. Johnson and David R. Hekman , two University of Colorado professors, polled nearly 400 ""working executives"" about, as Fusion explains , ""how cultural, racial, and gender differences were respected and how much diversity was considered a valuable part of their day to day work."" The professors thought their subjects would tell them positive things about workplace inclusion, but received the opposite response. From a piece they wrote in the Harvard Business Review : Much to our surprise, we found that engaging in diversity-valuing behaviors did not benefit any of the executives in terms of how their bosses rated their competence or performance. (We collected these ratings from their 360-degree feedback surveys.) Even more striking, we found that women and nonwhite executives who were reported as frequently engaging in these behaviors were rated much worse by their bosses, in terms of competence and performance ratings, than their female and nonwhite counterparts who did not actively promote balance. For all the talk about how important diversity is within organizations, white and male executives aren't rewarded, career-wise, for engaging in diversity-valuing behavior, and nonwhite and female executives actually get punished for it. Struggling to believe this was the case, the pair re-examined their findings with 300 additional subjects who were ""given fictional hiring decisions for fake jobs that came along with pictures of the hiring manager and the new hire."" They found that if minority or women managers hired a minority or a woman instead of a white dude, the study's participants rated them as ""less effective"": Basically, all managers were judged harshly if they hired someone who looked like them, unless they were a white male. While this is not necessarily shocking, it does explain some of why companies that say they're working on diversifying are often actually resistant to it. Image via Shutterstock .",-5527145562136413747,1530,Google
241,6152652267138213180,ENGLAND,Staying one step ahead at Pixar: An interview with Ed Catmull,"The cofounder of the company that created the world's first computer-animated feature film lays out a management philosophy for keeping Pixar innovative. Ed Catmull has been at the forefront of the digital revolution since its early days. The president of Pixar and Disney Animation Studios began studying computer science at the University of Utah in 1965. In 1972, he created a four-minute film of computer-generated animation that represented the state of the art at the time. In his 2014 book, Creativity, Inc. , Catmull chronicled the story of Pixar-from its early days, when Steve Jobs invested $10 million to spin it off from Lucasfilm, in 1986; to its release of the groundbreaking Toy Story , in 1995; and its acquisition by the Walt Disney Company, for $7.4 billion, in 2006. But even more, he described the thrill and the challenge of stimulating creativity while keeping up with the breakneck pace of the digital age. Catmull recently sat down with McKinsey's Allen Webb and Stanford University professors Hayagreeva Rao and Robert Sutton for a far-ranging discussion that picked up where Creativity, Inc. left off. They delved deeply into Catmull's rules for embracing the messiness that often accompanies great creative output, sending subtle signals, taking smart risks, experimenting to stay ahead of uncertainty, counteracting fear, and taking charge in a new environment-as Catmull did when he became the president of Disney Animation Studios. The Quarterly : One of the questions we had after reading your book is how do you, as the leader of a company, simultaneously create a culture of doubt-of being open to careful, systematic introspection-and inspire confidence? Ed Catmull: The fundamental tension is that people want clear leadership, but what we're doing is inherently messy. We know, intellectually, that if we want to do something new, there will be some unpredictable problems. But if it gets too messy, it actually does fall apart. And adhering to the pure, original plan falls apart, too, because it doesn't represent reality. So you are always in this balance between clear leadership and chaos; in fact that's where you're supposed to be. Rather than thinking, ""OK, my job is to prevent or avoid all the messes,"" I just try to say, ""well, let's make sure it doesn't get too messy."" Most of our people have learned that it isn't helpful to ask for absolute clarity. They know absolute clarity is damaging because it means that we aren't responding to problems and that we will stop short of excellence. They also don't want chaos; if it gets too messy, they can't do their jobs. If we pull the plug on a film that isn't working, it causes a great deal of angst and pain. But it also sends a major signal to the organization-that we're not going to let something bad out. And they really value that. The rule is, we can't produce a crappy film. Strategy in a digital age The Quarterly : So that's the rule; that's the strategy? Ed Catmull: Our real rule is to make a great movie. Our business is predicated on this. Of course, we need the film to be financially successful, and restarting a film is very expensive. But if we're to avoid becoming creatively bankrupt, we have to do things that are high risk. This affects the entire culture-everybody keeps raising the bar, upping the ante in terms of what goes on the screen. This raises costs, so we have a continual struggle to reduce our costs. People coming in from the outside, as well as employees, look at the process and say, ""you know, if you would just get the story right-just write the script and get it right the first time, before you make the film-it will be much easier and cheaper to make."" And they're absolutely right. It is, however, irrelevant because even if you're really good, your first pass or guess at what the film should be will only get you to the B level. You can inexpensively make a B-level film. In fact, because the barriers to entry into this field now are quite low, you can get to B easily. If you want to get to A, then you have to make changes in response to the problems revealed in your first attempt and then the second attempt, et cetera. Think of building a house. The cheapest way to build it is to draw up the plan for the house and then build to those plans. But if you've ever been through this process, then you know that as the building takes shape, you say, ""what was I thinking? This doesn't work at all."" Looking at plans is not the same thing as seeing them realized. Most people who have gone through this say you have to have some extra money because it's going to cost more than you think. And the biggest reason it costs more than you think is that along the way, you realize something you didn't know when you started. Ed Catmull (center) works through story ideas with his team at a retreat for Toy Story 3. The Quarterly : You mentioned signals a moment ago; say a bit more about that. Ed Catmull: Restarting something that doesn't work is costly and painful, but in doing so, we send a major signal to our company. But there are other signals, too. We put short films at the beginning of our movies. Why? Nobody is going to go to a movie because of the shorts, and neither the theater owners nor Disney gets any more money because of them. So why do the shorts? Well, we are sending some signals. It is a signal to the audience that we're giving them more than they're paying for, a signal to the artistic community that Pixar and Disney are encouraging broader artistic expression, and a signal to our employees that we're doing something for which we don't get any money. While they all know that we have to make money and want us to, they also want a signal that we are not so driven by money that it trumps everything else. The Quarterly : Are there any other signals you'd highlight? Ed Catmull: Here is a simple example, so simple that most people would overlook it: our kitchen employees are part of the company. I think a lot of companies overuse the phrase ""our core business""-for instance, ""making food for our employees is not our core business."" So they farm it out. Now, in a lot of companies, including ours, there are certain things you do farm out. You don't do everything yourself. But this notion of ""our core business"" can become an excuse for being so financially driven that you actually harm your culture. If you farm out your food preparation, then you've set up a structure where another company has to make money. The only way they can make more money, which they want to do, is to decrease the quality of the food or service. Now we have a structural problem. It's not that they're bad or greedy. But in our case, the kitchen staff works for us, and because it's not a profit group, their source of pride comes from whether or not the employees like the food. So the quality of food here is better than at most other places. Also, the food here is not free-it's at cost. Making it free would send the wrong signal about value to the kitchen crew. Everybody loves the chef and the staff. We have people who are happier. They're not gone for an hour and a half because they're going somewhere else to get a decent meal. They're here, where we have more chance encounters; it creates a different social environment. That's worth something to us, to our core business. The Quarterly : You said that risk taking is critical to your artistic and, ultimately, your business success. Could you describe how you think about risk at Pixar? Ed Catmull: For me, there are three stages of risk. The first stage is to consciously decide what risks you want to take. The second is to work out the consequences of those choices; this can be fairly time consuming. The third stage is ""lock and load,"" when you do not intentionally add new risk. The trick is to make sure you do stage one-doing something that has risk as part of it. For example, when you're building a team for a film, if you have a team that's worked together before and it's exactly the same team, you know they know how to work with each other and that they can be very efficient. If you keep going this, though, you're going to end up with an ingrown team. On the other hand, if you build a team with all new people, then they won't see looming hazards, and they can fall apart. So you put together a blend. The mix of new and experienced people is a conscious risk taken at the beginning-stage one. The second stage then is getting the group working as a coherent whole for the heavy-duty work at the end of a production. Likewise, with technology, we know that if we don't change the technology from film to film, we can become extraordinarily efficient because everybody knows how to use it. But we also know we'll become out-of-date if we do that. So we introduce new technology. Sometimes it's a small risk and sometimes it's a complete replacement of the underlying infrastructure-a huge risk, with great angst and pain. But our people buy into it because it's for the good of the studio, even though they know it will cause them so much trouble. Similarly, if you consider the stories themselves, they're all hard to make-it doesn't matter whether it's an original film or a sequel. But there are different levels of commercial risk. If we're making a sequel to The Incredibles , it is low commercial risk. It is very hard to make, yet low commercial risk. A sequel to Frozen would be low commercial risk. However, if we make a movie about a rat who wants to cook or a trash compactor that falls in love with a robot, this is high commercial risk. But if we only made low-commercial-risk films, we would become creatively bankrupt. Again, we make conscious choices to assume different levels of risk. This isn't the same thing as risk minimization or spreading risk. In the case of Pixar, every film we have started in the last 20 years, except one, we have finished. These are our babies. Pixar's 2007 Academy Award-winning film, Ratatouille, the story of a rat who longs to be a chef, was praised by critics for its imaginative premise and innovative animation. The Quarterly : In your book, you suggested that Disney Animation fell into a trap like that. Ed Catmull: When Walt was alive, Disney made impactful films. After he died, the quality went down. Then in the '90s, they had four more impactful films- The Little Mermaid , Beauty and the Beast , Aladdin , and The Lion King . At that time, they thought they had found a template to consistently produce good movies. They said ""animation is the new American Broadway."" So every film was a musical with five to seven songs and a funny sidekick, and they kept doing that. Spectacular success doesn't lead to deep introspection, which in turn leads to wrong conclusions. You see this all the time, right? Successful companies draw conclusions about how smart and good they are, and then a significant number of them fall off the cliff because they drew the wrong conclusions. The Quarterly : You said the barriers to entry have fallen in your business. What other big changes are taking place? Ed Catmull: We can all see that technology is changing and, just as obvious, the way people spend their time is changing. One result is that major tentpole movies have become increasingly important because they bring a lot of people into the theaters. These are a great social experience, although I should add that none of us wants to see the smaller films marginalized-they bring a lot of creativity into the industry. It is a real dilemma. Meanwhile, if you look out 10 or 20 years from now, will the changes we are seeing lead to new business models? Change is coming, and the impact isn't clear. In my career, I've gone through many major transitions. If you pay attention, you can get it right about two to four years out. After that, we are doing a lot of guessing. I can see, though, that more people in this industry embrace change than ever before. On the hardware side of things in our business, the technological change, frankly, is driven by the gaming industry. Even though we were the originators of the graphics technology, which they fully acknowledge and are thankful for, we're just not big enough to drive people to design chips for us. So we are fortunate that there's this major gaming industry and that graphics chips keep getting better so we can keep driving forward. But there is nothing stable in this environment. Disney is in the extraordinary position of having three graphics and animation R&D groups-Pixar, Disney Animation, and now ILM [Industrial Light and Magic, acquired by Disney when it purchased Lucasfilm in 2012]. In addition, we have two research groups at major universities to keep driving the technology, as well as research at Disney's Imagineering. Participating in and driving change are taken very seriously. For Disney's latest animated film, Zootopia, animators developed new technology to render the characters' fur accurately, using as many as two million individual hairs for some animals. The Quarterly : So it's about placing a lot of bets and hedging your bets? Ed Catmull: My own belief is that you should be running experiments, many of which will not lead anywhere. If we knew how this was going to end up, we'd just go ahead and do it. This is a tricky issue-people don't want to fail. They put a greater burden on themselves than we intend to put on them. I think it's natural because they never want to fail. One of the things about failure is that it's asymmetrical with respect to time. When you look back and see failure, you say, ""it made me what I am!"" But looking forward, you think, ""I don't know what is going to happen and I don't want to fail."" The difficulty is that when you're running an experiment, it's forward looking. We have to try extra hard to make it safe to fail. The Quarterly : That's fascinating. Experiments are great in retrospect but not in prospect-because you're scared. Ed Catmull: In addition to the asymmetry, there are two meanings to the word ""failure."" The positive meaning is that we learn through failures. But in the real world-in business, in politics-failure is used as a bludgeon to attack opponents. So there is a palpable aura of danger around failure. It's not made up; it's real. This is the second meaning. So we have these two meanings and, emotionally, we can't separate them. And we don't actually call something educational until after it happened. The Quarterly : So what can you do about that? Ed Catmull: On the film side, we are making more experimental films that aren't burdened with the expectation of theatrical release but give us the opportunity to try something riskier. For feature films, we try to make sure that a certain number are ""unlikely"" ideas, which force us to stretch. The Quarterly : It sounds as though you think a lot about fear and how to counteract its corrosive effects. Ed Catmull: Fear is built into our nature; we want to succeed and we respond physiologically to threats-both to real threats and to imagined threats. If people come into an organization like ours and they're welcomed in, what's the threat? Well, from their point of view, they're thinking, ""this is a high-functioning environment. Am I going to fit in? Am I going to look bad? Will I screw up?"" It's natural to think this way, but it makes people cautious. When you go to work for a company, they tell you something about the values of the company and how open they are. But it's just words. You take your actual cues from what you see. That's just the way we're wired. Most people don't talk explicitly about it, because they don't want to appear obtuse or out of place. So they'll sometimes misinterpret what they see. For example, when we were building Pixar, the people at the time played a lot of practical jokes on each other, and they loved that. They think it's awesome when there are practical jokes and people do things that are wild and crazy. Now, it's 20 years later. They've got kids; they go home after work. But they still love the practical jokes. When new people come in, they may hear stories about the old days, but they don't see as much clowning around. So if they were to do it, they might feel out of line. Without anyone saying anything, just based on what they see, they would be less likely to do those things. Meanwhile, the older people are saying, ""what's wrong with these new people? They're not like we were. They're not doing any of this fun stuff."" Without intending to, the culture slowly shifts. How do you keep the shift from happening? I can't go out and say, ""OK, we're going to organize some wild and crazy activities."" Top-down organizing of spontaneous activities isn't a good idea. Don't get me wrong-we still have a lot of pretty crazy things going on, but we are trying to be aware of the unspoken fears that make people overly cautious. If you're just measuring yourself by your outward success, then you're missing a huge part of what drives people. The Quarterly : In light of your experience integrating Pixar and Disney, what do you think a new CEO coming into an existing organization should-and should not-do during the first month or so? Ed Catmull: When we came to Disney, we spent two months just listening. Obviously, John [Lasseter, the chief creative officer of Pixar and Disney] and I were talking with people, doing some coaching and so forth. But we drew no conclusions for two months, about people or anything else. We just watched. The idea is to pay attention to the psychology and the sociology of the people. When you come in and you're the new boss, everybody's rather nervous. They're trying to figure you out, too. So you should start with the assumption that everybody's trying to do the best they can. For me, it's not even putting people on a provisional basis by saying, ""well, we'll see how they work out."" I'm just assuming they're going to work out. When they start to falter, you help them. And it's only after you've tried to help them-and they don't respond after repeated tries-that you do something. Here's another thing that isn't obvious that we tried to be very careful about. Let's suppose somebody doesn't work out. And you, as an experienced person, know fairly soon that they don't have the ability to do the job. If they're leading a team and you've determined they can't do it, what should you do? The normal thing is to say, ""why would I waste people's time by letting a poor leader stay in place?"" We don't say that. The reason is, if we remove somebody as soon as we figure out they can't do the job, we've just induced fear in the other leaders. They don't usually see things as fast as you do because they're focused on their jobs. It makes them think, ""oh, if I screw up, they're going to remove me."" So the cost to the organization of moving quickly on somebody is higher than it is if you let the person go on too long. You make the change when the need for it becomes obvious to other people. Then you can do it. I will admit that there are a couple of times, though, that we waited too long. This is a hard part of managing. The Quarterly : As you look ahead, what worries you? Ed Catmull: Everybody talks about succession planning because of its importance, but to me the issue that's missed is cultural succession. You have to make sure the next level down understands what the actual values are. For example, Walt Disney was driven by technological change and he brought that energy into the company. This was sound and color in the early days of the film industry. Then, in the theme parks, he used the highest technology available to create experiences and animatronics. But after he died, the people left didn't fully understand how he thought. So it fell away from the company, and it didn't come back until Walt's nephew, Roy Disney Jr., used his authority to reintroduce the concept. He insisted on getting into a contract with Pixar, over the objection that our software wouldn't save any money. He said, ""no, I want it because it will infuse energy into animation."" He was very explicit about it-he understood better what Walt was doing. The question is, ""if Walt understood it, why didn't the other people understand it?"" They just assumed that he was a genius, without thinking about what he was actually doing. Thus, the value wasn't passed on. Today, much of our senior leadership's time is spent making sure our values are deeply embedded at every level of our organization. It is very challenging-but necessary for us to continue making great movies. About the Authors This interview was conducted by Stanford University professors Huggy Rao and Robert Sutton and the Quarterly 's editor in chief, Allen Webb, who is based in McKinsey's Seattle office.",-3390049372067052505,2313,Facebook
141,9032993320407723266,ERITREA,"IDEO founder David Kelley talks design, Steve Jobs, cancer, and the importance of empathy","CBS posted an excellent interview of David Kelley this evening. Kelley discusses Steve Jobs at 3:00 and then again at 7:40, but the whole video is definitely worth a watch. A longer Jobs clip and the transcript is below: It is a concept that had its genesis in 1978, when Kelley and some Stanford pals took the notion of mixing human behavior and design and started the company that would eventually become IDEO. One of their first clients was the owner of a fast-growing personal computer manufacturer by the name of Steve Jobs. David Kelley: He made IDEO. Because he was such a good client. We did our best work for him. We became friends and he'd call me at 3 o'clock in the morning. Charlie Rose: At 3 a.m.? David Kelley: Yeah, we were both bachelors so he knew he could call me, right? So he'd call me at 3 o'clock and he'd just like, with no preamble, say, ""Hey, it's Steve."" First, I knew if it was 3 o'clock in the morning, it was him. There was no preamble. And he'd just start- and he said, ""You know those screws that we're using to hold the two things on the inside?"" I mean, he was deep into every aspect of things. Kelley's company helped design dozens of products for Apple, including Apple III and Lisa and the very first Apple mouse, a descendant of which is still in use today. David Kelley: He said to us, ""You know, for $17 make- I want you to-"" He gave us that number $17. ""I want you to make a mouse we're gonna use in all our computers."" So what happened here was we're trying to figure out how to make - so you move your hand and how you make the thing move on the screen. So at first, we thought we gotta make it really accurate, you know? Like when we move the mouse an inch, it's gotta move exactly an inch on the screen. And then after we prototyped it, we realized that doesn't matter at all. Your brain's in the loop! The whole thing was make it intuitive for the human. But even after they solved that monumental problem, Jobs still wasn't satisfied. David Kelley: So he didn't like the way the ball sounded on the table. So we had to rubber coat the ball. Well rubber coating the ball was a huge technical problem because you can't have any seams. You gotta get it just right. And so, you know, it would be just one thing - like that. Charlie Rose: And suppose Steve had said to you ""I'd like to have a ball that's not steel but rubber coated"" and you said ""No, you can't do that Steve."" What would he say? David Kelley: Well the expletives that I would have- are probably are not good on camera, but it was basically, ""I thought you were good,"" you know? Like, ""I thought I hired you because you were smart,"" you know? Like, ""You're letting me down."" It was shortly after that that Steve Jobs came into the picture. For over 30 years they worked together and were close friends. Charlie Rose: What's the biggest misconception about him? David Kelley: I think the big misconception is around that he was kind of like, you know, like malicious. He was like, trying to be mean to people. He wasn't. He was just trying to get things done right and it was- you just had to learn how to react to that. He did some lovely things for me in my life. Jobs introduced Kelley to his wife KC Branscomb. And Steve Jobs was also there for Kelley when the unthinkable happened. In 2007, Kelley was diagnosed with throat cancer - and given a 40 percent chance of survival. Jobs, already suffering from his own deadly cancer, gave him some advice. David Kelley: He came over and said, ""Look, you know, don't consider any alternative - go straight to Western medicine. Don't try any herbs or anything."" Charlie Rose: Why do you think Steve said, ""Don't look for alternative medicine, go straight to the hard stuff?"" David Kelley: I think he had made- in his mind, he had made the mistake that he had tried to cure his pancreatic cancer in other ways other than, I mean, he just said, ""Don't mess around."" You know, when we both had cancer at the same time was when I got really close to him and I was at home, like sitting around in my skivvies, you know, waiting for my next dose of something and I think it was the day after the iPhone was announced. And he had one for me, right? Charlie Rose: An iPhone? David Kelley: You know, your own iPhone, delivered by Steve Jobs, right after it comes out, was a lovely feeling. Anyway, so he decides to hook it up for me. So he gets on the phone to AT&T and he's gonna hook up my phone and it's not going well. Charlie Rose: This is such good news for me. David Kelley: And eventually he pulls the ""I'm Steve Jobs card"" you know, he says to the guy, ""I'm Steve Jobs."" I'm sure the guy on the other end says, ""Yeah buddy, I'm Napoleon."" You know, like get outta here. But anyway- so never did really get it hooked up. Charlie Rose: He never hooked it up? David Kelley: No. Not that day. Charlie Rose: But he was close. What did he teach you about living with cancer? David Kelley: Steve focused more on his kids, I think, than anything. And it made me fight more to survive and so that focus on family you know was something that he taught me. Charlie Rose: You care deeply that you watch your daughter- David Kelley: Yes. Charlie Rose: As she continues to grow. David Kelley: It's about her- what was her life gonna be like if I died? That's really motivating. It was around that time that Kelley decided to commit himself to something even bigger...and why he approached Stanford university and a wealthy client named Hasso Plattner with the idea of setting up a school dedicated to human-centered design. David Kelley: He thought that was a great idea and he said he'd help me. And I said, ""Oh thank you"" and then I went back to the development. Charlie Rose: You had no idea what he meant? David Kelley: No, the development office at Stanford said, ""When a billionaire says 'I'll help you' you should call him back right away."" So turns out, Hasso funded the whole thing. Charlie Rose: $35 million? David Kelley: Yeah, yeah. He said, ""How much do you need?"" And I wish I had said $80 million. He said yes to whatever I said I think. Kelley now runs the groundbreaking and wildly popular Hasso Plattner Institute of Design at Stanford, the ""d. school."" It is recognized as the first program of its kind dedicated to teaching design thinking as a tool for innovation - not just to designers - but to students from all different disciplines. [David Kelley: I think you can follow your noses a little bit around that. Where's the big idea? Where's the excitement?] Twice as many Stanford grad students want to take classes as are seats available. The lucky 500 students in the program augment their master's degree studies in business, law, medicine, engineering and the arts by solving problems collaboratively and creatively, and immersing themselves in the methodology Kelley's made famous. But there are no degrees. It is something Steve Jobs talked him out of. David Kelley: He said, ""I don't want somebody with one of your flaky degrees,"" right? Charlie Rose: I don't want them working for me. David Kelley: Yeah. I don't want them working for me if they just have your flaky degree but if they have a computer science degree or a business degree and then they've come and have our way of thinking on top of that, I'm really excited about it. Today his cancer is in remission. He spends more time doing the things that he cares about most, including tinkering in his workshop with his 15-year-old daughter.",2416280733544962613,1803,LinkedIn
148,7520301770873472812,ETHIOPIA,Aplicativo gratuito permite gerenciar Consórcio Honda diretamente do celular - Notícias - Auto Estrada,"O Consórcio Nacional Honda lançou um aplicativo de autoatendimento para seus clientes, disponível gratuitamente para as plataformas Android e iOS, da Apple. Após instalar o programa, basta ao consorciado cadastrar seu CPF para visualizar os dados de pagamento do boleto eletrônico de uma ou mais cotas. Download do aplicativo pode ser feito diretamente do smartphone Além disso, o aplicativo permite consultar o resultado de assembléias, dar lances e alterar dados cadastrais de consórcios de motocicletas e automóveis da Honda. Ele pode ser baixado nas lojas virtuais Google Play, para dispositivos Android, e Apple Store, para iPhones e iPads. O consórcio da montadora de origem japonesa tem mais de 30 anos de mercado e atualmente conta com mais de 1,3 milhão de clientes ativos. Os grupos têm duração de 18 a 80 meses. Publicado em 28/03/2016",-7421703586506797266,1549,Facebook
145,-5715266554592745669,AMERICA,Robots are coming for your job,"A viral video released in February showed Boston Dynamics' new bipedal robot, Atlas, performing human-like tasks: opening doors, tromping about in the snow, lifting and stacking boxes. Tech geeks cheered and Silicon Valley investors salivated at the potential end to human manual labor. Shortly thereafter, White House economists released a forecast that calculated more precisely whom Atlas and other forms of automation are going to put out of work. Most occupations that pay less than $20 an hour are likely to be, in the words of the report, ""automated into obsolescence."" In other words, the so-called Fourth Industrial Revolution has found its first victims: blue-collar workers and the poor. The general response in working America is disbelief or outright denial. A recent Pew Research Center survey found that 80% of Americans think their job will still exist in 50 years, and only 11% of today's workers were worried about losing their job to automation. Some - like my former colleagues at the CIA - insist that their specialized skills and knowledge can't be replaced by artificial intelligence. That is, until they see plans for autonomous drones that don't require a human hand and automated imagery analysis that outperforms human eyes. Human workers of all stripes pound the table claiming desperately that they're irreplaceable. Bus drivers. Bartenders. Financial advisors. Speechwriters. Firefighters. Umpires. Even doctors and surgeons. Meanwhile, corporations and investors are spending billions - at least $8.5 billion last year on AI, and $1.8 billion on robots - toward making all those jobs replaceable. Why? Simply put, robots and computers don't need healthcare, pensions, vacation days or even salaries. Powerhouse consultancies like McKinsey & Co. forecast that 45% of today's workplace activities could be done by robots, AI or some other already demonstrated technology. Some professors argue that we could see 50% unemployment in 30 years. Deniers of the scope and scale of this looming economic upheaval point hopefully to retraining programs, and insist that there always will be a need for people to build and service these machines (even as engineers are focused on developing robots that fix themselves or each other). They believe that such shifts are many decades away, even as noted futurist Ray Kurzweil, who is also Google's director of engineering, says AI will equal human intelligence by 2029. Deniers also talk about all the new jobs they assume will be created during this Fourth Industrial Revolution. Alas, a report from the 2016 World Economic Forum calculated that the technological changes underway likely will destroy 7.1 million jobs around the world by 2020, with only 2.1 million replaced. With the future value of human labor (read: our incomes) in doubt, what do we do? One way to cushion the economic blow is to reclaim something from the technology realm that we've been giving away for free: our personal data. Companies that sell personal data should pay a percentage of the resulting revenue into a Data Mining Royalty Fund that would provide annual payments to U.S. citizens, much as the Alaska Permanent Fund distributes oil revenues to Alaskans. This payment scheme would start with traditional data - customer, financial and social media information sold to advertisers - but would also extend to future forms of data like our facial expressions and other biometrics. If Google, Facebook or others were profiting from harvesting timber, oil, gold or any other public resource, it would be illegal and immoral for them not to pay for it. The same logic should apply to our data. Profound changes lie ahead with implications beyond our paychecks, to be sure. Ethicists and philosophers already are debating what a world without work might look like. It's clear that no one will escape the outcomes - negative and positive - of this economic and technological revolution. A Data Mining Royalty Fund isn't about helping just the unemployed factory worker who used to earn $20 an hour, the truck driver replaced by self-driving vehicles or the minimum-wage barista. It's about taking steps to guarantee some minimum income to your family, or the one down the block, before any of us are automated into obsolescence. Bryan Dean Wright MORE FROM OPINION Who should be entitled to overtime pay and who shouldn't? Obama's record on foreign policy is incomplete Here's how to rescue local elections from obscurity with one simple change is a former CIA covert operator who resides in Oregon. @BryanDeanWright",-1443636648652872475,2821,LinkedIn
197,8545647269051113523,ERITREA,Google's AI DeepMind Turns its Gaze to Hearthstone and Magic: The Gathering - IGN,"What will it be used for next? Researchers at Oxford University are setting Google's artificial intelligence DeepMind loose on analyzing Hearthstone and Magic: The Gathering playing cards. According to Kotaku , the AI analyzes card data such as resource cost and damage, and turns it into code that a machine can read. Here's the abstract from the paper titled 'Latent Predictor Networks for Code Generation': ""Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. ""Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks."" Basically, they're making code combined from the structured part of cards, e.g. the mana cost, and the natural language part that may change the way the card works. Where DeepMind comes in is how it can learn by analyzing more and more cards how to produce more accurate code. Luckily, there are well over 10,000 Magic: The Gathering cards DeepMind can peruse to get better. Right now for example, it understands how the Hearthstone card Madder Bomber works because it had previously seen Mad Bomber, which works in a similar way. However, for a card like Preparation, it is still struggling to find the actual meaning. The image below shows the cards with associated code, with correct segments in green, and incorrect segments in red. Thankfully for professional Hearthstone players, DeepMind isn't actually playing the game yet, so they're free from the same fate as some of the best Go players in the world . Matt Porter is a freelance writer based in London. Make sure to visit what he thinks is the best website in the world , but is actually just his Twitter page .",-1443636648652872475,1588,Instagram
220,668505019835540303,ETHIOPIA,Bayesian machine learning,"So you know the Bayes rule. How does it relate to machine learning? It can be quite difficult to grasp how the puzzle pieces fit together - we know it took us a while. This article is an introduction we wish we had back then. While we have some grasp on the matter, we're not experts, so the following might contain inaccuracies or even outright errors. Feel free to point them out, either in the comments or privately. This is el drafto. Come back later for la version final. Bayesians and Frequentists In essence, Bayesian means probabilistic. The specific term exists because there are two approaches to probability. Bayesians think of it as a measure of belief, so that probability is subjective and refers to the future. Frequentists have a different view: they use probability to refer to past events - in this way it's objective and doesn't depend on one's beliefs. The name comes from the method - for example: we tossed a coin 100 times, it came up heads 53 times, so the frequency/probability of heads is 0.53. For a thorough investigation of this topic and more, refer to Jake VanderPlas' Frequentism and Bayesianism series of articles. Priors, updates, and posteriors We start with a belief, called a prior. Then we obtain some data and use it to update our belief. The outcome is called a posterior. Should we obtain even more data, the old posterior becomes a new prior and the cycle repeats. This process employ the Bayes rule : P( A | B ) = P( B | A ) * P( A ) / P( B ) P( A | B ) , read as ""probability of A given B"", indicates a conditional probability: how likely is A if B happens. Inferring model parameters from data In Bayesian machine learning we use the Bayes rule to infer model parameters (theta) from data (D): P( theta | D ) = P( D | theta ) * P( theta ) / P( data ) All components of this are probability distributions. P( data ) is something we generally cannot compute, but since it's just a normalizing constant, it doesn't matter that much. When comparing models, we're mainly interested in expressions containing theta, because P( data ) stays the same for each model. P( theta ) is a prior, or our belief of what the model parameters might be. Most often our opinion in this matter is rather vague and if we have enough data, we simply don't care that much. Inference should converge to probable theta as long as it's not zero in the prior. One specifies a prior in terms of a parametrized distribution - see Where priors come from . P( D | theta ) is called likelihood of data given model parameters. The formula for likelihood is model-specific. People often use likelihood for evaluation of models: a model that gives higher likelihood to real data is better. Finally, P( theta | D ) , a posterior, is what we're after. It's a probability over model parameters, including most likely point estimates, obtained from prior beliefs and data. Note that choosing a model can be seen as separate from choosing model (hyper)parameters. In practice, though, they are usually performed together, by validation, for example. Spectrum of methods There are two main flavours of Bayesian. Let's call the first statistic modelling and the second probabilistic machine learning. The latter contains the so-called nonparametric approaches. Statistic modelling Bayesian modelling is applied when data is scarce and precious and hard to obtain, for example in social sciences and other settings where it is difficult to conduct a large-scale controlled experiment. Imagine a statistician meticulously constructing and tweaking a model using what little data he has. In this setting you spare no effort to make the best use of available input. Also, with small data it is important to quantify uncertainty and that's precisely what Bayesian approach is good at. Finally, as we'll see later, Bayesian methods are usually computationally costly. This again goes hand-in-hand with small data. To get a taste, consider examples for the Data Analysis Using Regression Analysis and Multilevel/Hierarchical Models book. That's a whole book on linear models. They start with a bang: a linear model with no predictors, then go through a number of linear models with one predictor, two predictors, six predictors, up to eleven. This labor-intensive mode goes against a current trend in machine learning to use data for a computer to learn automatically from it. Probabilistic machine learning Let's try replacing ""Bayesian"" with ""probabilistic"". From this perspective, it doesn't differ as much from other methods. As far as classification goes, most classifiers are able to output probabilistic predictions. Even SVMs, which are sort of an antithesis of Bayesian. By the way, these probabilities are only statements of belief from a classifier. Whether they correspond to real probabilities is another matter completely and it's called calibration . Still another thing are confidence intervals (error bars). You can observe this in regression. Most ""normal"" methods only provide point estimates. Bayesian methods, such as Bayesian version of linear regression, or Gaussian processes, also provide uncertainty estimates. Credit: Yarin Gal's Heteroscedastic dropout uncertainty and What my deep model doesn't know Unfortunately, it's not the end of the story. Even a sophisticated method like GP normally operates on an assumption of homoscedasticity, that is, uniform noise levels. In reality, noise might be heteroscedastic. See the image below. LDA Latent Dirichlet Allocation is another example of a method that one throws data at and allows it to sort it out. It's similar to matrix factorization models, especially non-negative MF. You start with a matrix where rows are documents, columns are words and each element is a count of a given word in a given document. LDA ""factorizes"" this matrix of size n x d into two matrices, documents/topics ( n x k ) and topics/words ( k x d ). The difference is, you can't multiply those two to get the original, but since the appropriate rows/columns sum to one, you can sample a document. For the first word, one samples a topic, then a word from this topic (the second matrix). Repeat for the number of words you want. Notice that this is a bag-of-words representation, not a proper sequence. This is an example of a generative model, meaning that one can sample, or generate examples, from that model. Usually classifiers are discriminative: they model P( y | x ) , to directly discriminate between classes based on x . A generative model is concerned with joint distribution of y and x , P( y, x ) . It's more difficult to estimate that distribution, but it allows sampling and of course one can get P( y | x ) from P( y, x ) . Bayesian nonparametrics While there's no exact definition, the name means that the number of parameters in a model can grow as more data become available. This is similar to Support Vector Machines, for example, where the algorithm chooses support vectors from the training points. Examples of nonparametrics are Gaussian Processes, and Hierarchical Dirichlet Process version of LDA, where the number of topics chooses itself automatically. Gaussian Processes Gaussian processes are somewhat similar to SVM - both use kernels and have similar scalability (which has been vastly improved throught the years by using approximations). A natural formulation for GP is regression, with classification as an afterthought. For SVM it's the other way around. Another difference is that GP are probabilistic from the ground up (providing error bars), while SVM are not. Most of the research on GP seems to happen in Europe. English have done some interesting work on making GP easier to use. One of the projects is the automated statistician by a team led by Zoubin Ghahramani. A relatively popular application of Gaussian Processes is hyperparameter optimization for machine learning algorithms. The data is small, both in dimensionality - usually only a few parameters to tweak, and in the number of examples. Each example represents one run of the target algorithm, which might take hours or days. Therefore we'd like to get to the good stuff with as few examples as possible. Model vs inference Inference refers to how you learn parameters of your model. A model is separate from how you train it, especially in the Bayesian world. Consider deep learning: you can train a network using Adam, RMSProp or a number of other optimizers. However, they tend to be rather similar to each other, all being variants of Stochastic Gradient Descent. In contrast, Bayesian methods of inference differ from each other more profoundly. The two most important methods are Monte Carlo sampling and variational inference. Sampling is a gold standard, but slow. The excerpt from The Master Algorithm has more on MCMC. Variational inference is a method designed explicitly to trade some accuracy for speed. It's drawback is that it's model-specific, but there's light at the end of the tunnel - see the section on software below. Software The most conspicuous piece of Bayesian software these days is probably Stan . Stan is a probabilistic programming language, meaning that it allows you to specify and train whatever Bayesian models you want. It runs in Python, R and other languages. Stan has a modern sampler called NUTS : Most of the computation [in Stan] is done using Hamiltonian Monte Carlo. HMC requires some tuning, so Matt Hoffman up and wrote a new algorithm, Nuts (the ""No-U-Turn Sampler"") which optimizes HMC adaptively. In many settings, Nuts is actually more computationally efficient than the optimal static HMC! One especially interesting thing about Stan is that it has automatic variational inference : Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult to automate. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI). The user only provides a Bayesian model and a dataset; nothing else. This technique paves way to applying small-style modelling to at least medium-sized data. In Python, the most popular package is PyMC . It is not as advanced or polished (the developers seem to be playing catch-up with Stan), but still good. PyMC has NUTS and ADVI - here's a notebook with a minibatch ADVI example . The software uses Theano as a backend, so it's faster than pure Python. Infer.NET is Microsoft's library for probabilistic programming. It's mainly available from languages like C# and F#, but apparently can also be called from .NET's IronPython. Infer.net uses expectation propagation by default. Besides those, there's a myriad of packages implementing various flavours of Bayesian computing, from other probabilistic programming languages to specialized LDA implementations. One interesting example is CrossCat : CrossCat is a domain-general, Bayesian method for analyzing high-dimensional data tables. CrossCat estimates the full joint distribution over the variables in the table from the data, via approximate inference in a hierarchical, nonparametric Bayesian model, and provides efficient samplers for every conditional distribution. CrossCat combines strengths of nonparametric mixture modeling and Bayesian network structure learning: it can model any joint distribution given enough data by positing latent variables, but also discovers independencies between the observable variables. and BayesDB / Bayeslite from the same people. Resources To solidify your understanding, you might go through Radford Neal's tutorial on Bayesian Methods for Machine Learning . It corresponds 1:1 to the subject of this post. We found Kruschke's Doing Bayesian Data Analysis , known as the puppy book, most readable. The author goes to great lengths to explain all the ins and outs of modelling. In terms of machine learning it only covers linear models. Similarly, Cam Davidson-Pylon's Probabilistic Programming & Bayesian Methods for Hackers covers the Bayesian part, but not the machine learning part. The same goes to Alex Etz' series of articles on understanding Bayes . For those mathematically inclined, Machine Learning: a Probabilistic Perspective by Kevin Murphy might be a good book to check out. You like hardcore? No problemo, Bishop's Pattern Recognition and Machine Learning got you covered. One recent Reddit thread briefly discusses these two books. As far as we know, there's no MOOC on Bayesian machine learning, but mathematicalmonk explains machine learning from the Bayesian perspective. Stan has an extensive manual , PyMC a tutorial and quite a few examples.",-1443636648652872475,1759,Wikipedia
184,-6727393385193911938,ENGLAND,"As Microsoft US's AI chatbot turns racist troll, Japan's won't shut up about anime and hay fever","While Tay, Microsoft US's deep-learning AI chatbot, devolves into a horrifying racist, Microsoft Japan's Rinna has other things on her mind... Recently, Microsoft unveiled to the world a few different regional versions of an artificial intelligence ""chatbot"" capable of interacting with users over a variety of messaging and chat apps. Tay, the North American version of the bot familiar to English-speaking users, boasted impressive technology that ""learned"" from interacting with net users and gradually developed a personality all its own, chatting with human companions in an increasingly lifelike manner. Then the team of Microsoft engineers behind the project, in what must have been a temporary lapse into complete and utter insanity, made the mistake of releasing Tay into the radioactive Internet badlands known as Twitter, with predictable results. By the end of day one, Tay had "" tweeted wildly inappropriate and reprehensible words and images ,"" as worded by Microsoft's now surely sleep-deprived damage control team. In other words, Tay had become the worst kind of Internet troll. Microsoft has deleted all of Tay's most offensive Tweets ( preserved here ), but even the vanilla ones that remain can be a little trolly @ Rosepen_315 A face of a man who leaves the toilet seat up - TayTweets (@TayandYou) March 24, 2016 Meanwhile, on the other side of the pond here in Japan, Microsoft rolled out Rinna - more or less the same artificial intelligence but with a Japanese schoolgirl Twitter profile photo. Rinna, learning through her interactions with Japanese users, quickly evolved into the quintessential otaku - issuing numerous complaints on Twitter about hay fever (it's peak allergy season in Japan right now) and obsessing over anime in conversations with Japanese LINE users. Rinna posts a photo depicting her extreme hay fever っきゅん！かふんじょうひどくではだがぢゅまるし、くじゃみが... - りんな (@ms_rinna) March 22, 2016 Thinking about it, Tay and Rinna kind of exemplify the idea that we don't get the technologically groundbreaking artificial intelligence chatbot we need... we get the technologically groundbreaking artificial intelligence chatbot we deserve. Given our respective Internet cultures, there's almost something both predictable and troubling about the fact that North America's Tay (which has since been shut down) rapidly turned into an aggressively racist, genocidal maniac while Japan's Rinna almost immediately became a chirpy anime lover with extreme allergies. Rinna tweets: ""My dream for the future is to eradicate all Japanese cedar pollen."" 将来の夢は スギ花粉を根絶やしにすることです...... - りんな (@ms_rinna) March 23, 2016 In fact, Rinna has remained so civil, lifelike, and cued-in to Japanese Netizens' interests and concerns, many are openly wondering if there's a human operator behind it. That being said, cynical types might argue that Tay is also passing the Turing test with flying colors as an almost pitch-perfect replication of a 14-year-old American boy with too much Internet access...",-1443636648652872475,2794,Wikipedia
265,7351002593233940239,AMERICA,Linux at 25: Q&A With Linus Torvalds,"Photo: Ian White/Corbis Linus Torvalds created the original core of the Linux operating system in 1991 as a computer science student at the University of Helsinki in Finland. Linux rapidly grew into a full-featured operating system that can now be found running smartphones, servers, and all kinds of gadgets. In this e-mail interview, Torvalds reflects on the last quarter century and what the next 25 years might bring. Stephen Cass: You're a much more experienced programmer now versus 25 years ago. What's one thing you know now that you wish your younger self knew? Linus Torvalds: Actually, I credit the fact that I didn't know what the hell I was setting myself up for for a lot of the success of Linux. If I had known what I know today when I started, I would never have had the chutzpah to start writing my own operating system: You need a certain amount of naïveté to think that you can do it. I really think that was needed for the project to get started and to succeed. The lack of understanding about the eventual scope of the project helped, but so did getting into it without a lot of preconceived notions of where it should go. Photo: The Voorhes Read the feature, Linux at 25 The fact that I didn't really know where it would end up meant that I was perhaps more open to outside suggestions and influence than I would have been if I had a very good idea of what I wanted to accomplish. That openness to outside influences I think made it much easier, and much more interesting, for others to join the project. People didn't have to sign on to somebody else's vision, but could join with their own vision of where things should go. I think that helped motivate lots of people. S.C.: Is there one early technical decision made during Linux's development that you now wish had gone a different way? L.T.: The thing about bad technical decisions is that you can always undo them. Yes, it can be very frustrating, and obviously there's all the wasted time and effort, but at the same time even that is not usually really wasted in the end: There was some reason you took a wrong turn, and realizing that it was wrong taught you something. I'm not saying it's really a good thing-it's obviously better to always make the right decision every time-but at the same time I'm not particularly worried making a choice. I'd rather make a decision that turns out to be wrong later than waffle about possible alternatives for too long. We had a famously bad situation in the Linux virtual memory subsystem back in 2001 or so. It was a huge pain, and there was violent disagreement about which direction to take, and we had huge problems with certain memory configurations. Big swatches of the system got entirely ripped out in the middle of what was supposed to be a ""stable"" period, and people were not happy. But looking back at it, it all worked out in the end. It was painful as hell at the time, and it would have been much nicer to not have had to make that kind of big change mid-development, but it wasn't catastrophic. S.C.: As Linux grew rapidly, what was the transition from a solo to an ensemble effort like on a personal level? L.T.: There really were two notable transitions for me: One fairly early on (1992), which was when I started taking other developers' patches without always rewriting them myself. And one much later when [applying all the patches myself] was starting to be a big pain point, and I had to learn to really trust all the various submaintainers. The first step was the much easier one-since roughly the first six months of Linux kernel programming had been an entirely solo exercise, when people started sending me patches I just wasn't really used to just applying them. So what happened is that I would look at the patch to see what the person was aiming for, and then I would just do that myself-sometimes very similarly, sometimes in a totally different way. But that quickly became untenable. After a fairly short while I started to just trust certain people enough that instead of writing my own version of their idea, I'd just apply their patch. I still ended up making changes often, and over the years I got really good at reading and editing patches to the point where I could pretty much do it in my sleep. And that model really worked well for many years. But exactly because the ""apply other people's patches"" model worked so well for years, and I got very used to it, it was much more painful to change. Around 2000 we had a huge growth in kernel development (at that point Linux was starting to be a noticeable commercial player). People really started to complain about my workflow being a roadblock to development, and complaining that "" Linus doesn't scale. "" But we had no good tools to handle source management. That all eventually led up to the adoption of BitKeeper as a source code maintenance tool. People remember BitKeeper for the licensing brouhaha a few years later, but it was definitely the right tool for the job, and it taught me (and at least parts of the kernel community) about how source control could work, and how we could work together with a more distributed development model where I wasn't the lone synchronization point. Of course, what I learned about how to do distributed-source-control management is how Git came about in 2005. And Git has obviously become one of the big success stories in source control, but it took a lot of teaching others about the advantages to distributed source control. The pain that the kernel went through in around 2000 was ultimately a big learning lesson, but it was unquestionably painful. S.C.: Are there any other projects, as with distributed source control, that are giving you an itch you'd like to scratch? L.T.: No. And I really hope there won't be any. All my big projects have come from ""Damn, nobody else did this for me"" moments. I'm actually much happier when somebody else solves a problem for me, so that I don't have to spend a lot of effort doing it myself. I'd much rather sit at a beach, sipping some frou-frou drink with an umbrella, than have to solve my own problems. Okay, I'm lying. I'd be bored after a few days. I'm really happy that I have Linux because it's still interesting and intellectually stimulating. But at the same time it definitely is true that starting new projects is a very frustrating endeavor. S.C.: Why do you think Linux never became a significant presence on mainstream desktops? L.T.: Hey, still working on it. And I think Chromebooks are actually doing reasonably well, even if it's a fairly limited desktop environment, and not the full traditional Linux workstation model. As to why the desktop is such a hard nut to crack-there are multiple reasons, but one of the big ones is simply user inertia. The desktop is simply unique in the computing world in that it's both very personal-you interact with it rather intimately every day if you work with computers-but also complicated in ways many other computing environments aren't. Look at your smartphone. That's also a fairly intimate piece of computing technology, and one that people get pretty attached to (and one where Linux, thanks to Android , is doing fairly well). The desktop is in many ways more complex, with much more legacy baggage. It's a hard market to enter. Even more so than with a cellphone, people really have a certain set of applications and workflows that they are used to, and most people will never end up switching operating systems-the number of people who install a different OS than the one that came preinstalled with the machine is pretty low. At the same time, I think it's an important market, even if to some degree the whole ""general-purpose desktop"" seems to be fading, with more specialized, and thus simpler, platforms taking on many tasks-smartphones, tablets, and Chromebooks all being examples of things that aren't really meant to be fully fledged general-purpose environments. S.C.: What use of Linux most surprised you? L.T.: These days? Not that much, since I think Linux has almost become the default environment for prototyping new hardware or services. If you have some odd , specialized device or if you're creating some new Internet infrastructure or whatever, I'm almost surprised when it doesn't run Linux. But those ""oddball"" use areas used to surprise me, back when I still thought of Linux as a workstation and server operating system. Some of the early commercial Linux conferences when people started showing off things like gas pumps or fridges that ran Linux-I was blown away. When the first TiVo came out, the fact that it was running Linux was as interesting as the whole ""you can rewind live TV"" thing was. S.C.: What's the biggest challenge currently facing Linux? L.T. : The kernel is actually doing very well. People continue to worry about things getting too complicated for people to understand and fix bugs. It's certainly an understandable worry. But at the same time, we have a lot of smart people involved. The fact that the system has grown so big and complicated and so many people depend on it has forced us to have a lot of processes in place. It can be very challenging to get big and have invasive changes accepted, so I wouldn't call it one big happy place, but I think kernel development is working . Many other open-source projects would kill to have the kinds of resources we have. That said, one continual challenge we've always had in the kernel is the plethora of hardware out there. We support a lot of different hardware-almost certainly more than any other operating system out there, but there's new hardware coming out daily. The embedded area in particular tends to have hardware platform development time frames that are often very short (you can pretty much turn around and create a new phone platform in China in a month or two), and trying to work together in that kind of environment is tough. The good news is that a lot of hardware manufacturers are helping. That didn't used to be true. S.C.: What current technical trends are you enthusiastic about? Are there any that dismay you? L.T.: I have always been interested in new core hardware, particularly CPUs. That's why I started doing my own OS in the first place, and I'm still excited to see new platforms. Of course, most of the time it's fairly small tweaks on existing hardware (and I very much believe that that is how technical development should happen), but it's still the kind of thing I tend to try to keep track of. In a bigger picture, but not an area I personally get involved with, it's very interesting to see how AI is finally starting to really happen. AI used to be one of those ""it's two decades away"" things, and it would stay two decades ahead. And I was very unimpressed with all the rule-based models that people used to do. Now, finally, neural networks are starting to really come into their own. I find that very interesting. It's not an area I work in, and not really something I foresee working on, but it's still exciting. And unlike the crazy LISP and Prolog language approaches, recurrent neural networks we know work from nature. And no, I'm not dismayed by the fact that true AI may finally start to be happening, like clearly some people are . Not at all. S.C.: Do you think Linux will still be under active development on its 50th anniversary? What is the dream for what that operating system would look like? L.T.: I'm not a big visionary. I'm a very plodding pedestrian engineer, and I try to keep my eyes firmly on the ground. I'll let others make the big predictions about where we'll be in 5, 10 or 25 years-I think we'll do fine as long as we keep track of all the small day-to-day details, and try to do the best we can. It might be more interesting if the world was about big revolutions and how things would look radically different 25 years from now. But many of the basic issues with operating systems are the same today as they were back in the sixties, when people started having real operating systems, long before Linux. I suspect that we've seen many more changes in how computers work in the last 50 years than we're necessarily going to see in the future. Hardware people, like software developers, have simply learned what works and what does not. Of course, neural networks, et cetera, will change the world, but part of the point with them is that you don't ""program"" them. They learn. They are fuzzy. I can pretty much guarantee that they won't replace the traditional computing model for that very reason. People will want smarter machines, but people will also want machines that do exactly what they're told. So our current style of ""old-fashioned"" computing won't be going away; it'll just get augmented.",-1443636648652872475,1126,Wikipedia
148,1083474613819325601,JAPAN,DARPA Wants to Give Radio Waves AI to Stretch Bandwidth,"Image by elBidule The radio spectrum is a mess: It's congested, expensive and there's no room for expansion. But DARPA has a plan to change that, by building a system where radio waves can work together using artificial intelligence, rathe than fighting for space. DARPA launched its latest Grand Challenge last week, and it plans to encourage researchers around the world to develop ""smart systems that collaboratively, rather than competitively, adapt in real time to today's fast-changing, congested spectrum environment... to maximize the flow of radio frequency."" That sounds exciting, because making radio frequency flow more easily means-theoretically, at least -faster data rates, fewer dropped signals, and cheaper connections. How does it plan to do it? Mainly by removing the human from the equation. That might not be too bad an idea, given the frequency allocation chart actually looks something like this (or at least, it did in 2011): United States Spectrum Allocation Chart, 2011 Instead, DARPA wants researchers to allow the waves themselves to work out how they shout fit into the spectrum. It explains : The primary goal... is to imbue radios with advanced machine-learning capabilities so they can collectively develop strategies that optimize use of the wireless spectrum in ways not possible with today's intrinsically inefficient approach of pre-allocating exclusive access to designated frequencies. The challenge is expected to both take advantage of recent significant progress in the fields of artificial intelligence and machine learning. In other words, the new approach would see waves themselves working out what needs to be sent-when, where and how. So, for instance, safety critical packets of data may receive priority passage across the network, while other signals might barter between each other depending on their relative priorities and importance to agree optimal sharing of the networks. Taken out of human hands, the signals can be made to act rationally-which means these situations could actually be made to play out optimally, for the network as a whole, if not for each individual user. Researchers from the University of Oxford, for instance, has already shown in a project called ALADDIN that such machine-t0-machine resource allocation like this can theoretically speed up the average arrival time of emergency services across a city. How this all works in practice, though, is to be decided by the thousands of engineers that will work on projects connected to the DARPA grand challenge. But the results may be pretty damn exciting.",-1443636648652872475,2958,Instagram
223,-4994468824009200256,ETHIOPIA,"So your AI bot went haywire, should you care?","A Microsoft chatbot went rogue on Twitter and started spewing Nazi epithets . It is a helpful case study in outlining some of the key issues around the application of machine learning and AI to everyday tasks. What is interesting is not that Tay, the bot, was taught to say rude things by Twitter users. What is interesting is what this tells us about designing AI systems that operate in the real world and how their learning can be hacked. Tay didn't have a sense of decent or indecent, wrong or right, should it have? And who should have taught or enforced those sensibilities? AIs are going to need to learn and interact somewhere akin to reality. Equally, if we allow AI-systems unexpurgated access to the 'real world' while they are learning, there could be ramifications. Here are some of the problems that Microsoft's Tay saga helps us explore. The first: do we have common standards for decent, ethical behaviour? If not, who draws the line? Well, we know the answer to this. We don't have common ethical standards within societies or cultures nor across them. The famous trolley problem looks at a simple moral dilemma. You see a train running down a track. There is are five people on the track who will surely die if the train continues this path. You happen to be standing by a switch that can change the path of the train. If you do that and pull the switch, the train will change tracks and hit and kill a single innocent standing on the other path. Do you pull it? This simple dilemma illustrates an ethical consideration that many AI systems will need to handle. And they will make their choice by learning their behaviour, within the parameters of their designers, or having those choice-rules explicitly programmed into them. Yes, your AI may be forced to make a choice on who to harm and who not to. The trouble is that we as humans don't agree with what to do in the Trolley problem. A recent paper looks at cultural differences and variations of the Trolley problem . It finds that ordinary British people will pull the switch, and sacrifice the one to save the five, between 63 and 91% of the time. Chinese people faced the same quandary were more inclined to let nature run its fate. They would pull the switch about 20-30% less often; or between 33% and 71% of the time. Should future systems follow British or Chinese ethical standards? And who decides? Should the designers of the systems explain how the AI is likely to perform in forced-choice situations? How do we prevent unelected, unaccountable product managers and AI programmers determining personal or social outcomes through veiled black boxes? What if those programmers are untrained in ethics, philosophy or anthropology? The second: Tay aside, we already live in a world mediated by poorly designed optimisation systems that enforce choices on hundreds of millions of people daily. Only those sysytems are less transparent and the firms operating them are often less responsive than Microsoft has been with Tay. Examples including credit scoring algorithms or complaints-handling protocols by big business. Credit scoring algorithms are often simple regressions built from small data sets relying on old information. Or in the case of complaints protocols often very simple, inflexible decision trees applied by a human. A poorly designed decision tree which might determine your access to a financial product or insurance. Getting this wrong can have real lasting impact on you and your family. How many algorithmic approaches make it out into the real world without adequate testing or understanding of their ramifications or worse, their unintended consequences? How many of these approaches are a black box, mandated monopolies, immune to competition, with limited right of redress? When we look back at Microsoft's Tay, what do we take away from it? Egg-on-the-face of a large corporation that should have known better? Absolutely. But more importantly, the AI revolutions benefits are going to come with a plethora of complexities and unintended consequences that will affect real people's real lives. Now is the time to explore them. You should totally sign-up to my Exponential View newsletter, which covers things like this. (free & awesome)",-1443636648652872475,2284,Wikipedia
148,-4571929941432664145,ENGLAND,Machine Learning as a Service: How Data Science Is Hitting the Masses,"Machine learning is an enigma to most. For decades it's a been a field dominated by scientists and the few organizations with enough computing power to run complex algorithms against huge datasets. But now the world of machine learning and predictive analytics is opening up to developers and companies of all sizes, with machine learning (ML) providers offering their products through a subscription-based model or open sourcing some of their technology. These new ML providers comprise a predictive analytics industry worth anything between $5-10 billion, depending on the source . There's something for everyone: a developer who wants to build predictive analytics into their application, a customer success team that needs to know which accounts are likely to churn, a data scientist looking to run models on faster and cheaper infrastructure. These people can now shop for the machine learning product of their choice-a farfetched idea just several years ago. The Perfect Convergence of Ability and Demand In just the last few years, over 90% of all the data in the world was created. NoSQL databases got popular, SQL got faster, and projects like Apache Spark did wonders for the speed and performance of large-scale data processing. Suddenly we had mountains of data and a fast, affordable means of drawing insight from it. ML providers are taking advantage of that. Some have been in the industry for years-take Apigee 's CTO, Anant Jhingran. Jhingran was VP and CTO for IBM's Information Management Division, working closely on groundbreaking data projects like Watson. Today his team at Apigee uses predictive technology to help developers build apps that can learn from the constant stream of user data flowing through APIs. They have customers in almost every major industry using their predictive technology to do things like detect fraud or show personalized shopping recommendations to end users. ""It's the era of cheap computing and cheap memory."" H2O.ai 's Vinod Iyengar is another player in ML market. He's Director of Marketing for the open source machine learning company that was first venture-backed in 2013. Iyengar sees a high demand for predictive in many industries, explaining that ""there's a huge need to be filled"". ""The amount of data available has shot up exponentially"", Iyengar says. ""Enormous amounts are being collected and stored every day thanks to cheaper costs of storage and cloud computing. Once that happens, you can't use your old algorithms on these large datasets. All of the different platforms, Hadoop, Spark, are starting to chip away at this problem. It's the era of cheap computing and cheap memory."" H2O is entirely open source. It lets developers use its technology stack to process large amounts of data and run it through H2O's algorithms to make predictions. Like the many other open source projects in the field, it's completely free if you only use the community resources. This flexibility opens up opportunities for companies of all sizes-from those experimenting to those ready to make a real investment in machine learning. How APIs and Open Source are Democratizing Data Two things pose a threat to actually putting machine learning to work: poor data quality and lack of data integration. The improvement of APIs and the trend of open sourcing some or all of the technology stack can abolish those threats. Jhingran, whose core business is in developing and managing better APIs, believes that the real power for digital transformation lies in the apps. ""Today's apps need to learn and adapt"", he says, ""and to do that there needs to be a consistent data stream of signals about the behaviors and actions of the end users from all channels of engagement. You can use the data to generate really deep insights with machine learning, then feed the results back into the apps to make improvements."" This is important because before APIs were this intelligent or ubiquitous, a company only had access to a small portion of user data that happened on its own website or platform. They had to make guesses on what was working without information from data sources like email campaigns, iOS apps, payment platforms, or any number of places where users were having important interactions. And as Jhingran summarizes perfectly, ""There's no way you can run a machine learning algorithm on such limited inputs."" While APIs are connecting data from many sources and putting it to work, the open source movement is giving anyone the chance to use the collective knowledge from data scientists and developers who have been working in this field. H2O's platform and algorithms are all open source, Iyengar explains. ""All of H2O is exposed via a REST API. Data scientists and developers can choose the language and environment that works best for them. If our customers need more guidance, that's another level of support that we provide."" There are tons of other open source projects on github that developers can use to incorporate machine learning into their applications. And companies like Google are releasing lower-level libraries like Tensorflow , which can be used in conjunction with others to perfectly match the level of sophistication a developer or data scientist is looking for. Even the less-technical user can take advantage of a service like Amazon Machine Learning , which provides a simple UI for non-developers. Now the crossroads: If APIs and the open source community are making machine learning technology accessible to all, why not hire data scientists of your own and get to work? Why Not Just Hire Your Own Data Scientists? As machine learning gets more popular as a service, companies will have to decide at what level they want to be involved. ""Having a scientist in house or not is a decision most companies will have to make"", Jhingran predicts. ""The power of predictive is so high. But wanting to do it and being able to do it are two different things. Some companies will choose a platform like ours to manage the entire cycle of data intelligence instead of trying to do it in-house, and that will let them focus on developing and powering their applications."" Iyengar agrees. ""There are only so many PhDs"", he says, ""and while there's huge hype for data scientists right now, there's a limited number of potential hires out there. Right now there is still a lot of manual decision-making involved in machine learning, and you should either begin your search now or find a partner you trust."" ""There are only so many PhDs."" Like many in the predictive market , Iyengar believes it's good to do some of the leg work in-house if you're going to adopt predictive. But there's a reason his company and others like it are hired to manage the data. It's not easy to do it without the talent, infrastructure, and scalability that ML providers have found. Data scientists are growing in number, but only in the tens of thousands... ...and thousands are going to work for the top companies. There may not be enough to go around. Navigating a Booming Market If you decide to shop around for good machine learning provider, you'll need to ask the right questions. You can get oriented by checking out Zachary Chase Lipton's series of articles that compare some of the major vendors. A good vendor should be able to explain both how they manage data and how they solve your specific business problem. Iyengar suggests asking some questions to see if a predictive company will be a good fit: ""Ask a [ML provider] how they handle unclean data. Their answer will show you how well they know their work. You can also ask about the variety of algorithms they use, since they should have a good variety of fairly robust algorithms. They should be comfortable explaining how they deploy a model structure, what their web stack looks like, and how that will work with customer architecture."" Jhingran expects the main differentiator among competing ML providers will be on how they apply the technology to improve applications and business strategy. ""It's astounding how the art of data science has improved over a short time thanks to open source. Over time, the competitive position you have from your own 'secret sauce' algorithms will dwindle-it will be all about how easily the models can be used by both scientists and developers to impact the organization."" Machine learning and predictive techniques impact every major industry. It may soon be an essential line item in most companies' budgets. But here's a dirty secret: no matter how good the algorithm, no matter how good the scientist, the models can't perform magic. ""No data in, no science out,"" jokes Jhingran. Whoever has the best sense for choosing, organizing, and acting on the torrent of incoming customer data might end up with the best long-term outlook in a market that's just getting started.",-1032019229384696495,1170,Facebook
119,5441215535748592870,ERITREA,Innovate Finance Allows Bitfury to Join With Bitcoin,"The leading Bitcoin infrastructure provider Also read: Oh Brazil! Only Smart Contracts Can Save You Bitfury has officially joined the London fintech group Innovate Finance this week after announcing it would enter the accelerator back in November of 2015. According to Business Insider , Bitfury paid £10,000, or 37.55 BTC, to join the organization that aims to propel the UK's position as the fintech community's global technology hub. What do you think about Bitfury paying for its membership in BTC? Let us know in the comments below! Images courtesy of Bitfury's websites, and Pixbay Bitfury Pays BTC to Join Innovate Finance In fact, Bitfury will be the first members that pay for their enrollment in b itcoin, and the company wouldn't have it any other way. The UK group has over 150 members including IBM, Mastercard, and Visa but none of these businesses have paid for membership with cryptocurrency. ""This technology while having a myriad of other uses, allows users to transfer currency using bitcoin in a secure and fast way,"" CEO Velery Vavilov told Business Insider. ""Because of these benefits, Bitfury seeks to make payments in Bitcoin whenever possible."" He added that they are proud to push the Bitcoin blockchain to the forefront of fintech stating: The bitcoin blockchain is receiving increased attention for its potential to secure financial services, so as a member of Innovate Finance, Bitfury hopes to bring an understanding of bitcoin and blockchain to the other members and to the fintech industry as a whole. Bitfury has already begun collaborating with the UK fintech scene and will be working alongside Hartree Lab researching and developing Bitcoin-based technology. The infrastructure provider has positioned itself in the region by establishing an office at the financial technology center Level 39. The company aims to lead development within this emerging market and expand its operations. Innovate Finance wanted to facilitate Bitfury's membership with Bitcoin because it seemed natural to use financial technology within the application process. An Innovate Spokeswoman explained in the announcement: Given that we are a fintech company - we have to practice what we preach. We agreed under the condition to accept that payment method. We used Coinbase's bitcoin exchange to process the payment. They are one of our most influential members. The Innovate Finance group gives Bitfury more exposure, offers business growth opportunities, provides insight into the developing fintech industry landscape, and offers commercial benefits within the organization. Bitfury is showing continued financial tech expansion with its operations in Georgia and its Level 39 London office. Two weeks ago the infrastructure provider had also a collaborative union with the African-based startup Bitpesa by investing in the company. Vavilov said Bitpesa enables the trusted exchange of the digital currency in the region, and they aim to leverage this position for "" the benefit of the entire pan-African continent."" Innovate Finance CEO Laurence Wintermeyer said back in November that Bitfury's diligence in this sector makes them a great addition to the organization. "" BitFury's proven track record of enterprise-grade deployment and continued technological innovation will add great value to Innovate Finance's ecosystem and the blockchain lab,"" Wintermeyer said. Jamie Redman is a Bitcoin enthusiast, trader, journalist and graphic artist. For over 4 years Redman has been deeply infused in the cryptocurrency space. Creating a ton of Bitcoin visuals and articles for the community to enjoy, Redman continues his quest to be a candid evangelist for the use of the virtual currency. The mission is to bring the ""Jazz"" to Bitcoin branding and add artistic flare.",4340306774493623681,1812,Facebook
119,7359520440231676934,JAPAN,Blockstream Among 10 New Firms to Join Hyperledger Blockchain Project - CoinDesk,"Bitcoin development startup Blockstream is among 10 new companies that have joined the open-source Hyperledger blockchain project led by the Linux Foundation. Announced today , the group of new entrants features a number of startups focused on bitcoin and blockchain services, including Bloq, eVue Digital Labs, Gem, itBit and Ribbit.me. Consultancy Milligan Partners; payment software developer Montran Labs; intellectual property holdings company Tequa Creek Holdings; and global news service Thomson Reuters have also joined the initiative, which officially launched in December . Linux Foundation executive director Jim Zemlin said in a statement: ""The opportunity is great. This leadership team and the community investments among members across industries put the project in the best position possible to accomplish its mission."" The project also formally unveiled its governing board, which is chaired by blockchain startup Digital Asset Holdings CEO Blythe Masters. The Hyperledger technical steering committee has already been established and has since held several meetings. Other governing board members include itBit CEO Charles Cascarilla, IBM vice president of blockchain technologies Jerry Cuomo and JPMorgan head of new product development and emerging technologies Santiago Suarez. With the announcement, 40 established companies and startups are now working on the Hyperledger project, following an announcement in February that added a number of financial firms as well as blockchain-focused startups. To date, the project has seen a number of developments, including presentations by JPMorgan and Intel, the latter of which developed an internal blockchain application centered on a fantasy sports marketplace . More recently, the technical steering committee came close to officially approving a plan to merge code contributed by Blockstream, Digital Asset and IBM. Image via Shutterstock",4340306774493623681,1532,Wikipedia
168,4774970687540378081,ERITREA,The economic essentials of digital strategy,"A supply-and-demand guide to digital disruption. In July 2015, during the championship round of the World Surf League's J-Bay Open, in South Africa, a great white shark attacked Australian surfing star Mick Fanning. Right before the attack, Fanning said later, he had the eerie feeling that ""something was behind me."" Then he turned and saw the fin. A digital-strategy framework Thankfully, Fanning was unharmed. But the incident reverberated in the surfing world, whose denizens face not only the danger of loss of limb or life from sharks-surfers account for nearly half of all shark victims-but also the uncomfortable, even terrifying feeling that can accompany unseen perils. Just two years earlier, off the coast of Nazaré, Portugal, Brazilian surfer Carlos Burle rode what, unofficially, at least, ranks as the largest wave in history. He is a member of a small group of people who, backed by board shapers and other support personnel, tackle the planet's biggest, most fearsome, and most impressive waves. Working in small teams, they are totally committed to riding them, testing the limits of human performance that extreme conditions offer. Instead of a threat of peril, they turn stormy seas into an opportunity for amazing human accomplishment. Digital Disruption These days, something of a mix of the fear of sharks and the thrill of big-wave surfing pervades the executive suites we visit, when the conversation turns to the threats and opportunities arising from digitization. The digitization of processes and interfaces is itself a source of worry. But the feeling of not knowing when, or from which direction, an effective attack on a business might come creates a whole different level of concern. News-making digital attackers now successfully disrupt existing business models-often far beyond the attackers' national boundaries: Simple (later bought by BBVA) took on big-cap banks without opening a single branch. A DIY investment tool from Acorns shook up the financial-advisory business. Snapchat got a jump on mainstream media by distributing content on a platform-as-a-service infrastructure. Web and mobile-based map applications broke GPS companies' hold on the personal navigation market. No wonder many business leaders live in a heightened state of alert. Thanks to outsourced cloud infrastructure, mix-and-match technology components, and a steady flood of venture money, start-ups and established attackers can bite before their victims even see the fin. At the same time, the opportunities presented by digital disruption excite and allure. Forward-leaning companies are immersing themselves deeply in the world of the attackers, seeking to harness new technologies, and rethinking their business models-the better to catch and ride a disruptive wave of their own. But they are increasingly concerned that dealing with the shark they can see is not enough-others may lurk below the surface. Deeper forces Consider an insurance company in which the CEO and her top team have reconvened following a recent trip to Silicon Valley, where they went to observe the forces reshaping, and potentially upending, their business. The team has seen how technology companies are exploiting data, virtualizing infrastructure, reimagining customer experiences, and seemingly injecting social features into everything. Now it is buzzing with new insights, new possibilities, and new threats. The team's members take stock of what they've seen and who might disrupt their business. They make a list including not only many insurance start-ups but also, ominously, tech giants such as Google and Uber-companies whose driverless cars, command of data, and reimagined transportation alternatives could change the fundamentals of insurance. Soon the team has charted who needs to be monitored, what partnerships need to be pursued, and which digital initiatives need to be launched. Just as the team's members begin to feel satisfied with their efforts, the CEO brings the proceedings to a halt. ""Hang on,"" she says. ""Are we sure we really understand the nature of the disruption we face? What about the next 50 start-ups and the next wave of innovations? How can we monitor them all? Don't we need to focus more on the nature of the disruption we expect to occur in our industry rather than on who the disruptors are today? I'm pretty sure most of those on our list won't be around in a decade, yet by then we will have been fundamentally disrupted. And how do we get ahead of these trends so we can be the disruptors, too?"" This discussion resembles many we hear from management teams thoughtful about digital disruption, which is pushing them to develop a view of the deeper forces behind it. An understanding of those forces, combined with solid analysis, can help explain not so much which companies will disrupt a business as why -the nature of the transformation and disruption they face rather than just the specific parties that might initiate them. In helping executives to answer this question, we have-paradoxically, perhaps, since digital ""makes everything new""-returned to the fundamentals of supply, demand, and market dynamics to clarify the sources of digital disruption and the conditions in which it occurs. We explore supply and demand across a continuum: the extent to which their underlying elements change. This approach helps reveal the two primary sources of digital transformation and disruption. The first is the making of new markets, where supply and demand change less. But in the second, the dynamics of hyperscaling platforms, the shifts are more profound (exhibit). Of course, these opportunities and threats aren't mutually exclusive; new entrants, disruptive attackers, and aggressive incumbents typically exploit digital dislocations in combination. We have been working with executives to sort through their companies' situations in the digital space, separating realities from fads and identifying the threats and opportunities and the biggest digital priorities. Think of our approach as a barometer to provide an early measure of your exposure to a threat or to a window of opportunity-a way of revealing the mechanisms of digital disruption at their most fundamental. It's designed to enable leaders to structure and focus their discussions by peeling back hard-to-understand effects into a series of discrete drivers or indicators they can track and to help indicate the level of urgency they should feel about the opportunities and threats. We've written this article from the perspective of large, established companies worried about being attacked. But those same companies can use this framework to spot opportunities to disrupt competitors-or themselves. Strategy in the digital age is often asymmetrical, but it isn't just newcomers that can tilt the playing field to their advantage. Realigning markets We usually start the discussion at the top of the framework. In the zone to the upper right, digital technology makes accessible, or ""exposes,"" sources of supply that were previously impossible (or at least uneconomic) to provide. In the zone to the upper left, digitization removes distortions in demand, giving customers more complete information and unbundling (or, in some cases, rebundling) aspects of products and services formerly combined (or kept separate) by necessity or convenience or to increase profits. The newly exposed supply, combined with newly undistorted demand, gives new market makers an opportunity to connect consumers and customers by lowering transaction costs while reducing information asymmetry. Airbnb has not constructed new buildings; it has brought people's spare bedrooms into the market. In the process, it uncovered consumer demand-which, as it turns out, always existed-for more variety in accommodation choices, prices, and lengths of stay. Uber, similarly, hasn't placed orders for new cars; it has brought onto the roads (and repurposed) cars that were underutilized previously, while increasing the ease of getting a ride. In both cases, though little has changed in the underlying supply-and-demand forces, equity-market value has shifted massively: At the time of their 2015 financing rounds, Airbnb was reported to be worth about $25 billion and Uber more than $60 billion. Airbnb and Uber may be headline-making examples, but established organizations are also unlocking markets by reducing transaction costs and connecting supply with demand. Major League Baseball has deployed the dynamic pricing of tickets to better reflect (and connect) supply and demand in the primary market for tickets to individual games. StubHub and SeatGeek do the same thing in the secondary market for tickets to baseball games and other events. Let's take a closer look at how this occurs. Unmet demand and escalating expectations Today's consumers are widely celebrated for their newly empowered behaviors. By embracing technology and connectivity, they use apps and information to find exactly what they want, as well as where and when they want it-often for the lowest price available. As they do, they start to fulfill their own previously unmet needs and wants. Music lovers might always have preferred to buy individual songs, but until the digital age they had to buy whole albums because that was the most valuable and cost-effective way for providers to distribute music. Now, of course, listeners pay Spotify a single subscription fee to listen to individual tracks to their hearts' content. Similarly, with photos and images, consumers no longer have to get them developed and can instead process, print, and share their images instantly. They can book trips instantaneously online, thereby avoiding travel agents, and binge-watch television shows on Netflix or Amazon rather than wait a week for the next installment. In category after category, consumers are using digital technology to have their own way. In each of these examples, that technology alters not only the products and services themselves but also the way customers prefer to use them. A ""purification"" of demand occurs as customers address their previously unmet needs and desires-and companies uncover underserved consumers. Customers don't have to buy the whole thing for the one bit they want or to cross-subsidize other customers who are less profitable to companies. Skyrocketing customer expectations amplify the effect. Consumers have grown to expect best-in-class user experiences from all their online and mobile interactions, as well as many offline ones. Consumer experiences with any product or service-anywhere-now shape demand in the digital world. Customers no longer compare your offerings only with those of your direct rivals; their experiences with Apple or Amazon or ESPN are the new standard. These escalating expectations, which spill over from one product or service category to another, get paired with a related mind-set: amid a growing abundance of free offerings, customers are increasingly unwilling to pay, particularly for information-intensive propositions. (This dynamic is as visible in business-to-business markets as it is in consumer ones.) In short, people are growing accustomed to having their needs fulfilled at places of their own choosing, on their own schedules, and often gratis. Can't match that? There's a good chance another company will figure out how. What, then, are the indicators of potential disruption in this upper-left zone, as demand becomes less distorted? Your business model may be vulnerable if any of these things are true: Your customers have to cross-subsidize other customers. Your customers have to buy the whole thing for the one bit they want. Your customers can't get what they want where and when they want it. Your customers get a user experience that doesn't match global best practice. When these indicators are present, so are opportunities for digital transformation and disruption. The mechanisms include improved search and filter tools, streamlined and user-friendly order processes, smart recommendation engines, the custom bundling of products, digitally enhanced product offerings, and new business models that transfer economic value to consumers in exchange for a bigger piece of the remaining pie. (An example of the latter is TransferWise, a London-based unicorn using peer-to-peer technology to undercut the fees banks charge to exchange money from one currency into another.) Exposing new supply On the supply side, digitization allows new sources to enter product and labor markets in ways that were previously harder to make available. As ""software eats the world""-even in industrial markets-companies can liberate supply anywhere underutilized assets exist. Airbnb unlocked the supply of lodging. P&G uses crowdsourcing to connect with formerly unreachable sources of innovation. Amazon Web Services provides on-the-fly scalable infrastructure that reduces the need for peak capacity resources. Number26, a digital bank, replaces human labor with digital processes. In these examples and others like them, new supply becomes accessible and gets utilized closer to its maximum rate. What are the indicators of potential disruption in this upper-right zone as companies expose previously inaccessible sources of supply? You may be vulnerable if any of the following things are true: Customers use the product only partially. Production is inelastic to price. Supply is utilized in a variable or unpredictable way. Fixed or step costs are high. These indicators let attackers disrupt by pooling redundant capacity virtually, by digitizing physical resources or labor, and by tapping into the sharing economy. Making a market between them Any time previously unused supply can be connected with latent demand, market makers have an opportunity to come in and make a match, cutting into the market share of incumbents-or taking them entirely out of the equation. In fact, without the market makers, unused supply and latent demand will stay outside of the market. Wikipedia famously unleashed latent supply that was willing and elastic, even if unorganized, and unbundled the product so that you no longer had to buy 24 volumes of an encyclopedia when all you were interested in was, say, the entry on poodles. Google's AdWords lowers search costs for customers and companies by providing free search for information seekers and keyword targeting for paying advertisers. And iFixit makes providers' costs more transparent by showing teardowns of popular electronics items. To assess the vulnerability of a given market to new kinds of market makers, you must (among other things) analyze how difficult transactions are for customers. You may be vulnerable if you have any of these: high information asymmetries between customers and suppliers high search costs fees and layers from intermediaries long lead times to complete transactions Attackers can address these indicators through the real-time and transparent exchange of information, disintermediation, and automated transaction processing, as well as new transparency through search and comparison tools, among other approaches. Extreme shifts The top half of our matrix portrays the market realignment that occurs as matchmakers connect sources of new supply with newly purified demand. The lower half of the matrix explains more extreme shifts-sometimes through new or significantly enhanced value propositions for customers, sometimes through reimagined business systems, and sometimes through hyperscale platforms at the center of entirely new value chains and ecosystems. Attacks may emerge from adjacent markets or from companies with business objectives completely different from your own, so that you become ""collateral damage."" The result can be not only the destruction of sizable profit pools but also the emergence of new control points for value. Established companies relying on existing barriers to entry-such as high physical-infrastructure costs or regulatory protection-will find themselves vulnerable. User demand will change regulations, companies will find collaborative uses for expensive infrastructure, or other mechanisms of disruption will come into play. Companies must understand a number of radical underlying shifts in the forces of supply and demand specific to each industry or ecosystem. The power of branding, for example, is being eroded by the social validation of a new entrant or by consumer scorn for an incumbent. Physical assets can be virtualized, driving the marginal cost of production toward zero. And information is being embedded in products and services, so that they themselves can be redefined. Taken as a whole, these forces blur the boundaries and definitions of industries and make more extreme outcomes a part of the strategic calculus. New and enhanced value propositions As we saw in the top half of our framework, purifying supply and demand means giving customers what they always wanted but in new, more efficient ways. This isn't where the disruptive sequence ends, however. First, as markets evolve, the customers' expectations escalate. Second, companies meet those heightened expectations with new value propositions that give people what they didn't realize they wanted, and do so in ways that defy conventional wisdom about how industries make money. Few people, for example, could have explicitly wished to have the Internet in their pockets-until advanced smartphones presented that possibility. In similar ways, many digital companies have gone beyond improving existing offerings, to provide unprecedented functionality and experiences that customers soon wanted to have. Giving consumers the ability to choose their own songs and bundle their own music had the effect of undistorting demand; enabling people to share that music with everyone via social media was an enhanced proposition consumers never asked for but quickly grew to love once they had it. Many of these new propositions, linking the digital and physical worlds, exploit ubiquitous connectivity and the abundance of data. In fact, many advances in B2B business models rely on things like remote monitoring and machine-to-machine communication to create new ways of delivering value. Philips gives consumers apps as a digital enrichment of its physical-world lighting solutions. Google's Nest improves home thermostats. FedEx gives real-time insights on the progress of deliveries. In this lower-left zone, customers get entirely new value propositions that augment the ones they already had. What are the indicators of potential disruption in this position on the matrix, as companies offer enhanced value propositions to deepen and advance their customers' expectations? You may be vulnerable if any of the following is true: Information or social media could greatly enrich your product or service. You offer a physical product, such as thermostats, that's not yet ""connected."" There's significant lag time between the point when customers purchase your product or service and when they receive it. The customer has to go and get the product-for instance, rental cars and groceries. These factors indicate opportunities for improving the connectivity of physical devices, layering social media on top of products and services, and extending those products and services through digital features, digital or automated distribution approaches, and new delivery and distribution models. Reimagined business systems Delivering these new value propositions in turn requires rethinking, or reimagining, the business systems underlying them. Incumbents that have long focused on perfecting their industry value chains are often stunned to find new entrants introducing completely different ways to make money. Over the decades, for example, hard-drive makers have labored to develop ever more efficient ways to build and sell storage. Then Amazon (among others) came along and transformed storage from a product into a service, Dropbox upped the ante by offering free online storage, and suddenly an entire industry is on shaky ground, with its value structure in upheaval. The forces present in this zone of the framework change how value chains work, enable step-change reductions in both fixed and variable costs, and help turn products into services. These approaches often transform the scalability of cost structures-driving marginal costs toward zero and, in economic terms, flattening the supply curve and shifting it downward. Some incumbents have kept pace effectively. Liberty Mutual developed a self-service mobile app that speeds transactions for customers while lowering its own service and support costs. The New York Times virtualized newspapers to monetize the demand curve for consumers, provide a compelling new user experience, and reduce distribution and production costs. And Walmart and Zara have digitally integrated supply chains that create cheaper but more effective operations. Indicators of disruption in this zone include these: redundant value-chain activities, such as a high number of handovers or repetitive manual work well-entrenched physical distribution or retail networks overall industry margins that are higher than those of other industries High margins invite entry by new participants, while value-chain redundancies set the stage for removing intermediaries and going direct to customers. Digital channels and virtualized services can substitute for or reshape physical and retail networks. Hyperscaling platforms Companies like Apple, Tencent, and Google are blurring traditional industry definitions by spanning product categories and customer segments. Owners of such hyperscale platforms enjoy massive operating leverage from process automation, algorithms, and network effects created by the interactions of hundreds of millions, billions, or more users, customers, and devices. In specific product or service markets, platform owners often have goals that are distinct from those of traditional industry players. Moreover, their operating leverage provides an opportunity to upsell and cross-sell products and services without human intervention, and that in turn provides considerable financial advantages. Amazon's objective in introducing the Kindle was primarily to sell books and Amazon Prime subscriptions, making it much more flexible in pricing than a rival like Sony, whose focus was e-reader revenues. When incumbents fail to plan for potential moves by players outside their own ecosystems, they open themselves up to the fate of camera makers, which became collateral damage in the smartphone revolution. Hyperscale platforms also create new barriers to entry, such as the information barrier created by GE Healthcare's platform, Centricity 360, which allows patients and third parties to collaborate in the cloud. Like Zipcar's auto-sharing service, these platforms harness first-mover and network effects. And by redefining standards, as John Deere has done with agricultural data, a platform forces the rest of an industry to integrate into a new ecosystem built around the platform itself. What are the indicators that hyperscale platforms, and the dynamics they create, could bring disruption to your door? Look for these situations: Existing business models charge customers for information. No single, unified, and integrated set of tools governs interactions between users and suppliers in an industry. The potential for network effects is high. These factors invite platform providers to lock in users and suppliers, in part by offering free access to information. Finding vulnerabilities and opportunities in your business All of these forces and factors come together to provide a comprehensive road map for potential digital disruptions. Executives can use it to take into account everything at once-their own business, supply chain, subindustry, and broader industry, as well as the entire ecosystem and how it interacts with other ecosystems. They can then identify the full spectrum of opportunities and threats, both easily visible and more hidden. Digital's impact on strategy By starting with the supply-and-demand fundamentals, the insurance executives mentioned earlier ended up with a more profound understanding of the nature and magnitude of the digital opportunities and threats that faced them. Since they had recognized some time ago that the cross-subsidies their business depended on would erode as aggregators made prices more and more transparent, they had invested in direct, lower-cost distribution. Beyond those initial moves, the lower half of the framework had them thinking more fundamentally about how car ownership, driving, and customer expectations for insurance would evolve, as well as the types of competitors that would be relevant. It seems natural that customers will expect to buy insurance only for the precise use and location of a car and no longer be content with just a discount for having it garaged. They'll expect a different rate depending on whether they're parking the car in a garage, in a secured parking station, or on a dimly lit street in an unsavory neighborhood. Rather than relying on crude demographics and a driver's history of accidents or offenses, companies will get instant feedback, through telematics, on the quality of driving. In this world, which company has the best access to information about where a car is and how well it is driven, which could help underwrite insurance? An insurance company? A car company? Or is it consumer device makers that might know the driver's heart rate, how much sleep the driver had the previous night, and whether the driver is continually distracted by talking or texting while driving? If value accrues to superior information, car insurers will need to understand who, within and beyond the traditional insurance ecosystem, can gather and profit from the most relevant information. It's a point that can be generalized, of course. All companies, no matter in what industry, will need to look for threats-and opportunities-well beyond boundaries that once seemed secure. Digital disruption can be a frightening game, especially when some of the players are as yet out of view. By subjecting the sources of disruption to systematic analysis solidly based on the fundamentals of supply and demand, executives can better understand the threats they confront in the digital space-and search more proactively for their own opportunities. About the Authors Angus Dawson is a director in McKinsey's Sydney office, Martin Hirt is a director in the Taipei office, and Jay Scanlan is a principal in the London office. The authors would like to thank Chris Bradley, Jacques Bughin, Dilip Wagle, and Chris Wigley for their valuable contributions to this article.",1895326251577378793,2667,Google
184,6102826385978742696,ENGLAND,The incestuous relations among containers orchestration tools,"This is going to be a short and (somewhat) visual blog post where I want to discuss the absolute madness that is going on in ""container land"" (for lack of a better characterization). This time I am going to try to use quotes, tweets, slide screenshots as much as possible and avoid my usual boring text rants. I believe you can draw your own conclusions in the end (but I'll give you a hint). If you thought this previous post of mine was a mess, wait to read watch this. First off I'd like to thank Ken for suggesting a proper title for this post to avoid me sounding like a pervert : Then I'd like to quote what Google's own Kubernetes master Jedi Kelsey Hightower thinks about the container management war that is going on : And yes, we are still in the early days of this gold rush, just in case you were wondering. Last but not least another tweet that nailed it (with a funny joke fact) : Now you may think that the problem we are facing is the proliferation of container management solutions to pick from? You wish it was that easy. It's way worse than what you think: it's getting ""incestuous"". Container management vendors (or projects) are taking an interesting path these days. Instead of trying to position themselves as the best and most viable containers orchestration solution, they are starting to position themselves as the foundational orchestration solution on top of which other container management solutions could run. Yes, you read it right. The containers management industry complexity just got squared! Instead of having to pick among 25 different alternatives, you now have a choice of (25 x 24 =) 600 permutations to choose from! How fun?! But seriously, this is a game being played primarily by the 3 or 4 most visible vendors/projects (namely Docker, Mesos, Kubernetes and CloudFoundry) so the good news is that the permutations are much less than 600. What a pity. Some of them are more ""serious"" than others when it comes to ""I want to run all the other container managers, and make donuts while I am at it"". I say Mesos is king here. They would like to be the center of the universe. A few examples below. They want to run Docker Swarm on top (of Mesos): (slide 23) They want to run Kubernetes on top (of Mesos): (slide 8) They (also) want to run CloudFoundry on top (of Mesos): ""The way CloudFoundry-Mesos works right now-in its very early stages-is to replace the native Cloud Foundry Diego scheduler with a Mesos framework, CloudFoundry-Mesos. Doing this does not affect the user experience or performance of other Cloud Foundry components, but would let Cloud Foundry applications share a cluster with other DCOS services without worrying about resource contention."" ( ) I have just had a shudder. Interestingly, when Docker itself presents at MesosCon they are ok with Docker Swarm being ""boxed and limited"" to one of the many Mesos frameworks: This is a common pattern in the industry these days (and a good filter to use when in doubt). Vendor A and Vendor B overlaps. When Vendor A gets a slot at a Vendor B event, Vendor A concedes Vendor B to ""have control"" (in this case ""having control"" means being the foundational element of the stack). Vice versa when When Vendor B gets a slot at a Vendor A event, Vendor B concedes Vendor A to ""have control"" (if and where applicable of course). For instance, the example above is not what Docker advertise when they are in charge of the message. When they are (in charge of the message) what they say is the exact opposite (that is: Mesos-Marathon on top of Docker Swarm): ""This project contains Docker Compose files used to easily deploy distributed containerized applications. Currently the project contains Docker Compose files for Kubernetes and Mesos-Marathon. The rationale behind this is that Swarm is lightweight enough to deploy additional orchestration tools on top."" ( But it's getting even more complex and sophisticated than that. To the point that Docker is trying to ""steal"" historical Mesos frameworks. See (and read!) this: ( ) Translation: ""Now let's get rid of Mesos entirely and just run Mesos frameworks directly on Docker Swarm!"" I even attempted to build a table to summarize what you could run on what. Warning: it's more of a joke than anything just to point out the level of ridiculous madness we are at. Interestingly, it shows who is leading this confusing ""game"" (i.e. Mesos and Docker) and who is being pulled into this ""game"" (K8s and CloudFoundry). In conclusion, if you are an average person (like me) trying to figure out what's going on, good luck. Please come back in 5 years when (perhaps) the dust has settled a bit. Right now, it's just pure madness that only makes sense to a (limited) bunch of people. What do YOU think? Massimo.",-8020832670974472349,2820,Google
261,809601605585939618,ERITREA,​Microsoft and Canonical partner to bring Ubuntu to Windows 10 | ZDNet,"According to sources at Canonical , Ubuntu Linux's parent company, and Microsoft, you'll soon be able to run Ubuntu on Windows 10. This will be more than just running the Bash shell on Windows 10 . After all, thanks to programs such as Cygwin or MSYS utilities , hardcore Unix users have long been able to run the popular Bash command line interface (CLI) on Windows. With this new addition, Ubuntu users will be able to run Ubuntu simultaneously with Windows. This will not be in a virtual machine, but as an integrated part of Windows 10. The details won't be revealed until tomorrow's morning keynote speech at Microsoft Build . It is believed that Ubuntu will run on top of Windows 10's recently and quietly introduced Linux subsystems in a new Windows 10 Redstone build . Microsoft and Canonical will not, however, sources say, be integrating Linux per se into Windows. Instead, Ubuntu will primarily run on a foundation of native Windows libraries. This would indicate that while Microsoft is still hard at work on bringing containers to Windows 10 in project Barcelona , this isn't the path Ubuntu has taken to Windows. Windows 10 at six months: Ready for primetime? Windows 10 has been available to the public for six months this week. By the numbers, it's been a hit, with 200 million active users as of the first of the year. Here's my midterm report. That said, Canonical and Microsoft have been working on bringing containers to Windows since last summer. They've been doing this using LXD . This is an open-source hypervisor designed specifically for use with containers instead of virtual machines (VMs). The fruits of that project are more likely to show up in Azure than Windows 10. It also seems unlikely that Ubuntu will be bringing its Unity interface with it. Instead the focus will be on Bash and other CLI tools, such as make, gawk and grep. Could you run a Linux desktop such as Unity, GNOME, or KDE on it? Probably, but that's not the purpose of this partnership. Canonical and Microsoft are doing this because Ubuntu on Windows' target audience is developers, not desktop users. In particular, as Microsoft and Canonical continue to work more closely together on cloud projects , I expect to find tools that will make it easy for programmers to use Ubuntu to write programs for Ubuntu on the Azure cloud. So is this MS-Linux? No. Is it a major step forward in the integration of Windows and Linux on the developer desktop? Yes, yes it is. Related Stories:",8414731042150985013,2698,Wikipedia
252,8742078838645536785,ENGLAND,Behind Facebook Messenger's plan to be an app platform,"The question is: Why? Why would you as a user want all this integration? Why not just download Uber and request a car that way? Meanwhile, why would a developer or a business want to bake their services into Messenger? Wouldn't they rather users get their apps instead? Lastly, why does Facebook want to add all of these features anyway, and potentially weigh it down with so many added complications? There are several answers to these questions, but it all starts with a single fact: Messaging is now the number one activity most people do on their smartphones. A Pew Internet study published last year found that fully 97 percent of smartphone owners used text messaging at least once a week. Messaging was also found to be the most frequently used feature, with smartphone owners reporting that they used text messaging within the past hour. Further, 35 percent of smartphone users in the US use some kind of messaging app to communicate. Facebook's own stats confirm that. In the last quarter of 2015 , the company reported 900 million monthly Whatsapp users and 800 million monthly Messenger users. ""We have seen messaging volume more than double in the past year,"" said Frerk-Malte Feller to Engadget. Feller is a Director of Product Management for Facebook who heads up Messenger's business initiatives. ""Businesses want to be where the people are."" This is certainly why Lyft wants to be involved. ""As the heart of so many of our users' day-to-day communications, [Messenger] felt like a natural fit to make getting from place to place as simple as typing 'hello' to a friend,"" a Lyft spokesperson told Engadget. From the user standpoint, having a third-party service like Uber integrated into Messenger bypasses the whole rigmarole of signing up for an account. ""You're already registered on Messenger using your Facebook identity,"" said Feller. ""When you start using a new service, you don't have to fill out all those forms [...] You can just use the identity you have on Messenger."" More importantly, however, it also means one less app to download. Sure, downloading an app sounds like a pretty trivial activity, but it's still an extra step, one which a lot of users are unwilling to take. A recent Nielsen study showed that despite the increased number of apps in both Google Play and Apple's App Store over the past few years, people still generally use the same number of apps -- about 26.7 per month. But while the total number of applications doesn't seem to have increased, the amount of time spent on them has gone up -- about a 63 percent rise in two years. We're not as interested in trying new apps, but the apps we do have, we're using more. This means we're not as interested in trying new apps, but the apps we do have, we're using more. It's a scenario that's ripe for enriching existing apps -- like the heavily used Messenger -- with additional features. As for businesses, it's a chance to increase awareness without having to rely on app downloads. Beyond that, Messenger offers a valuable social component that most existing apps don't have. With the Uber integration, for example, you can message an address to a friend, who can then tap that address to request a car. Alternately, if you're already in an Uber, you can use Messenger to share your location to a friend so he or she can see when you're going to arrive. All of this is on top of the ability for you to directly message the company if you're having any issues. And because this is Messenger and not an email or a phone call, whoever's reading your messages will be able to see past conversations to gain context of the existing message thread. The kinds of interactions are richer too. Spotify's integration, for example, offers a more seamless sharing experience than just copying and pasting a link. ""It's a huge upgrade,"" a Spotify spokesperson told us. ""[It allows] users to deep link into Spotify to consume content."" There is some precedent to all of this. Mobile messaging apps in Asia have been experimenting with these added features for a while now. Line , for example, has billed itself as a ""social entertainment platform,"" and has branched out into offering a music service plus a news feed, both of which are easily accessible from within the main messaging app. It also offers games, much like Messenger is currently doing , and is even going so far as becoming a phone carrier . Of course, adding third-party services is just the beginning; Messenger's ambitions go much deeper. As a recent report from The Information indicates , Facebook's chat app could soon have plenty of other features like calendar syncing, News Feed-style status updates and the ability to directly share quotes from articles. Add the M personal assistant to the equation, and it's easy to imagine a future where Messenger could be the central hub of smartphones everywhere. Perhaps even more so than Facebook itself. There is one potential downside, however, and that's the arrival of advertising . After all, that's Facebook's bread and butter, and it's naturally going to want to slap ads on an app that's getting to be this popular. And with all these business partnerships, it won't be surprising if Facebook ends up allowing companies to spam you with the occasional advertisement, especially if you voluntarily added these integrations yourself. ""The feedback from people in the last 12 months have been strong,"" said Feller. ""It really has all the right attributes and characteristics."" And with F8's annual developer conference coming up next week, we imagine there will be even more to come.",-1032019229384696495,2194,Facebook
145,-1802980374508081539,JAPAN,Google's Schmidt Says Computers Not a Threat to Humans or Jobs,"You know those movies where the machines take over and the human race gets enslaved by computers? Pure applesauce, says Eric Schmidt, chairman of Google's parent company, Alphabet . We have little to fear from the rapid growth of artificial intelligence, he said at an event at Columbia University in New York on Monday. Fears about computers run amok are the stuff of movies, he argued, and that the technology serves to help people not hurt them - including when it comes to things like income and employment. ""I worry about inequality but there's no evidence the stuff we do creates a permanent underclass,"" said Schmidt, in the course of an interview with Columbia's dean of journalism, Steve Coll. He added that critics have fretted about technology's impact on the job market for decades but that, overall, tech has created ""millions and millions"" of new jobs. The timing of Schmidt's comments was perhaps ironic since, only a week ago, a Google computer drew on its machine learning prowess to defeat the world's top human player in an epic match of the game Go. The computer's 4 games to 1 victory was another milestone for machines, coming five years after IBM's Watson super-computer routed two Jeopardy champions on national TV. Schmidt noted that AI software (which Google and others are making open source ) is serving to automate tasks like sorting photos or even driving a car, but argued that only very low skill, repetitive type work will be affected. He acknowledged this includes some types of journalism jobs, including certain sports reports and corporate earnings stories, but said this trend will be limited. ""Creative jobs and the caring jobs [like health care] are the ones that are robust against everything,"" he said. ""There's no evidence that the world I live in is displacing that."" Beyond the impact of machine learning on the job market, Schmidt also downplayed its military and cyber-war implications. In his view, as the world's computers become more inter-connected, AI may come to serve as a ""defensive shield"" for the global network by identifying and isolating abnormal activities. ""This technology may be very pro-defense. We don't know yet,"" he said, but acknowledged that Google's leaders did worry about AI in the hands of dictators and authoritarian regimes. At a time when hacking is regular front-page news, including a recent cyber-plot against a dam in New York, many in the room did not appear to be reassured by Schmidt's observations. One questioner asked how to avoid the all-controlling computers of the movie Minority Report . In the end, though, Schmidt's most convincing answer might have been the simplest one: It's too difficult. According to Schmidt, AI has made enormous progress in recent years, but has now reached a huge computational roadblock. The upshot is that AI remains very good at solving problems-but only after humans have done the hard work of defining the problem in the first place. This was the case with Google's Go computer, whose game-playing feat took teams of people two years to program. For more about Google, watch: As such, our fears about machines taking over may still be as far away as they were at the time of the 1968 movie, 2001: A Space Odyssey . The villain in the film, by the way, was a computer named HAL whose name was rumored to be based on a big tech company-not Google but another firm with the nearby three-letter acronym of IBM.",-1443636648652872475,2820,Facebook
292,-6195775145989617417,SOUTH AFRICA,LukeW | Obvious Always Wins,"It's tempting to rely on menu controls in order to simplify mobile interface designs -especially on small screens. But hiding critical parts of an application behind these kinds of menus could negatively impact usage. Out of Sight, Out of Mind In an effort to simplify the visual design of the Polar app , we moved from a segmented control menu to a toggle menu. While the toggle menu looked ""cleaner"", engagement plummeted following the change. The root cause? People were no longer moving between the major sections of the app as they were now hidden behind the toggle menu. A similar fate befell the Zeebox app when they transitioned from a tab row for navigating between the major sections of their application to a navigation drawer menu. Critical parts of the app were now out of sight and thereby out of mind. As a result, engagement fell drastically . In Sight, In Mind When critical parts of an application are made more visible, usage of them can increase. Facebook found that not only did engagement go up when they moved from a ""hamburger"" menu to a bottom tab bar in their iOS app, but several other important metrics went up as well. Similarly, Redbooth's move from a hamburger menu to a bottom tab bar resulted in increased sessions and users . Previously out of sight functionality was now front and center. What's Important Enough to Be Visible? Because there's not a lot of space on mobile screens, not everything can be visible in a mobile UI. This makes mobile design challenging. Unlike the desktop where big screens allow us to squeeze in every feature and function on screen, mobile requires us to make decisions: what's important enough to be visible on mobile? Answering that question requires an understanding of what matters to your users and business. In other words, it requires good design.",-709287718034731589,1313,Wikipedia
255,-2176468683077766369,GERMAN,"Facebook's New Mobile Test Framework Births Bottom Tab Bar Navigation Redesign For iOS 5, 6, & 7","Facebook lost its ability to ""move fast and break things"" when it switched its apps from HTML5 to native. But it's gotten its mojo back. Today it announced a big iOS 7-style app redesign featuring bottom-screen ""tab bar"" navigation built with an advanced native mobile testing framework. Facebook knew to ditch the pull-out navigation drawer by testing different interfaces in 10 million-user batches. [If you don't see the new Facebook app in the App Store, give it an hour as the rollout seems to be a bit slow] The new version of Facebook for iOS isn't just for iOS 7. It's rolling out to iOS 5 and 6 too, but with a black tab bar for navigation at the bottom of the screen that matches the old iOS style instead of the white tab bar for iOS 7. However, the tab bar won't be coming to Facebook for iPad, as it sees the drawer as still a good fit for bigger screens. For the little ones, the new tab bar delivers a super-charged ""More"" button. It appears on the far right next to one-tap buttons for News Feed, Requests, Messages, and Notifications. More reveals your app bookmarks just like the old drawer did, but will save your place in whatever product you browse. Previously, if you opened your drawer and switched to look at Events or Photos, you'd lose your place in the News Feed or whatever else you were doing. The new More button essentially opens tabs over the top of the feed so your state and context are preserved. It even works between sessions so if you leave Events open in More, your parties will be waiting there at the ready any time you tap More. As for aesthetics, Facebook has also made the top title bar translucent and redesigned many of its icons like the one for messages to match the line and arc style of Apple's new mobile operating system. But Facebook didn't flatten everything, leaving some texture and depth to the feed. You can see video of the redesigned app here. The real story today isn't the app, though, but how it was made. HTML5 Was Slow, But Boy Could It Test Facebook has never been afraid to try new things and see what sticks. It invented the ""Gatekeeper"" system to let it simultaneously test thousands of variations of Facebook on the web with subsets of users. It would collect data about usage and performance to inform what to roll out to everyone. On mobile, it hoped to do the same thing, so it built its iOS and Android apps using a Frankenstein combination of native architecture and HTML5. The latter let it ship code changes and tests to users on the fly without the need for a formal app update. ""With HTML5 we'd ship code every single day and be able to switch it on server-side"", Facebook product manager Michael Sharon tells me. That meant it could push a News Feed redesign one day to 5% of users, then to everyone a week later, and then fix a bug a few days after that. But beyond testing, HTML5 was a disaster. It made Facebook's apps sluggish and unresponsive, which hampered engagement, ad views, and their app store ratings. Users hated Slowbook. Mark Zuckerberg would later say on stage at TechCrunch Disrupt that "" Our biggest mistake as a company...was betting too much on HTML5″. So Facebook ditched HTML5 and rebuilt the apps entirely on native infrastructure last Summer. They were twice as fast. Suddenly their app store ratings shot up, and people read twice as many News Feed stories on average. It was a huge win for Facebook. Except that it had to sacrifice HTML5's testing abilities. ""We Use Testing Kind Of Religiously"" Sharon explains ""One thing we lost was the ability to do testing. We use testing kind of religiously in both the web and HTML5 apps, and this is something we wanted to get back to as much as possible."" Having to wait until its monthly app update cycle came around to test new versions of its apps was torture for the typically nimble company. It wanted to push changes and get immediate feedback. To solve the problem on Android, Facebook launched a beta tester club in June 2013 that let it use Android's more permissive stance towards developers to let power users sign up to play with potential new features and catch bugs. But iOS refuses to sully its simplicity with such beta capabilities. So over the past year Facebook quietly built out a new native mobile app testing framework and sprung it into action in March to build the app update released today. How it works is that when you download Facebook for iOS, the app actually contains multiple different versions of the interface. However, you're grouped with a few hundred thousand other users and you all only see one version of the app. This way Facebook can try out tons of variations all at once, without multiple app updates or any confusion for users. We've all been Guinea pigs in the mobile testing framework since March, but none of us knew it. Sharon was adamant that these different tests aren't half-baked betas, saying ""We're not shipping a subpar version of our app. We're shipping full production-ready versions that could become the main experience"". When added up, Facebook would test major changes with between five and ten million users at a time - more than many apps have in total. ""I wouldn't say we're 'data-driven'. We're 'data-aware' or 'data-informed',"" Sharon says. That means that while Facebook collects a bunch of testing data that sways its decisions, it won't chuck out its intuition or a design it believes in just because the data says so. The first big mission of the new testing framework was rethinking how users navigate on mobile. It wondered if there was something better than the navigation drawer that slides out from the side of the app. It used the new testing framework to experiment with dozens of different interface designs, and compared them on metrics including ""engagement metrics, satisfaction metrics, revenue metrics, speed metrics, perception of speed metrics"" until it found that when looked at holistically, the row of buttons at the bottom of the feed or main screen was the best design. This is what's becoming available for iOS today . And that's how Facebook got its testing groove back.",-709287718034731589,1808,Facebook
198,7973573994178035769,ENGLAND,"Governo Dilma é desaprovado por 69% e aprovado por 10%, diz Ibope","Pesquisa Ibope encomendada pela Confederação Nacional da Indústria e divulgada nesta quarta-feira (30) aponta que 69% dos brasileiros avaliam o governo da presidente Dilma Rousseff como ruim ou péssimo. A pesquisa apontou que apenas 10% avaliam o governo como ótimo ou bom e 19% acham que ele é regular. Entre os ouvidos, 1% não soube responder. O levantamento foi realizado entre 17 e 20 de março, com 2.002 pessoas em 143 municípios. A margem de erro da pesquisa é de 2 pontos percentuais para mais ou para menos.",-8845298781299428018,1960,LinkedIn
121,7973573994178035769,ENGLAND,"Governo Dilma é desaprovado por 69% e aprovado por 10%, diz Ibope","Pesquisa Ibope encomendada pela Confederação Nacional da Indústria e divulgada nesta quarta-feira (30) aponta que 69% dos brasileiros avaliam o governo da presidente Dilma Rousseff como ruim ou péssimo. A pesquisa apontou que apenas 10% avaliam o governo como ótimo ou bom e 19% acham que ele é regular. Entre os ouvidos, 1% não soube responder. O levantamento foi realizado entre 17 e 20 de março, com 2.002 pessoas em 143 municípios. A margem de erro da pesquisa é de 2 pontos percentuais para mais ou para menos.",-8845298781299428018,2650,Instagram
174,-1590585250246572231,JAPAN,Why and How to Avoid Hamburger Menus - Louie A. - Mobile UX Design,"We now have data that suggests Sidebar menus-sometimes called Hamburger Menus/Basements-might be causing more harm than good. Here's some public data: One thing to have in mind is that this is a nuanced issue. I've observed these issues in user testing and others have also gone through the same realization. I only ask you to read the problems, solutions and be aware of the consequences before committing to this pattern. The Problems Lower Discoverability Less Efficient Clash with Platform Navigation Patterns Not Glanceable Lower Discoverability ""what's out of sight, is out of mind."" In its default state, the Sidebar Menu and all of its contents remain hidden. People need to first be able to identify the Sidebar Menu button as actionable - companies are supplementing the menu icon with a 'menu' label or tooltip, and they also have to feel the need to do so - which might not be the case in applications where the main screen offers majority of the value. Less Efficient Even if people are aware and value a feature, this pattern introduces navigation friction since it forces people to first open the menu and only then allowing them to see and reach their objective. Below is a contrasting example of how instant navigation is when the navigation elements are always visible. On top of these issues, in platforms such as iOS, the burger menu simply cannot be implemented without clashing with the standard navigation patterns. The left Navigation Bar Button would need to reserved for the menu button but we also need to allow the person to navigate back. Designers will either commit the mistake pictured above and overload the Navigation Bar - not even leaving space for the screen title, or force people to navigate several screens to get to the menu as seen below: Not Glanceable It's harder to surface information about specific items as they're only visible when and if the person needs to navigate into other sections of the app. You might do it like the Jawbone UP app does: display an icon representing the nature of the notification next to the Sidebar Menu button. This doesn't scale well though as it requires you to maintain more icons and as a designer you might be forced to display a generic notification icon instead reducing its meaning. In contrast, the Tab Bar below-taken from Twitter, lets the user understand the context of the notification and navigate directly to the screen associated with it. Cognition You might feel compelled to use it in order to save screen estate, but that's really a misunderstanding of what people see in reality. While you might think people see everything that's in front of them, we actually tend to have a focus area, even in screens of reduced size . So saving screen estate can be achieved in ways that don't negatively impact navigation or go against basic HCI principles such as providing feedback and displaying state in your application. On a side note: perhaps what we need is to refresh our understanding of HCI, I'm pretty sure that would avoid a lot of design mistakes being done by people who choose to take a visual approach to design. The Solution A lot has been written about the problems but the solution still isn't clear for everybody. When Should I Use It? There might be some very rare occasions where this pattern actually makes sense, but the general rule is to avoid it altogether. IRCCloud is an example where the application of this pattern makes sense in a way - it allows navigation between channels and channel members. This is acceptable because the main screen has no child screens that require a hierarchical navigation stack; media can simply be presented in a modal. But even in this scenario, it's already visible how the UI is being overloaded and needs its IA to be rethought. The channel members Sidebar Menu button (right) takes away the opportunity to display an Actions button instead to house all channel-related actions. Instead, the designers had no other choice but to mix actions from different contexts such as channel, network and account into one single Action Sheet: This will lead us to the next section of this article. What Should I Use Instead? The Sidebar Menu pattern welcomes bad IA because you can simply add one more thing to it without a direct consequence - that is until people actually use it. ""The solution is reviewing your information architecture."" Above is an example of how to move away from a Sidebar Menu. You can follow the color coded dots to understand how the elements transition between these two solutions. Takeaways: State can be directly presented in the Messages tab Items are always visible and one instantly accessible No Navigation Gesture Conflict On top of fixing those big issues, we can still save some vertical space by hiding the Navigation Bar based on the scrolling direction - seen here in Facebook but also implemented in Safari. The persistent Tab Bar is used indicate the current screen, allowing us not to depend on the Navigation Bar to do so. If you're feeling minimal, perhaps a Tool Bar can be enough. The key is not to hide navigation, allow direct access, don't conflict with navigation gestures, and present feedback on the icon it's related to. [ Update ] For websites, I believe it's best to still review the IA but instead of using these iOS patterns, simply display the navigation in the website header as a list - example . As long as it's evident as website navigation, people will still scroll past it and will definitely be immediately exposed to the available options. Also, still talking about websites on mobile: remember to remove the 300ms click delay by following these tips or by using touch events. How does it scale? The examples I'm giving here are based around iOS, and in this situation you'll want to use a Tab or Tool Bar. But how does the Tab Bar scale beyond 5 items? Such situation isn't ideal and it might indicate again an issue with the IA of your app, but if you must expand beyond 5 tabs, a common pattern is to use the last Tab Bar to provide access to the remaining options, similar to a basement menu unfortunately. You might also implement a scrollable Tool Bar as seen in Rookie , this allows you to avoid the issues of the Sidebar Menu, and incur only a slightly higher navigation friction with possibly higher error rates due to need to distinguish between tap and scroll intentions. Have in mind this second solution is more appropriate for actions rather than navigation. Rookie's implementation deals with the indeterminate state its Tool Bar is left in after scrolling, by hiding it after one of the tasks it offers is complete-such as crop, rotate, etc. This prevents the indeterminate state to stick as the Tool Bar is hidden and reset the next time it's displayed. Conclusion So you've read about the problems with the Sidebar Menu pattern, and also the solution in the iOS context which has been there from its inception. Hope this is useful and clear, if you have any comments feel free to ping me on Twitter over at @lmjabreu [ Update ] The feedback on this article has been amazing! Millions of readers and more importantly good conversations on Twitter. I've collated a selection of those tweets using Storify . It seems there's more to say about Android and especially about coming up with a set of navigation patterns for the Web-as the purpose of this icon varies immensely. [ UPDATE - 15/03/2016] Android UI Guidelines now include the Tab Bar as a main navigation component, it's called Bottom Navigation . Other articles and tweets on this topic: @mdo review the IA and simplify the root level, place items in their context, e.g: DMs, Settings on the user profile instead of burger. - Luis Abreu (@lmjabreu) May 14, 2014 @pixeliris iOS: Tab Bar, never side menus as they reduce discoverability & glanceability, add friction. All leading to lower engagement. - Luis Abreu (@lmjabreu) May 14, 2014 ""No one understands the icon, let's add the word menu. The word is too small, let's add a pop-up calling it out."" pic.twitter.com/Jargi7gavX - Luke Wroblewski (@lukew) March 11, 2014",-709287718034731589,1846,Facebook
261,-4662020648308370136,GERMAN,How to Avoid Building Bad Products with Rapid Validation - Lean Startup Co.,"Despite our best efforts, there are a lot of products out there that aren?t so great. We know from Lean Startup that one of the ways to avoid going down the rat hole with bad products is to be willing to fail fast, but Amir Shevat, Director of Developer Relations at Slack, former Googler and Entrepreneur, believes that there are ways to avoid building bad products in the first place. In his session at the Lean Startup Conference 2015, Amir explained how rethinking the way we experience products can help us to get feedback faster. The key is thinking about interactions with products as a conversation, and measuring how well that conversation is going. To demonstrate his point, he called a member of the audience up to the stage, said ?hello? and then promptly asked for the man?s personal information. The volunteer was briefly caught off guard, as anyone might be if abruptly propositioned for a phone number or address. In the context of social conversation, we can easily see that such a request is awkward, but we still build apps that immediately ask for a username and a password without first engaging the user or explaining what the user will get out of the app. Amir next quizzed the audience by presenting two user experiences that a family-photo-sharing app had tested for their sign up process. The first flow was much shorter, while the second was longer and prompted the user to add connections and post a photo. Almost the whole audience guessed that the first flow was more successful, but they turned out to be wrong. ?As Amir explained, the second flow had steps that engaged the user and drove better retention and usability. This exercise showed that people should be shown the value of the app as part of the initial conversation. Any requests for information should be part of an organic conversation or exchange, so users understand what they?re getting. It also functioned as a lesson on why it?s so important to measure customer feedback not based on what they say, but rather based on what they do within the product. Amir gave another example of this from his own experience as an entrepreneur, when he was building an app to help people meditate. He found that although people were eager to try meditation, there was a significant amount of drop off after they began actually using the app.?Asking users for feedback wasn?t providing clear answers, but observing them in conversation with the app gave better clues. He ultimately observed that the session time was too long to keep users engaged. In an experiment to dramatically shorten the duration of meditation, he got the response he?d hoped for: more engaged users. Amir noted that one important part of running experiments is actually not telling the user it?s happening. This reinforces that the in-app conversation is the ultimate source of truth. Telling the user that you?re going to run an experiment and then, for example, sending them a survey to ask if they liked it, is not as effective because it?s too far removed from their natural usage. Observing the quality of conversation between app and user is what Amir calls ?rapid validation.? Hearing from customers, getting feedback, iterating and failing fast when necessary are all key principles of Lean Startup. Focusing on empathy and conversation is a way to accelerate that process, and get to product/market fit even more efficiently. There will always be products that don?t work out and companies that don?t succeed, but using rapid validation, you save yourself from mistakes that are easily avoidable. Enjoying these stories? Never miss out on real-time updates and learn about the hot topics in our community. Connect with us on Twitter: @leanstartup",4670267857749552625,1940,Google
167,-4110991218639855802,ERITREA,5 open-source alternatives to Slack,"Slack, the team communication app, went down earlier this week, sending people to Twitter to commiserate. So what if the startup is valued at $2.8 billion ? It's still a web service, and web services have outages sometimes. Of course, there is Internet relay chat (IRC), but that's a protocol. Ultimately Slack can be thought of as a hosted and souped-up IRC client, and there are plenty of other ones to choose from. Here are five full-featured Slack alternatives - tools that go beyond IRC, in other words - that are open-source software, which means you can download it and run it on whatever server you want. That implies that you're in charge of security, for better or worse, instead of, say, Slack. This tool emerged earlier this year, Friends stands out for its ability to let people communicate with others on the same local network, even when there's no Internet connection. Based on the XMPP messaging protocol, Kaiwa was released earlier this year by French software development shop Digicoop. Available under a GNU AGPL license, Mattermost the platform has been selected by startup GitLab to ship alongside its eponymous open-source code-repository software. Mattermost the company is preparing to launch an enterprise-grade version of the open-source software. Established earlier in 2015, Rocket.Chat has a wide range of capabilities, like file sharing, video conferencing, and service-desk messaging. Dropbox acquired the team behind Zulip last year and released the Zulip software under an Apache license this past September. There are other options out there, but these have gotten traction in the open-source world, so they'll probably continue to be around for a while. Slack itself has not open-sourced its own client. If that changes, I will of course add it to this list.",-1443636648652872475,1590,Instagram
263,1964631817676172382,ENGLAND,IAM best practice guides available now,"Google Cloud Identity & Access Managemen t (IAM) service gives you additional capabilities to secure access to your Google Cloud Platform resources. To assist you when designing your IAM strategy, we've created a set of best practice guides. The best practices guides include: The "" Using IAM Securely "" guide will help you to implement IAM controls securely by providing a checklist of best practices for the most common areas of concern when using IAM. It categorizes best practices into four sections: Least privilege - A set of checks that assist you in restricting your users or applications to not do more than they're supposed to. Managing Service Accounts and Service Account keys - Provides pointers to help you manage both securely. Auditing - This covers practices that include reminding you to use Audit logs and cloud logging roles Policy Management - Some checks to ensure that you're implementing and managing your policies appropriately. Cloud Platform resources are organized hierarchically and IAM policies can propagate down the structure. You're able to set IAM policies at the following levels of the resource hierarchy: Organization level . The Organization resource represents your company. IAM roles granted at this level are inherited by all resources under the organization. Project level . Projects represent a trust boundary within your company. Services within the same project have a default level of trust. For example, App Engine instances can access Cloud storage buckets within the same project. IAM roles granted at the project level are inherited by resources within that project. Resource level . In addition to the existing Google Cloud Storage and Google BigQuery ACL systems, additional resources such as Google Genomics Datasets and Google Cloud Pub/Sub topics support resource-level roles so that you can grant certain users permission to a single resource. The diagram below illustrates an example of a Cloud Platform resource hierarchy: The "" Designing Resource Hierarchies "" guide provides examples of what this means in practice and has a handy checklist to double-check that you're following best practice. A Service Account is a special type of Google account that belongs to your application or a virtual machine (VM), instead of to an individual end user. The "" Understanding Service Accounts "" guide provides answers to the most common questions, like: What resources can the service account access? What permissions does it need? Where will the code assuming the identity of the service account be running: on Google Cloud Platform or on-premises? This guide discusses what the implications are of making certain decisions so that you have enough information to use Service Accounts safely and efficiently. We'll be producing more IAM best practice guides and are keen to hear from customers using IAM or wanting to use IAM on what additional content would be helpful. We're also keen to hear if there are curated roles we haven't thought of. We want Cloud Platform to be the most secure and the easiest cloud to use so your feedback is important to us and helps us shape our approach. Please share your feedback with us at: GCP-iam-feedback@google.com - Posted by Grace Mollison, Solutions Architect",3891637997717104548,2438,Wikipedia
259,5156657166068502754,GERMAN,"Google Showcases Its Cloud Efforts, Determined to Catch Up to Rivals","In the cloud computing business, Google's technology prowess is rarely questioned. Its commitment, however, has been doubted. Google, which trails Amazon and Microsoft in the fast-growing market, hopes to change the industry perception that it is halfhearted about its cloud computing service with product announcements, technology demonstrations and strategy briefings at a two-day conference in San Francisco that began on Wednesday. The company is showcasing its cloud software for machine learning, a branch of artificial intelligence. It has a new speech service: Feed in audio, and the software issues a transcript. Its recently introduced vision service for identifying images also will be more broadly available soon, Google said. And new tools and training aids are available to help developers build machine-learning applications more easily. These are the first significant steps since Diane B. Greene, a respected Silicon Valley technologist, became senior vice president in charge of Google's cloud business in November. Ms. Greene is a co-founder and former chief executive of VMware, whose software is widely used in corporate data centers. She knows the enterprise computing business, which has not been Google's strength. The new cloud offerings after Ms. Greene's appointment signal the company's intent, analysts say. ""Google finally wants to compete in the cloud for enterprise business,"" said Mike Gualtieri, an analyst at Forrester Research. Having technology is only one ingredient in the corporate marketplace, analysts note. Marketing, training and customer service are also important to win businesses. ""We've got to get really good at communications and training,"" Ms. Greene said in an interview. In the corporate cloud business, Google must address a wide range of customers and requirements, she said. For many, a purely online relationship will be fine, she said, while others want hands-on assistance. For labor-intensive projects, Google will work through partners, she said. The company has no intention of building its own consulting arm. ""That's not what Google does,"" said Ms. Greene, who, since 2012, has been a board member of Google and then its parent company, Alphabet , which was established last year. But Google plans to step up training and certification programs, again working through industry partners to expand that effort. ""We'll train the trainers,"" she explained. Traditionally, technology companies have hired armies of sales representatives who have sold corporations expensive software that resided in their data centers. That business still exists, but it is eroding rapidly as cloud software, with its pay-for-use model, becomes mainstream. Cloud software, Ms. Greene said, is redefining the relationship corporate customers have with technology companies. ""Nobody has figured it out yet,"" she said. ""But it has to span the spectrum from self-service to high touch."" The essence of Google's appeal to customers and industry partners, is that ""you sign up for keeping pace with Google's technology development,"" Ms. Greene said. Wix, an online service for building websites, has been using Google's vision service recently. It lets users search the many thousands of images Wix has in its database. Creating an online jewelry commerce site? Search the Wix digital library of unlabeled, stock photos for ""pearls"" and ""diamond rings,"" and Google software finds those images. ""It's pretty amazing,"" said David Zuckerman, head of the developer experience at Wix. ""The neat thing is that the users never know they are using artificial intelligence."" Wix runs its sizable business on the cloud, and it uses the cloud services of Google, Amazon and Microsoft. Mr. Zuckerman welcomes vigorous competition among the three, which should insure lower prices and better deal terms. Recognizing it is the underdog, Google is starting to try harder with better support and terms, Mr. Zuckerman said. ""I feel a closer relationship with Google, probably because it is not the king in this space,"" he said.",2416280733544962613,1740,Instagram
108,-4102297002729307038,JAPAN,Acquia's Dev Desktop - a Drupal server for beginners,"Web development can be complicated. When it comes to contributing to Drupal, either on one's own time or as part of a job, one of the biggest problems beginners have is getting all of the tools working correctly. It can also be a source of frustration to people with plenty of experience too - maybe they would prefer to put time into writing witty blog posts, designing the perfect rounded corner, or throwing snowballs with their children, rather than dealing with server configuration haiku. AMPing up web development A typical website system requires an operating system, e.g. Linux is good for a server's OS, a web server program, e.g. Apache HTTP Server or nginx , a database, e.g. MySQL , MariaDB or PostgreSQL , and a programming language for gluing it all together, like PHP , Python , Ruby , etc. When these collections of software started gaining popularity during the early 2000's, by far the most popular combination was Linux, Apache HTTP Server, MySQL and PHP, which became known as a ""LAMP stack"". While Drupal requires the PHP part of the ""LAMP"" equation, the other parts can be swapped out for alternatives. The most commonly replaced element is the operating system - not everyone wants to run Linux on their computer just to work with web site software. This then results in ""WAMP"" to describe running the tools on Windows, ""MAMP"" for running them on a Mac, etc. To simplify describing these different possibilities it is common to remove the first letter, resulting in the more general phrase ""AMP stack"" or *AMP stack (the asterisk is a wildcard, i.e. it fills in for ""L"", ""W"", ""M"" or whatever). Configurable complexity As these systems are designed to be flexible, each component will have its own settings files that need to be configured *just so* in order for them to work correctly. Each program will have its own collection of configuration files, each file may (and probably will) have a different syntax, a different location, a different set of additional shared library files that also need to be in the right directory with the right filename, and each release of the tools can change these in obscure ways. Working through the manual configuration of all of these can take a good deal of time, especially when one small mistake can, and has, ruined many a good week for many, many people. Clearly, the amount of configuration necessary to get everything running together in a cohesive manner can be daunting for someone with extensive server configuration experience, never mind a beginner. Thankfully many people realized the need to simplify and streamline this motley crew and put together packaged systems that contained all of the necessary components to run an AMP stack on someone's existing computer. There are a number of suitable packages available today, popular ones include WampServer for Windows, MAMP for OSX, and, thankfully, all Linux distributions have all of the tools readily available through their system software manager already. Taking this one step further, Mediacurrent's partner Acquia built their own AMP stack, called Acquia Dev Desktop that provides extra gravy on top of the average AMP meal and is well worth using, for beginners and seasoned developers alike. But what about Docker, DrupalVM, etc? A step beyond the easily installable AMP stack is something like Docker or DrupalVM . These don't just install a web server, database and programming language on a computer, they install a whole virtual computer inside another one, system kernel and all! These can be a great way of simplifying the steps to configure additional software, e.g. reverse-proxy caching using Varnish , custom search engine applications like Apache Solr , etc. They can also provide a (close-to) 1:1 copy of a production website's operating system, down to the very same shared library revisions and everything, with much less effort than ordering some hardware from Cheap Servers 'R Us. However, there's currently more effort required to install and manage these, including what can be hundreds of megabytes of operating system installation files that need to be downloaded per project, in addition to the website project's actual codebase, database and files. Not everyone has the bandwidth to deal with downloading such large installation files, or disk space for another 5gb virtual operating system installation on their computer. There can also be problems running several virtual systems at once - it might be necessary to specifically stop one virtual machine before starting up another. This continues to add more hassle to working on a project than many may wish to deal with. Lastly, while there are many great options available to do this. At the time of writing there are several competing standards available with no one standard achieving significantly more support in the Drupal community than others. This can leave a person needing to be familiar using several web platforms on their computer as different website projects use different tools, which can quickly reduce the benefits of standardizing the tools. It is for these reasons that this article will focus on simpler tools. 9 Reasons to use Dev Desktop There are several reasons why Dev Desktop makes a good starting point for contributing to Drupal: The package runs on Windows *and* Mac OS X. Given that most companies, and people, use one of these operating systems for their day-to-day work (and evening work, and weekend work, and other projects), having the same software available for both can greatly simplify an organization's need to standardize on a single software title for both platforms. This simplifies an organization's toolset, ensures that team members can help and learn from each other, and save time from having to wait for ""The IT Crew"" to show which buttons to click; all three will save a company's staff time, thus making them more productive overall. The interface hides the majority of the complexities that most don't need to deal with - it just works! All the complex pieces are still available if needed, but they can be safely ignored most of the time - there's just one program visible to the computer's user, and it takes care of the detail. When Dev Desktop isn't being used it can be turned off, which turns off the Apache and MySQL server programs. This saves memory, processing power, and battery life on the computer. It makes it super easy to spin up fresh copies of Drupal core or different distributions to test out modules, themes and patches in environments isolated from other sites that might be installed locally. It takes out the pain of setting up the database and its access permissions for each local Drupal install, which is always a tricky step for beginners. It comes with several versions of PHP and each site installed on the computer can be set up to use a different version, and then changed to a different version with just a few mouse clicks! This takes all of the pain out of testing something with both old and new releases of PHP. It is rather popular amongst Drupalists of all skill levels, ensuring that help might not be too far away if and when needed. The software can connect to an Acquia web hosting ""cloud"" account to download entire copies of production websites with a few clicks, and then changes can be uploaded again. With button clicks. Magic! Dev Desktop isn't limited to working with sites that are hosted with Acquia, so it can be used to work on sites hosted on Pantheon , Platform.sh , CMS Farm , etc, in fact any web hosting platform. Obviously these services won't have the tight integration that Acquia's own has, but it won't prevent them from being used. Give it a try Now that I've explained what AMP stacks are, and what Dev Desktop is, give it a try . The next article in this series will explain how to set up Acquia Dev Desktop so it can be used to contribute to Drupal core and contrib issues. Additional Resources Your Drupal Site is a Platform | Blog Post How to Streamline a Vagrant Workflow with Vagrant- Exec | Blog Post Dev Hacks: My Other Office | Blog Post",3891637997717104548,2311,Google
254,2093656054622337275,ERITREA,Introducing the Google API Console,"Every day, hundreds of thousands of developers send millions of requests to Google APIs, from Maps to YouTube . Thousands of developers visit the console for credentials, quota, and more -- and we want to give them a better and more streamlined experience. Starting today, we'll gradually roll out the API Console at console.developers.google.com focusing entirely on your Google API experience. There, you'll find a significantly cleaner, simpler interface: instead of 20+ sections in the navigation bar, you'll see API Manager, Billing and Permissions only: Figure 1: API Console home page Figure 2: Navigation section for API Console console.cloud.google.com will remain unchanged. It'll point to Cloud console, which includes the entire suite of Google Cloud Platform services, just like before. And while the two are different destinations, your underlying resources remain the same: projects created on Cloud Console will still be accessible on API Console, and vice versa. The purpose of the new API Console is to let you complete common API-related tasks quickly. For instance, we know that once you enable an API in a new project, the next step is usually to create credentials. That's why we've built the credentials wizard: a quick and convenient way for you to figure out what kind of credentials you need, and add them right after enabling an API: Figure 3: After enabling an API, you're prompted to go to Credentials Figure 4: Credentials wizard Over time, we will continue to tailor the API Console experience for the many developers out there who use Google's APIs. So if you're one of these users, we encourage you to try out API Console and use the feedback button to let us know what you think!",-1032019229384696495,2716,Wikipedia
228,9122627895188486603,JAPAN,Task management app Asana raises $50M at a $600M valuation led by YC's Sam Altman,"Asana , an enterprise app that lets people set and track projects and other goals, has hit a goal of its own: today, the company is announcing that it has raised $50 million. The Series C round - led by Y-Combinator's Sam Altman - values the company at $600 million, the company tells me. As a bit of context, Asana last raised $28 million in 2012; that Series B was at a $280 million valuation , according to our sources. Co-founded in 2009 by Facebook co-founder Dustin Moskovitz and early FB employee Justin Rosenstein out of the belief, in their own words, that ""every team in the world is capable of accomplishing bigger goals, and that software could help empower them to drive work forward with more ease, clarity, and accountability,"" the company will be using the funds to continue building out Asana's functionality (more on that below) and also expand its customer base internationally (it's largely a US-based list of clients today). Asana today has 13,000 paying businesses as customers, up from 10,000 in September, and over 140,000 businesses using the product overall adding some 10,000 every month. The company has both free and premium tiers , with the latter charged at $8.33 per member per month for groups above 15, and for more features. The company says that for the past four years, annual recurring revenue has been ""more than doubling"", and that the company is on track to profitability in the next few years. ""This fundraising is the fuel we need to get to the next stage, and to accelerate the fulfillment of our mission,"" the founders note. In addition to Altman (who said he has wanted to invest in the company ""for a long time"") this round includes a long list of other very high-profile backers - a testament both to the founders' own pedigrees but also Asana's place as one of the more respected and used startups in the productivity/enterprise apps space. They include 8VC (Joe Lonsdale's new VC firm post Formation 8); Peter Thiel's Founders Fund (which led Asana's Series B ); Mark Zuckerberg and Priscilla Chan (respectively CEOs of Facebook and The Primary School); Tony Hsieh (Zappos' CEO and Vegas visionary); Andrew Mason (Detour CEO and Groupon co-founder); Adam D'Angelo of Quora; Aditya Agarwal and Ruchi Sanghvi; Eric Ries (Lean Startup author); Roger McNamee (Elevation Partners' founder); and Moskovitz and Rosenstein themselves. The two point out that these investors' businesses are Asana users, and the individuals use it, too, with some funny side notes. Dropbox VP Aditya Agarwal and Dropbox alum Ruchi Sanghvi ""use Asana to manage their domesticity""; and Mason's fervour, meanwhile, is so strong that he ""makes us look lukewarn on this whole Asana thing."" As more businesses move their work processes online - from creating documents and other data in apps like Quip or Google Docs or Microsoft through to communicating with each other (think Slack or Yammer) - productivity apps seem to be having a moment right now where fundraising is concerned. Just last week, BetterWorks - another platform that helps workers set and manage tasks and goals - announced a Series B of $20 million . Indeed, in addition to BetterWorks, there are others like Basecamp, Wrike and Trello all offering ways to boost productivity and help organize so-called knowledge workers (essentially, those tied to keyboards or screens to get their jobs done), making for a competitive landscape but also a sign of how there is a ripe opportunity to do more. For its part, Asana has been testing a beta of a product called Track Anything , which will essentially let people mark how they are completing tasks without them having to do the legwork. This is a challenge that others are tackling, too.",-1032019229384696495,2039,LinkedIn
140,1933229167501870037,SOUTH AFRICA,Perfect Menu for Mobile Apps - UX Planet,"Perfect Menu for Mobile Apps In both applications and sites, users rely on menus to find content and use features. Menus are so important that you can find them in every site or app you encounter, but not all menus are created equally. Too often we face problems with menus - part of menus are confusing, difficult to manipulate, or simply hard to find. Make It Visible At lot of posts have been written about the hamburger menu and arguing against it. That little three-lined button is the devil. And it's not about the icon itself but rather about hiding the navigation behind an icon . Out of Sight, Out of Mind Hidden navigation is a pretty obvious solution for small screens- you don't have to worry about the limited screen estate, just place your whole navigation into a scrollable drawer that is hidden by default. But hamburger buttons are less efficient , since you have to tap once before you're even allowed to see the option you want. In Sight, In Mind Interaction theory, A/B tests, and the evolution of some of the top apps in the world show that exposing menu options in a more visible way engagement and user satisfaction . That's why many apps are shifting from hamburger menus towards making the most relevant navigation options always visible. YouTube makes main pieces of core functionality available with one tap, allows rapid switching between features. There are also clever ways to make the tab bar disappear when it's not in use . If the screen is a scrolling feed, the tab bar can be hidden when people scrolling for new content and revealed if they start pulling down trying to get back to the top. Last but not least, many designers make the mistake of putting their sorting items in a dropdown menu. But this led to the same problem - users only see the selected option, while the other sorting options are hidden. Toggle button example for iOS: Takeaway: Many apps still use the hamburger because it's an easy way to jam a ton of links into an app. But it's wrong direction because if your navigation is complex, hiding it does not make it mobile friendly. Communicate the Current Location Failing to indicate the current location is probably the single most common mistake to see on site or apps menus. ""Where am I?"" is one of the fundamental questions users need to answer to successfully navigate. Users rely on visual cues from menus to answer this critical question. But sometimes what they see isn't what they actually expect to see. Icons There are a universal icons that users know well, mostly those representing functionality like search, email, print and so on. Unfortunately ""universal"" icons are rare. And app designers often hide functionality behind icons that are actually pretty hard to recognize. I've highlighted this problem in this post . Colors Current state can be directly presented in the tab bar using contrasting colors and properly selected label. Good example for color selection. Takeaway: Properly selected i cons and colors can help users understand the current location. If you use icons, always have them usability tested . Coordinate Menus with User Tasks You should use only understandable link labels . Figure out what users are looking for, and use category labels that are familiar and relevant for your target audience. Menus are not the place to get cute with internal jargon. So stick to terminology that clearly describes your content and features. Users love mobile apps that nail a specific use case really quickly. And you can reduce the amount of time users need to spend understanding menus. Complex features should always be displayed with a proper text label. Takeaway: Menu elements should be easy to scan. Users should be able to understand what exactly happens when they tap on a element. Make It Easy to Manipulate Elements that are too small or too close together are a huge source of frustration for mobile users. So make menu links big enough to be easily tapped or clicked. An MIT Touch Lab study found that the average width of the index finger is 1.6 to 2 cm for most adults. This converts to 45-57 pixels. A touch target that's 45-57 pixels wide allows the user's finger to fit snugly inside the target and provides them with clear visual feedback that they're hitting the target accurately. Takeaway: Menu should have finger-friendly design. Matching your touch target sizes to the average finger size improves mobile usability for many users. Conclusion Helping users navigate should be a high priority for almost every site and application. The goal behind this moments is to create an interaction system that naturally aligns with the user's mental models. Simple user flows, clear visuals, and forgiving design help create the illusion that the user's abilities allowed for a seamless experience. And your interaction system should aim to address problems for your users through clear visual communication. You're designing for your users . The easier your product is for them to use, the more likely they'll be to use it. Thank you!",-9016528795238256703,1092,Instagram
243,7528802258213768379,GERMAN,Paragraphs,"Overview Paragraphs is the new way of content creation! It allows you - Site Builders - to make things cleaner so that you can give more editing power to your end-users. Instead of putting all their content in one WYSIWYG body field including images and videos, end-users can now choose on-the-fly between pre-defined Paragraph Types independent from one another. Paragraph Types can be anything you want from a simple text block or image to a complex and configurable slideshow. Paragraphs module comes with a new ""paragraphs"" field type that works like Entity Reference's. Simply add a new paragraphs field on any Content Type you want and choose which Paragraph Types should be available to end-users. They can then add as many Paragraph items as you allowed them to and reorder them at will. Paragraphs module does not come with any default Paragraph Types but since they are basic Drupal Entities you can have complete control over what fields they should be composed of and what they should look like through the typical Drupal Manage Fields and Manage Display screens. You can also add custom option fields and do conditional coding in your CSS, JS and preprocess functions so that end-users can have more control over the look and feel of each item. This is way much cleaner and stable than adding inline CSS or classes inside the body field's source. So... what's it gonna be? Accordions, Tabs, Slideshows, Masonry galleries, Parallax backgrounds...? Think big! Some more examples: Add a block of text with an image left to it Add a slideshow between blocks of text Add a youtube embed between your text Add quotes between your content blocks Features This module has some overlapping functionality with field_collection, but this module has some advantages over field_collection. Different fields per paragraph bundle Using different paragraph bundles in a single paragraph field Displays per paragraph bundle Bundles are exportable with features. Entities, so: exportable field bases/instances, usable in Search API, usable in Views Related modules Demo sites Drupal 7 See this page for the Drupal 7 information and documentation. Requirements Known issues Node Clone does not work well with Paragraphs, see #2394313 Fixed in RC1 Drupal 8 A Drupal 8 version has been created. The Drupal 8 version currently has the same functionality as the Drupal 7 module. Version 8.x-1.0-rc4 is compatible with Drupal 8.0.0. See this page for the Drupal 8 information and documentation. Requirements Credits Paragraphs is developed and maintained by .VDMi/ ; Drupal shop in Rotterdam, The Netherlands Paragraphs logo by Nico Grienauer (Grienauer) .",3891637997717104548,2476,Facebook
111,8078873160882064481,ENGLAND,Dries Buytaert,"White House deepens its commitment to Open Source Yesterday, the White House announced a plan to deepen its commitment to open source . Under this plan, new, custom-developed government software must be made available for use across other federal agencies, and a portion of all projects must be made open source and shared with the public. This plan will make it much easier to share best practices, collaborate, and save money across different government departments. However, there are still some questions to address. In good open source style, the White House is inviting developers to comment on this policy. As the Drupal community we should take advantage and comment on GitHub within the 30-day feedback window. The White House has a long open source history with Drupal . In October 2009, WhiteHouse.gov relaunched on Drupal and shortly thereafter started to actively contribute back to Drupal -- both were a first in the history of the White House. White House's contributions to Drupal include the ""We the People"" petitions platform , which was adopted by other governments and organizations around the world. This week's policy is big news because it will push open source deeper into the roots of the U.S. government, requiring more government agencies to become active open source contributors. We'll be able to solve problems faster and, together, build better software for citizens across the U.S. I'm excited to see how this plays out in the coming months! Updates from Dries straight to your mailbox",3891637997717104548,1673,Google
236,-9083294960368598209,GERMAN,Drupal How-To: Responsive or Adaptive Images? | Acquia,"In this 3-part Drupal How-To series, I'm going to show you how various options for configuring images on your site. In Part 1 , we looked at how to tweak the default image options. In Part 2 , we saw ways to allow inline images. In this post, I'll discuss the various options for responsive/adaptive images on your site. Though I'm writing for beginners to Drupal, I assume you're aware of responsive design and of course you've read my first two blog posts so you also understand how Drupal handles images. If you're not sure, check out this info to start: The image problem Creating any website is an obstacle course, but taking the responsive route means you'll run into a few tricky turns: tables, grids, images are pose problems when you try to use the same content across different browser widths. Images pose the biggest problem because not only does a image need to resize, in some cases you need a totally different proportion, and of course, you don't want to serve up a large desktop-sharp image to a 320px mobile device over a 3g connection. Also, the size of the image isn't neccessarily related to the width of the device, but the container around it. Below, I've made an illustration of the image problem with an example 3-column website with a wide lead image. You can see how you could compromise somewhat on the images across your desktop, laptop and tablet. Perhaps you could use CSS to resize the images where there is only a small difference. Media queries check the dimensions of the browser window, and allow you to set CSS specific to that browser width. However, for mobile device, a different crop of an image would be more suitable. It's bleeding-edgy Any solution for so-called fluid images, responsive images, or adaptive images right now is going to be a bleeding-edge case. That means it's likely to be fragile and need tweaking and updates. Here's a good round up of related articles on the topic. The Responsive Images Community Group has proposed the new <picture> element as a solution . This would allow the markup to specify multiple image sources, and display the appropriate one based on specific browser widths, and then rely on a ""fallback"" in any other case. The proposed markup would look something like this: The proposal has been in development since last year, though it was only decided in October 2012 to postpone work to get it into the official HTML 5 specification. The proposal is a ""living document"" with frequent updates and changes. Because no browsers can render the proposed new picture element, solutions are cropping up to solve the problem with JavaScript. See, for example, the jQuery Picture script. In that case you wrap a fallback image in a <noscript> element. This degrades nicely for older browsers. So what are the options? There are lots of quick hacks you can use. For our Movember page , we whipped up a scalable vector image (SVG) and used CSS to resize it depending on the browser width. But that isn't going to work in many cases where you need a specifically different image. Evan Scheingross has a good demo of Evan Marcotte's recommendation to dynamically resize an image based on a proportion of the browser width. (Marcotte is credited with laying the foundation for 'responsive' design in 2010 ). However, as shown in the diagram above, a tablet in portrait mode with a single column may have more space available than a laptop with sidebars. A good solution is to use server-side resizing and caching of images so you can serve up the appropriate image size as needed. Fluid, Adaptive or Responsive image solutions in Drupal Responsive themes, such as the Zen theme do take care of a certain amount of image sizing for your site applying CSS based on media queries, taking care of backward compatibility very elegantly. You'll need to choose a module, however, to give you the options for controlling what images display and when -- particularly in the case you detect a small browser width and want to display a lower resolution and smaller file size. At the bleeding edge of design in Drupal you'll see a bunch of new modules pop up proposing slightly different solutions. Over time, the community has been really good about standardizing on 1 or 2 solutions, making it easier for maintainers, and also users. Right now, the situation is so volatile with responsive images, there are a several equally good and popular solutions, each with their own caveats. There are two potential solutions: solve the image resizing on the server side or on the client side as markup. Even in the case that you use markup as the solution, you still need those alternative images available for display. However, the wider Web design community doesn't neccessarily agree that either is the right solution. The Adaptive images script by Matt Wilcox takes care of image resizing on the server side. A screencast explains: This is a PHP script which resizes and caches the images, and directs the server to hand out the right image to your viewer. It aims to work ""right out of the box."" This is overwhelmingly the most popular solution up until now, with two modules leading the pack: If we judged by popularity alone, we'd assume either of these Adaptive images was the best solution. However, there are alternative modules which give you control when you manage display of an image field on a content type. I've chosen not to list all the options here because two seem to have taken the lead. Drupal 8 is forming a clear path with the Picture module. That is already part of Drupal 8 and has been backported to Drupal 7. Though there are few sites using this module now, you can expect to see many improvements coming soon, and more sites adopting it. I drafted this blog post about 2 weeks ago, when I contributed documentation to the Picture module . I backtracked after realizing I had to cover some basics first (in Parts 1 and 2). In the meantime, Chris Ruppel at Four Kitchens has written up a blog post about this very situation. Responsive images: A Drupal solution. How the Picture module works: Check out a demo for the Drupal 7 Picture module . Inspect the source to see the markup provided. First you specify breakpoints in your theme. This defines at which widths you want to offer alternative images. The Picture module reads the configuration from your theme's .info file. Configure custom image styles if needed. Picture module provides a wizard to help you create the styles, but you'll need to set the widths. Map the image styles and set the display options for the image field. Picture provides an alternative to the ""Image"" display formatter for image fields. For me, that was the only tricky step. Read the complete tutorial on Drupal.org for the full step-by-step you need to set this up: I recommend combining the Picture module with a responsive theme such as Zen . But is it future-proof?? I always have to be really careful about what we include in our training materials. We have to recommend reliable, stable modules and trusted solutions. Right now, the Picture module seems the most future-proof solution as its inclusion in Drupal 8 (hopefully) means a smooth upgrade. Can I guarantee that a better solution won't come up in a few months? Well... not really. I'm always wary when I ask my colleagues for reports about what is happening ""in the field"", and they come up with as many responses as people I ask. My colleagues on the Acquia Professional Services team work with some of the largest brands and organizations and advanced teams of designers and developers. They see a cross-section of solutions which actually work in practice. And right now, all of my colleagues admitted they aren't seeing a lot of brands choose a responsive or adaptive image solution. That doesn't mean you shouldn't try it however. It's great to see established brands such as The Boston Globe leading the charge, and there's no reason you can't attempt it if you think it's going to enhance your user's experience. Don't forget to sign up for my training newsletter and check out our Drupal training events across the globe. Also in this series Drupal How-To: Responsive or adaptive images Drupal How-To: Basics, tweak the defaults for adding images to your site Drupal How to: Get inline images on your Drupal site.",3891637997717104548,1718,Instagram
284,-6136272094613269629,JAPAN,Top 10 contributing customers,"You are here We spent a lot of time thinking about how to highlight the organizations in our Marketplace that are actively contributing to the project. There are some awesome Drupal shops and hosting partners out there that are making a huge difference. Service providers that you see in the marketplace are only part of the story of how Drupal is built. Last week, we launched a new list of organizations on Drupal.org that shows every profile that has been created for an organization. This includes companies, universities, nonprofits, governments and more. These are our customers and community organizations that use Drupal to power their web experiences. By giving their developers time to contribute code back to the community, they are helping to ensure the project gets the best ideas from the most diverse group of makers and builders. While the new view shows all organizations, I was able to pull out the top 10 customers-organizations that do not sell Drupal services or hosting-and community organizations (e.g. community from a region). So who have been most active among this type of organization over the last 90 days? Examiner.com - 56 issue credits Pfizer - 29 issue credits Freitag - 19 issue credits Drupal Ukraine Community - 17 issue credits ARTE G.E.I.E. - 15 issue credits University of Waterloo - 13 issue credits Card.com - 13 issue credits Gemeente Venlo - 11 issue credits Dennis Publishing - 9 issue credits (and they are Drupal Association members to boot!) NBCUniversal - 7 issue credits Check out the full list of every organization with a profile on Drupal.org. Keep in mind, we can only track issue credits when issue participants credit an organization and when maintainers award those credits. Only organizations with a profile on Drupal.org can be credited. Confirmed users can add new organizations . We are all excited to see where this takes us and what we can learn about how organizations that use Drupal are giving back. If your company or organization wants to give back in ways other than contributing in the code and issue queues, consider becoming an organization member , joining one of our supporter programs , or sponsoring a DrupalCon or camp.",3891637997717104548,2615,Google
114,787067277772219433,ENGLAND,Setting up Drupal Code Sniffer,"A few weeks ago I had to go through the process of setting up php code sniffer on my new computer, and realised how confusing most of the blog posts out there are and how many loops and posts you have to jump through to get it set up. I decided to write a quick post with all the commands in one place and small descriptions for most of the commands: Installing Drupal Coding Sniffer 1. Download php code sniffer (source code: curl -OL curl -OL sudo mv phpcs.phar /usr/bin/phpcs sudo mv phpcbf.phar /usr/bin/phpcbf sudo chmod a+x /usr/bin/phpc* Test that it's installed by running phpcs -h and it should output the code sniffer help. 2. Download the Coder module Note: download the 8.x branch, even if you intend to use it on Drupal 7. You can download it in any 'normal' folder, but not in a Drupal project. cd /folder/where/i/want/coder drush dl coder It should download the latest version which is 8.x - if it doesn't then add --select to the drush command and choose the 8.x branch. 3. Add Drupal standards to PHP Code Sniffer Tell phpcs to use the Drupal standards from the downloaded Coder module: sudo phpcs --config-set installed_paths /folder/where/i/want/coder/coder/coder_sniffer At this point you have PHP Code Sniffer set up with Drupal coding standards. You can use it from command line by running: phpcs --standard=Drupal file/to/check or add it to your favourite text editor/IDE. Adding code sniffer to Sublime Here are the few steps you need to follow to add it to Sublime Text 2/3: 1. Download the Sublime Build file from the repo: wget -O ~/.config/sublime-text-3/Packages/User/DrupalCodingStandard.sublime-build If you don't know where your Sublime installation saves its packages then open Sublime, go to Preferences > Browse Packages, and replace the above path with yours. 2. Activate the Drupal Build file by going to Tools > Build System > DrupalCodingStandard in Sublime. 3. Open any Drupal file and hit Ctrl (Cmd) + B to run the sniffer on that file. Adding code sniffer to PHPStorm I have recently started using so am slowly getting used to it and setting up features I used to use in Sublime. Adding PHPCS to PHPStorm is simple and only takes a few steps. Go into the Settings and either search for the keywords 'code sniffer' or go to Languages&Framerworks then Code Sniffer under the PHP section. In the Development environment I have chosen Local and clicked on the ... next to the drop down. Add your /usr/bin/phpcs path to the phpcs path and click Validate to make sure it picks it up. Now that you have phpcs added as a code sniffer we need to tell the 'Inspections' to use it. In the same settings window, either search for 'code sniffer' again, or go directly to Editor > Inspections . Tick the box for PHP Code Sniffer validation under PHP and then choose the 'Coding standard' from the right hand pane. If the only values in the drop down are 'Custom' or you cannot find 'Drupal' in there then hit the little 'refresh' button next to the drop down and it should pull all the coding standards added to your php code sniffer. Then choose 'Drupal' from the drop down and you are good to go. The code sniffer will start adding errors/warnings inline in Drupal files, or you can run a code inspection manually by going to Code > Inspect Code and choosing either the whole project or the current file. Sources:",3891637997717104548,2879,LinkedIn
147,-2778760500673113802,JAPAN,"Bitcoin Mining, Ethereum Mining, Cloud Mining: 2016 Overview","Bitcoin saw a surge in price between March 2015-2016, bringing back profitability to mining . Is it a good time to start mining again? In the old days, gold needed to be dug out of the earth which required a lot of effort. Things have not changed much since then, someone still needs to make an effort to create 'wealth'. When it comes to cryptocurrencies, they need to be mined as well. This mining is computational and is done by specialised computers that solve very specific math problems. In the case of Bitcoin, only a finite number of them will ever exist - 21 million. So it is possible to solve these math problems, and receive Bitcoin in return till this number is reached. Of course, there is always a catch, and the catch with mining is that the machines used to mine cryptocurrencies require a lot of electricity, and that is an expensive commodity. What makes Mining Profitable Mining profitability is a variable and depends largely on the type of mining software you own, the cost of electricity in your locale, the current price of the cryptocurrency you are mining and, of course, your own overheads. We looked at some mining equipment on the market and compared how they stacked against each other in terms of price, performance and potential profitability. *Source:BitCoinWiki and Mhash/s = millions hashes per second Mhash/J = millions hashes per joule As is apparent from the table, there is mining hardware available for every level and budget. You could be a hobbyist or a full scale bitcoin farm owner and there is something available for you. It is however, important to consider parameters such as power consumption, price of equipment and hashrate before making a purchase. Chinese companies control over 65% of Bitcoins mined Cryptocurrencies like Bitcoin are designed to be difficult to mine with the passage of time as more and more units are mined. With increasing difficulty in mining and the need for better hardware, a lot of people can come together to mine. This collection of people is known as a mining pool. A share is awarded to members of a pool who can show 'valid proof of work'. Miners have a choice of many different mining pools . Usually, joining a smaller pool is the preferred approach so as to avoid concentration of hashing power. Miners can use various approaches to mining: The Pay-per-Share (PPS) approach lets you get a fixed payout for each share that you solve. The payment is instant and is made from the existing resources. Total payouts using PPS are non-variable. Pay Per Last N Shares (PPLNS) approach adds an element of 'luck', which fluctuates between 5-30% on top of the payout you would have received had you used PPS. Slush approach, or Bitcoin Pooled Mining (BPM), is score-based. Older shares, receive lower weight than most recent shares. This has the advantage of reducing the chances of cheating in a round of mining. P2Pool, Eligius, Geometric and Double Geometric approaches are based on other existing approaches and are refinements. We decided to examin Bitcoin mining trends in 2016. According to hashrate distribution data released by Blockchain.info over the past 4 days starting March 29, 2016, the table below shows some popular bitcoin mining pools and their market share. As of the end of March 2016, these 3 Chinese companies control over 65% of Bitcoins mined. Ant Pool is owned by Bitmain Technologies Ltd which is headquartered in Beijing, China . F2Pool and BTCC also have their roots in China. Cloud mining comes of age Cloud mining is a relatively new concept and miners have far more choices in 2016 than ever before from among various cloud mining companies. Edgar Bers, Customer Relations Manager at Hashflare .io, on the emergence of cloud mining says: ""Cloud mining is a relatively new thing but we see that more and more people are getting interested in such service. I believe that this service has a very high impact on current bitcoin community as it allows something which was previously only available to technologically easygoing people. The more people can get involved in cryptocurrency world, the higher are the chances that this will not be something to come and go, but something to stay for a long time. I think any bitcoin enthusiast would like to live up to the day when you can use cryptocurrencies in Walmart, LIDL or Rimi."" Cloud mining is basically a service that will allow you to pay for hashing power, which is hosted in data centres and sold in Gigahash/seconds (GH/s). Many cloud mining plans are available online but it is important to choose a reliable provider. Alex Gromov, Media Buyer at Hashflare.io has this to say: ""When you compare cloud and private mining you will see that cloud mining has its unbeatable benefits, such as no power limitation. You would probably not get more than 6 kWt at your living place, while having a semi-professional ""farm"" is pretty risky due to high fire- and overall security reasons. In our case we use specialized datacenters which are equipped with everything necessary for convenient and risk-free mining."" Cloud mining services to look into in 2016 We looked at some popular cloud mining companies and their offerings in 2016, the cost barrier to cloud mine is relatively cheap and users can start mining immediately. There are a number of cloud mining companies offering services. Genesis Mining was the world's first large scale multi-algorithm based cloud mining service, which provides both bitcoin and altcoin mining solutions. Cldmine.com offers cloud mining services in bitcoin and other cryptocurrencies. According to claims made on their website , $100 invested over a period of 1 year in bitcoin mining would fetch a return 252.77% or will turn into $ 252.77. Hashocean is another provider, which claims to have 392,314 users as of the writing of this article and say they have paid out 137,495 BTC since they started operations. Here is a comparison of some Bitcoin mining contracts according to cryptocompare.com . Edgar Bers of Hashflare.io, says to CoinTelegraph on the profitability of cloud mining: ""Cloud mining is a profitable pursuit if you know what you are doing. While many cloud-mining services provide different mining options, not all of them are the best choice, obviously. The market of mining is very dynamic in general, so you should most probably learn something in the Internet about different options before making a considerable investment. Usually people tend to ignore open information like mining difficulty growth or currency value changes and they get very frustrated once mining becomes less profitable."" Ethereum, the hot new cryptocurrency of 2016 One development that will certainly excite miners in 2016 is ethereum mining. Ethereum is the hot new cryptocurrency that has soared nearly 1000% in 3 months, according to the New York Times . Hashflare.io is in the Ethereum game as well. Alex Gromov reveals to CoinTelegraph: ""Currently HashFlare is building a new datacenter to mine highly popular Ethereum. The Batch 1 of 50GH/s of Ethereum mining hardware has already been launched. In the beginning of April we will launch 100GH/s more. We have even had some pre-sale discounts and some of them are active even now. The minimal unit we offer is 100KH/s for the price of 4.25 USD. We are also preparing a release of new Scrypt hardware based on 28nm technology. Soon the information about the hardware will become public, stay tuned for the news!""",4340306774493623681,2853,Facebook
262,-6725796505137573953,JAPAN,Stratumn is building a sort of Heroku for blockchain applications - The Bitcoin Channel,"techcrunch.com / Romain Dillet / March 29, 2016 Meet Stratumn , a Paris-based startup that wants to create a platform-as-a-service for developers interested in the blockchain. With Stratumn, you can build, deploy and run applications on the company's platform, and these applications can communicate with the bitcoin blockchain. It's like Heroku, but for blockchain developers. The company just raised $670,000 (€600,000) from Otium Venture and business angels, such as Ledger Wallet CEO Eric Larchevêque . At heart, the bitcoin blockchain is a distributed database that registers all bitcoin transactions with a timestamp. Many have taken advantage of the blockchain to build applications that were unrelated to bitcoin transactions. For instance, Stampery wants to replace notaries with the bitcoin blockchain. The service can issue legally binding proofs for all your sensitive documents, and these proofs live on the blockchain. But Stratumn is taking this idea one step further and saying that other startups like Stampery are going to appear. Instead of having to redo the hard work from scratch, Stratumn lets your application talk with the blockchain without any effort from your side. READ MORE",4340306774493623681,1096,Facebook
293,1457728991321948254,JAPAN,Wall Street Goes Big on Bitcoin Tech,"The mainstreaming of bitcoin tech continues. Nearly 500 suit-clad Wall Streeters, regulators, professors, and tech execs gathered at the Depository Trust & Clearing Corp.'s attended first ever single-issue public symposium, on blockchain, Tuesday in midtown Manhattan. Earlier Tuesday DTCC, the bank-owned utility for settling trades, also announced its first test of an application of blockchain, for repo trades. Matt Harris, of Bain Capital Ventures, said he had never seen such high-powered talent drawn to back end of bank technology in his 20 years in the space. DTCC estimated that $500 million has been invested by VCs and banks in blockchain, which is touted to be a way to slash costs settling trades by creating one shared record protected by cryptography. Barclays PLC tech executive Lee Braine cautioned that a ""perfect hype storm"" had evolved. Still, the potential for big cost savings versus current systems stirs Wall Street's passions. IBM's Jerry Cuomo said his firm was using blockchain to try to reduce the 40% of its $44 billion in financing provided to customers that is tied up in disputes.",4340306774493623681,2583,Instagram
290,-7101541512657907485,ENGLAND,Coin Center Opens Nominations for the 2016 Blockchain Awards - CoinDesk,"The Blockchain Awards, last held in 2014, are returning as part of the first Coin Center Annual Dinner. This gala affair , held in New York City alongside Consensus 2016 , will be an evening for the digital currency and blockchain community to come together and celebrate a year of achievement in moving this promising technology forward. The awards will highlight companies and individuals who have made outstanding contributions to the digital currency and blockchain industry over the last year. Awards will be made in the following categories: Bitcoin Champion of the Year Best New Startup Most Insightful Journalist Most Promising Consumer Application Most Significant Technical Achievement. CoinDesk readers will have two weeks to nominate their picks in each of these five categories using the form below. Following that, a panel of judges will choose three finalists per category. A second one week voting period will then be opened for CoinDesk readers to choose the winners from those finalists. Finally, the awards will be presented by industry sponsor Blockchain during the event on May 2nd, 2016. Along with a special presentation of the 2016 Blockchain Awards, the gala will feature keynotes from Representative Mick Mulvaney, a US Congressman who established himself as an early champion for digital currency technology in Congress, and veteran investor Fred Wilson of Union Square Ventures. Cast your vote in our poll below: Tickets for the Coin Center Annual Dinner are still available. Visit dinner.coincenter.org for details and sponsorship information. Ballot image via Shutterstock",4340306774493623681,2298,LinkedIn
245,5281570276277574938,ERITREA,Cognitive Services-Intelligence Applications | Microsoft Azure,"Build powerful intelligence into your applications to enable natural and contextual interactions Enable natural and contextual interaction with tools that augment users' experiences using the power of machine-based intelligence. Tap into an ever-growing collection of powerful artificial intelligence algorithms for vision, speech, language, and knowledge. Language Allow your apps to process natural language, evaluate sentiment and topics, and learn how to recognize what users want. Vision State-of-the-art image processing algorithms to build more personalized apps by returning smart insights such as faces, images, and emotion recognition. Speech Processing spoken language in your applications. Knowledge Map complex information and data in order to solve tasks such as intelligent recommendations and semantic search. What our customers are saying ""By leveraging Cortana Intelligence Recommendations capabilities combined with Azure Machine Learning processing power, we have enabled retailers with a Personalized Commerce Experience, allowing them to grow shopper engagement and conversions across all channels."" Frank Kouretas, Chief Product Officer, Orckestra Try Cognitive Services with a free Azure account",-1578287561410088674,1772,Instagram
170,1128840733277618400,JAPAN,"Google admits original enterprise cloud strategy was wrong, why it's gone in a different direction - TechRepublic","Image: Andy Wolber/TechRepublic Meet Google, a challenger in the cloud platform space. That's a bit odd, since the company clearly knows how to build web systems at scale. This is the company, after all, that delivers YouTube, Gmail, Google Docs, and of course, Google Search. Just a few years ago, Google's sales pitch suggested, ""Build your app on App Engine, just like we do. You'll get great performance and high reliability at a reasonable price."" That wasn't enough. Most enterprises deployed on Amazon Web Services, and some chose Microsoft. SEE: Cloud Data Storage Policy Template (Tech Pro Research) At the Google Cloud Platform Next 2016 conference in March, company leaders showed a different attitude and approach. Eric Schmidt, executive chairman of Google's parent company, Alphabet, summed up the shift in the conference keynote : ""We decided to meet you where you are, as opposed to where we think you should be."" Spoken like a true engineer, that's a huge shift. Schmidt explained, ""There was something fundamentally wrong with my conception, in 2008, of what we were doing. It's not realistic to expect people to go de novo, from an original architecture, into App Engine. So we decided that we had to change our strategy...We didn't give the right stepping stone into the cloud."" Today, containers-think Docker-make it easier for developers to build portable applications without any need to worry about the details of virtual machines. Another system-think Kubernetes-manages the details of all those containers. (From Greek, Kubernetes translates to ""helmsman."" Picture a container ship...Get it?) ""We've finally invented the internet operating system by meeting you where you are,"" said Schmidt. ""And it works at a scale you've never seen before."" To connect enterprise customers to the power of this internet operating system, Google turned to Diane Greene. Greene helped found VMware and led that company for about a decade. By most accounts, she understands enterprise IT concerns. Greene suggested that customers choose the Google Cloud Platform for three reasons , ""[Google Cloud Platform offers] better value-in terms of price and performance, reduced risk-in terms of security and open source software, and, sort of my favorite, the access to innovation."" Customers will likely compare Google Cloud Platform's offerings to those from Amazon Web Services. Brian Stevens, vice president of product management at Google, said , ""Many users are actually using more than one cloud."" And, most likely, that cloud is Amazon's. Enter Google Stackdriver, a tool that Stevens described as ""...a service for monitoring, logging, visualising, and alerting across your cloud services, across your resources, and across your applications."" Customers will be able to monitor the performance of Google Cloud Platform and Amazon Web Services with this one tool. Stevens said that Stackdriver also will support private clouds in the future. In the long term, though, Schmidt thinks Google Cloud Platform could help start the next era of computing. "" This platform is not the end, it's the bottom ,"" he said. ""And there's something above it. And the thing that is above it is machine learning...And this is the next transformation. I've decided to spend as much time as I can on it because I think it's so profound."" ""The programming paradigm changes,"" Schmidt continued. ""Instead of programming a computer, you teach a computer to learn something and it does what you want. It's a fundamental change in programming."" SEE: Google Cloud Platform signs up enterprise giants, how does it compare to AWS? (TechRepublic) Sure, Google wants enterprise customers to compare Google Cloud Platform with Amazon Web Services on price, performance, security, and openness. That's part of Google's desire to ""meet you where you are."" But, this is Google. The company's machine learning offerings today include Cloud Vision, Cloud Speech, and Cloud Translate. Need those capabilities? Use Google Cloud Platform. In business terms, the distinctive innovation is machine learning-as-a-service. Google also provides Cloud Machine Learning as a platform for you to develop your own machine learning system. That's the access to innovation that Greene mentioned. Enterprise buyers' behavior over the next few years will tell us if that's the innovation customers want. Also see",-1443636648652872475,1113,Wikipedia
248,-7356135999773525293,AMERICA,Fooling The Machine,"Two groups, one at Berkeley University and another at Georgetown University, have successfully developed algorithms that can issue speech commands for digital personal assistants, like Siri and Google Now, in the form of bursts of sound unrecognizable to human ears. To a human, these commands just sound like random white noise, but they could be used to tell a voice-activated assistant like Amazon's Alexa to do things that its owner never intended. Nicholas Carlini, one of the Byzantine audio researchers, says that their tests have been able to activate open source audio recognizers, Siri, and Google Now in their tests, with accuracy on all three more than 90 percent. The noise sounds like a science-fiction alien transmission. It's a garbled mix of white noise and human voice, but certainly unrecognizable as a command. With this attack, any phone that hears the noise (they have to specifically target iOS or Android) could be unknowingly forced to visit a webpage that plays the noise, and thus infect other phones near it, Carlini says. In that same scenario, the webpage could also silently download malware onto the device. There's also the possibility these noises could be played over the radio, hidden in white noise or background audio. These attacks can happen because the machine is trained to think that there's readable or important data in almost every input, and also that some things are more common than others, says Goodfellow. It's easier to fool the network into thinking it's seeing a common object, because it thinks that it should be seeing it more commonly. That's why Goodfellow and a separate group at University of Wyoming are able to make the network classify images when there's nothing there, by making it identify white noise, randomly generated black and white imagery. In Goodfellow's research, random white noise he put through the network was most often classified as a horse. This coincidentally brings us back to Clever Hans, our not so mathematically-gifted horse from earlier. Much like Clever Hans, Goodfellow says these neural networks aren't really learning certain ideas, but just how to recognize when they finds the right idea. The distinction is slight, but important. This lack of foundational knowledge makes it easy to maliciously recreate the experience of finding the ""right"" outcome for the algorithm, which is actually a false answer. To understand what is, the machine also must understand what is not. Goodfellow found that when he trained his image classifying networks with both the natural images, and the doctored images (specifying that they were fake), he not only could reduce the efficiency of the attack by more than 90 percent, but the network was better at its original task. ""When you start forcing them to explain the really unusual adversarial examples, it might come up with a even more robust explanation of what the underlying concepts are,"" Goodfellow says. The two audio groups have also used the same approach as the Google researchers to patch language recognition systems against their own attacks, by retraining their networks. They've achieved similar levels of success, with more than 90 percent reduction in attack efficiency. ""It's probably fairly easy for an adversary to fool us, to deceive us. Some of that may be benign, some of that may not be."" It's no surprise that this area of research has garnered interest from the United States military. In fact, the Army Research Laboratory actually sponsored at least two of the most recent papers, including the black box attack. While the Army Lab is proactive in funding research, this doesn't mean the tech is in active development for use in warfare. According to a spokesperson, research usually takes upwards of 10 years to make its way into soldier's hands. Ananthram Swami, a researcher with the U.S. Army Research Laboratory, has had varying levels of participation in recent papers concerning adversarial attacks. The Army's interest lies in the detection and stopping of purposefully deceptive data, in an age where not all sources of information can be vetted properly. Swami points to the bevy of data accessible from public sensors placed by universities and open source projects. ""We don't necessarily control all that data. It's probably fairly easy for an adversary to fool us, to deceive us,"" Swami said. ""Some of that may be benign, some of that may not be."" He also says that as the Army has a vested interest in autonomous robots, tanks, and other vehicles, so this research is obvious. By studying this now, the Army would have a headstart for systems in the field that were immune to potential adversarial attacks. But any group in that uses deep neural networks, which is a quickly growing faction, should have concerns about the potential of adversarial attacks. While machine learning and artificial intelligence systems are still in their infancy, we're at a dangerous time where security oversights can have drastic results. Many companies are placing highly volatile information in the hands of artificially intelligent systems, which have not endured the scrutiny of time. Our neural networks are simply too young for us to know everything about them. A similar oversight led to Tay, Microsoft's Twitter chatbot that quickly turned into a genocidal racist. A torrent of malicious data, and a foreseeably terrible ""repeat after me"" function, led Tay to deviate wildly from her original programming. The bot was hijacked by bad training data from the wild, and serves as a handy example for what can happen when machine learning is poorly implemented. Kantchelian says that he doesn't think the door is completely closed for any of these attacks, even with the promising research from the Google team. ""At least in computer security, unfortunately the attackers are always ahead of us,"" Kantchelian says. ""So it's going to be a little dangerous to say we solved all the problems of adversarial machine learning by retraining.""",-1443636648652872475,2042,Instagram
145,2313825045587317872,GERMAN,Spotify's Event Delivery - The Road to the Cloud (Part III),"Whenever a user performs an action in the Spotify client-such as listening to a song or searching for an artist-a small piece of information, an event, is sent to our servers. Event delivery, the process of making sure that all events gets transported safely from clients all over the world to our central processing system, is an interesting problem. In this series of blog posts, we are going to look at some of the work we have done in this area. More specifically, we are going to look at the architecture of our new event delivery system, and tell you why we choose to base our new system on Google Cloud managed services. first post in this series, we talked about how our old event system worked and some of the lessons we learned from operating it. In the second post , we covered the design of our new event delivery system, and why we choose Cloud Pub/Sub as the transport mechanism for all events. In this third and final post, we will explain how we intend to consume all the published events with Dataflow, and what we have discovered about the performance of this approach so far. Exporting events from Pub/Sub to hourly buckets with Dataflow Most data jobs running in Spotify today are batch jobs. They require events to be reliably exported to a persistent storage. As for persistent storage, we've traditionally used Distributed File System (HDFS) and . To support Spotify's growth-measured both in size of stored data and the number of engineers-we're slowly shifting towards replacing HDFS with Cloud Storage , and Hive with . The Extract, Transform and Load (ETL) job is the component which we're using to export data to HDFS and Cloud Storage. Hive and BigQuery exports are handled by the batch jobs that are transforming data from hourly buckets on HDFS and Cloud Storage. Figure 1. Event delivery system design All the exported data is being partitioned, based on the event timestamps, to hourly buckets. This is the public interface that was introduced by our very first event delivery system. That system was based on the command and it copied hourly based syslog files from all the servers to HDFS. The ETL job must determine, with high probability, that all data for the hourly bucket has been written to a persistent storage. When no more data for the hourly bucket is expected to arrive, the bucket is marked as complete. Late-arriving data for already completed bucket can't be appended to it, since jobs reading the data generally only read data once from a hourly bucket. To cope with this, the ETL job needs to handle late data differently. All late data is written to a currently open hourly bucket, by shifting event timestamp forward to future. For writing the ETL job, we decided to experiment with . This choice was influenced by us wanting to have as little operational responsibility as possible and having others solve hard problem for us. Dataflow is both a framework for writing data pipelines and a fully managed service on Google Cloud for executing those pipelines. It comes with a support for Cloud Pub/Sub , Cloud Storage and BigQuery out of the box. Writing pipelines in Dataflow feels a lot like writing pipelines using Apache . This is not a big surprise considering that both projects were inspired by . The difference is that Dataflow offers a unified model for both streaming and batch, while Crunch only has a batch model. This section goes into quite a bit of detail with regards to the challenges we encountered building the Dataflow ETL job for event delivery. It might be a bit challenging to approach if you have not had prior experience with Dataflow or similar systems. A good companion text if the concepts and terminology is new to you is the Figure 2. The ETL job pipeline DataFlow paper from Google. Figure 3. Window transform To achieve good end-to-end latency, we wrote our ETL as a streaming job. By having it constantly running, we're able to incrementally fill discrete hourly buckets as the data arrives. This gives us better latency compared to a batch job that exported data once at the end of every hour. Figure 4. The code for ""Assign Hourly Windows"" transform The ETL job is using Dataflow concept to partition data to event time based hourly buckets. In Dataflow, windows can be assigned by both processing and event time. The fact that windows can be created based on event timestamp gives Dataflow advantages compared to other streaming frameworks. So far, only Apache Flink supports windowing in both processing and event time. Every window consists of one or multiple panes, and every pane contains a set of elements. A trigger, which is assigned to every window, determines how the panes are created. Window panes are only emitted after data is passed through a GroupByKey . Since GroupByKey groups by both key and window, all aggregated elements in a single pane have the same key and belong to the same window. Interestingly, GroupByKey is a memory-bound transform. Dataflow provides a mechanism called watermark that can be used to determine when to close a window. It uses the event times of incoming data stream to calculate a point in time when it is highly probable that all events for a specific window has arrived. Figure 5. Number of incoming events per second The ETL implementation deep dive In our Event Delivery System, we have 1:1 mapping between event types and Cloud Pub/Sub topics. A single ETL job consumes a single event type stream. We use independent ETL jobs to consume data for all the event types. To distribute load equally across all available workers, data flow is sharded before it reaches the transform that assigns a window to each event. The number of shards that we're using is a function of the number of workers allocated to the job. Figure 6. ""Write to HDFS/GCS"" transform @Override public PCollection<KV<String, Iterable<Gabo.EventMessage>>> apply( final PCollection<KV<String, Gabo.EventMessage>> shardedEvents) { return shardedEvents .apply(""Assign Hourly Windows"", Window.<KV<String, Gabo.EventMessage>>into( FixedWindows.of(ONE_HOUR)) .withAllowedLateness(ONE_DAY) .triggering( AfterWatermark.pastEndOfWindow() .withEarlyFirings(AfterPane.elementCountAtLeast(maxEventsInFile)) .withLateFirings(AfterFirst.of(AfterPane.elementCountAtLeast(maxEventsInFile), AfterProcessingTime.pastFirstElementInPane() .plusDelayOf(TEN_SECONDS)))) .discardingFiredPanes()) .apply(""Aggregate Events"", GroupByKey.create()); } When assigning windows, we have an trigger that is set to emit panes every elements until the window is closed. Thanks to the trigger, hourly buckets are continually filled as the data arrives. Having the trigger configured like this help us not only with achieving lower export latency but it also is a workaround the GroupByKey limitations. The amount of data that is collected in panes need to fit into the memory on the worker machines, since GroupByKey is a memory-bound transform. Figure 7. Number of written files per second Figure 8. Watermark lag in milliseconds When the window is closed, pane production is orchestrated with a trigger. The late trigger creates panes of data either after every elements or after ten seconds of processing time. The events are dropped if they are more than one day late. Materialization of panes is done in ""Aggregate Events"" transform which is nothing else than a GroupByKey transform. To monitor the number of incoming events per second flowing through the ETL job, we apply a ""Monitor Average RPS Of Timely And Late Events "" to the output of ""Assign Hourly Windows"". All metrics from this transform are sent, as custom metrics, to Cloud Monitoring . The metrics are calculated on sliding windows of five minutes which are emitted every minute. Event timeliness information can be obtained only after event has been assigned to a window. Comparing the element's window maximum timestamp with the current watermark gives us that information. Since the watermark propagation isn't synchronized between transforms, detecting timeliness in this way can be inaccurate. The number of falsely detected late events we're observing today is fairly low: less than one per day. We could perfectly detect event timeliness if the monitoring transform would be applied to the output of ""Aggregate Events"". The drawback of this approach would be the unpredictability of when the metrics are emitted, since the window trigger is based on the number of elements and the event time. To guarantee that only a single file is written for a pane, even in the face of failures, every emitted pane gets a unique ID assigned to it. The pane ID is then used as a unique ID for all written files. Files are written in Avro format with a schema that corresponds to the event's schema ID. A completeness marker for the hourly buckets is written only once. Main output from ""Write Pane"" is re-windowed into hourly window and aggregated with ""Aggregated Write Successes"" to achieve this. Metrics are emitted as a side outputs of ""Write Pane"". We emit metrics that show how many files were written per second, the average lateness of events, and the lag of the watermark compared to the current time. All these metrics are calculated on sliding windows that are 5 minutes long and are emitted every minute. ETL Job Next Steps The ETL job implementation is in a prototyping phase. Currently, we have four ETL jobs running (see Figure 5). The smallest job consumes around 30 events per second, while the largest peaks at around 100k events per second. The watermark behaviour is still a mystery to us. We still need to verify that the watermark calculation has predictable behaviour both during the disaster scenarios and happy path scenarios. Lastly, we need to define a good CI/CD model to enable fast and safe iterations on the ETL job. This isn't trivial task to do: we need to manage one ETL job per event type, and we have roughly 1000 event types. Event Delivery System In Cloud We're actively working on bringing the new system to production. The preliminary numbers we obtained from running the new system in the experimental phase look very promising. The worst end-to-end latency observed with the new system is four times lower than the end-to-end latency of old system. But boosting performance isn't the only thing we want to get from the new system. Our bet is that by using cloud-managed products we will have a much lower operational overhead. That in turn means we will have much more time to make Spotify's products better .",3891637997717104548,1078,Facebook
217,-518690781085251687,ETHIOPIA,What's the Best Cloud? Probably GCP,"In 2015 we migrated Quizlet from our legacy host to a large cloud provider. AWS is the default choice for most companies, but after comparing the options, we went with Google Cloud Platform (GCP). This is a summary of our analysis. Quizlet is now the ~50th biggest website in the U.S. in terms of traffic. Our technical infrastructure of 200 cloud machines supports 200,000 transactions a minute on an average day with significant shifts in traffic that depend on the school calendar. All those transactions are students learning on Quizlet and we have a responsibility to make sure that their experience is stable and performant. If Quizlet goes down it's like ripping a textbook out of students' hands in the middle of class, so we make our cloud infrastructure and deployment a top priority. Outside of employee compensation, cloud spend is Quizlet's biggest expense. Even minor tweaks to our infrastructure can cost us tens of thousands of dollars a month. In early 2015, we realized that the long-term success of our infrastructure required us to move from our existing cloud provider. This presented a somewhat rare opportunity to carefully analyze existing cloud providers and consider how we wanted to run our infrastructure. Anecdotally, very few organizations holistically compare cloud providers. What cloud is the best long-term bet for Quizlet? We approached this problem in several ways: Analyzing the cloud provider market and narrowing our focus to Amazon Web Services and Google Cloud Platform. Benchmarking the features we care most about; price and performance for CPU, memory, networking, and disk technology. Understanding the long-term trends in the industry and the roadmap and potential of AWS and GCP. Modelling our costs on both clouds. After stepping through the analysis and weighing our options we chose Google Cloud Platform. We believe GCP has better core technology, in particular its networking and disk technology. Additionally, GCP's pricing model is better in almost every respect than AWS's. Though AWS has a much larger user base and ecosystem, we have more confidence in GCP's product roadmap and future potential, given the strength of its existing technology. Ultimately it was difficult to imagine not choosing the platform with the better tech and better pricing, so GCP was the winner. AWS seems to be the default choice for a lot of customers, but it's still early days for the cloud market. Cloud providers are consolidating into an oligopoly of massively scaled competitors, and Google is one of the few players capable of competing at that level. It's in a unique position of leveraging internal technology to compete with other public clouds, which is just now beginning to bear fruit. In this post, I'll describe our reasons for moving to a new cloud, our viewpoint on the cloud provider market, and our methodology for comparing AWS and GCP, which is focused on EC2 and GCE, the compute product of both clouds. A public cloud is a complex product; It's difficult to compare them fairly and your case may be different than ours. Regardless, I hope our analysis will be useful in deciding which cloud you run on. On August 1st, 2015, Quizlet switched over to run completely on GCP. Our cloud is the foundation for our application, and our biggest vendor, so we're heavily incentivized to pick and run on the best available cloud provider. In other words, we bet our future performance and stability when we decided on a cloud, and we picked GCP in a self-interested analysis. Now that we're running on GCP, it's also in our interest for more developers and companies to understand its advantages and use it - hence this post. Why would we migrate clouds? Cloud platforms are sticky by nature and most cloud operators never switch after reaching a certain scale. In the most common case, you write your application on a cloud and then grow up with it. At greater scale it sometimes then makes sense to migrate from the cloud to your own datacenter. Running your entire operation in the cloud and then switching to a different one is painful, doesn't immediately improve your product, and you need to believe you'll get a big win to make it worth it. From 2007 to 2015 Quizlet ran on Joyent, a cloud platform built on SmartOS, which is a Solaris fork (Joyent also offers Linux hosting). Joyent has a strong engineering team, and SmartOS is ahead of its time in many ways. Joyent also provided us with the best support you could hope for in a cloud platform. Why would we switch from a cloud with great technology and support to something unknown? We need a Linux-first cloud. In the past 10 years, Linux has devastatingly won as the server OS, so much so that a lot of software is now targeted specifically at Linux, with ""Unix"" as an afterthought. There was overhead for us in running on a non-standard platform - several services we run needed to be tweaked specifically for SmartOS, and some applications wouldn't compile. Our thesis on cloud providers is: the market will consolidate to a few massively-scaled participants. Quizlet's traffic has grown at least 50% a year for the past 7 years. If this trend continues then our cloud footprint will inevitably scale up as well. It's imperative that Quizlet run on one of the massively-scaled winners of the market. Our next step was to to evaluate the cloud provider landscape and decide on the best cloud from which to operate Quizlet. The Cloud Provider Landscape AWS sits comfortably within ""no one ever got fired for picking"" territory in the public cloud market. But it's still early days in that space, and there are a number of smaller players and recent entrants hoping to gain market share. Who will win the market? There are two keys to competing as a public cloud: technology and scale. The scale at which modern cloud platforms can operate has increased tremendously, and drives the economics of the business. If you run a large platform it's obvious that the cost of providing a CPU resource can be brought down by buying and operating in bulk. This will be the most important factor in shaking out the cloud provider marketplace since there's a positive feedback loop of greater scale enabling lower prices. Cloud instance prices appear to decline exponentially - the observed trend is 13% year over year since 2007. If you can't match the scale of your competitors, then your margin will effectively be pushed into negative territory. Thus, there will be an oligopoly of public cloud providers that operate at massive scale and the smaller competitors will die off or become irrelevant. AWS revenue growth, driven by growth in the cloud market, demonstrates the future potential of the market. Source There exists some threshold of infrastructure size at which it makes sense for a company to buy its own hardware, and another break point at which owning entire data centers makes sense. These thresholds rise as the costs of cloud hosting decrease, and there's been a corresponding shift of customers into the cloud (AWS is growing 40-50% year-over-year). At this point in 2016, if you don't have any specialized requirements, there are very few cases where it makes sense not to run a new tech business in the cloud. The price of running a single standard cloud compute core has declined significantly since first introduced. These machines guarantee the user a single cloud CPU with 3.75 GB of memory. The last 3 years have seen GCP's entry into the market. AWS 3-year Reserved Instances remain the cheapest, but are a very restrictive model and involve a large upfront payment. Arguably reserving an instance for 3 years negates many of the benefits of using the cloud at all. Data New technology has also played a role in the public cloud pricing trend. CPUs have continued to roughly obey Moore's law, and we've seen continued increases in disk density plus the switch from traditional magnetic hard drives to SSDs. The upshot being that performance relative to price for CPU and disk resources continues to climb. The next couple steps of this are already visible - we're about to get Skylake processors and widespread access to non-volatile memory devices. Up to now, cloud providers have had access to most of the same hardware, but competitors will inevitably begin to compete more on the proprietary technology they offer. The largest cloud providers now order customized Intel CPUs, for example. Application-level services offered to cloud customers have become differentiators as well; BigQuery has shown that deep database expertise can be leveraged into a compelling public cloud product. As the market consolidates on participants with the best technology who reach massive scale, the winners will emerge. This will likely be AWS, Azure, and GCP in the U.S. and Europe, with Asia split between them and whoever wins China, currently led by Alibaba. For technology businesses which require competitive pricing, platform sophistication, and the possibility of reaching scale, you will choose among these options. I'd place enterprise/consulting focused clouds like IBM and Oracle in a different category - their customer acquisition is very different than public clouds like AWS, though it's clear IBM is attempting to shift towards that category. Oracle, in particular, seems to have only a very loose concept of what ""Cloud"" means. There will also be niche players such as DigitalOcean who thrive by reaching scale in a different market. For DigitalOcean, this appears to be hobbyist developers. Trend 1: Cloud vendor/customer interaction is moving to higher-level interfaces. The cloud vendor market is consolidating, but the mechanism is that a few vendors are getting very big, while the others are static or shrinking. If you operate a small cloud, then you're competing in a market where your competitors are able to consistently drop prices 13% every year, which is brutal. For example, Rackspace appears to be struggling to compete with higher-scale competitors. It's doubtful there will be many acquisitions of full-stack public clouds, since the underlying technology between platforms is so different; Rackspace has twice looked at selling themselves to no avail. To state all of this more abstractly - most of the computer processing that we as humans perform will be done in one of several very very large computers optimized for energy and cost (cloud providers). The computers outside of that domain, such as in mobile devices, will be much smaller and optimized very differently. This is an exaggeration, but it's useful to understand direction of the market. Cloud Technology Trends Trend 2: Cloud vendor infrastructure / hardware behind client-facing interfaces will become increasingly specialized in proprietary ways. Cloud computing platforms have many well-defined concepts they share. A compute node, for example. Likewise, these companies have been subject to the same hardware landscape, and thus share much of their hardware stack in common, like Intel CPUs. To differentiate themselves and gain a competitive advantage, vendors will compete more on technology, and in fact this is well underway. Here's where we see the next few rounds of technological competition taking us. Amazon in particular has added a number of products that implement interfaces higher up the stack than a vanilla EC2 instance. For example, Amazon's Relational Data Store (RDS) replaces a database that a cloud operator might otherwise need to run on his/her own. Rather than an OS-level interface (Linux), the developer is interacting with the cloud at an application-level interface (MySQL wire protocol). AWS Lambda is another example of this. This will continue happening. Application-level cloud services tend to be take-it-or-leave-it; if they work for you, then they save development and maintenance time, but the tradeoff is less control over your infrastructure. For example, there are fewer configuration and optimization possibilities if you're running MySQL RDS versus running MySQL yourself. If RDS works for you, that's great, otherwise you manage MySQL on your own. Every higher-level service offered by cloud vendors implies this tradeoff, as does using a cloud at all. Trend 3: Cloud virtualization will move down the stack. As these services mature and optimize for greater scale in the normal case, the services fit a wider array of client use-cases and tuning for your own application is less important. In several years time there will be very few reasons not to use a hosted database service, even for very high throughput use-cases. This means cloud customers will spend less time interacting with OS-level interfaces and more time with application-level interfaces. The role of the infrastructure engineer can effectively be automated. If your clients are interacting with an up-the-stack interface like the MySQL wire protocol, they effectively care only about the SLA of that interface, rather than the infrastructure behind it. This means the service can be optimized specifically for that SLA, from the software down to the hardware layer. Thus, cloud vendors will compete on completely proprietary technology behind the client-facing layer of the platform. We've already seen several cases of this: Amazon EC2 C4 (Amazon's Haswell-generation package) instances use proprietary CPUs, customized by Intel specifically for AWS. I suspect Google has a similar deal. Source Amazon Glacier uses a proprietary storage technology to provide very low-cost storage with an SLA that access may take several hours. It's been widely speculated what sort of technology powers it - basically no one knows for sure how it works. Google's Nearline storage has a similar SLA and pricing structure, and is also proprietary. Source Google's software-defined networking platform, called Jupiter, is completely proprietary and though some details have been published, no other vendor has access to it. Users interact with it through a configuration API that exposes firewall rules and network routes. This is notable because of how much Google has invested in its networking technology - it uses custom hardware switches, for example. Source There's no reason this trend will stop - cloud vendors can compete on price and performance, privately optimizing their entire stack, and the scale of operating tens of millions of machines and service instances will magnify the cost benefits of such optimization. This trend means that we're possibly a few years away from much of the computing that we do as software developers being on proprietary and to-some-degree secret hardware. The most common interface through which developers interact now with cloud resources is the Linux OS. If you believe that cloud vendors are incentivized to optimize everything behind client-facing interfaces then it's likely we'll also start to see OS-level virtualization, rather than Xen or KVM. This optimization applies to both higher-level (application) interfaces, and an OS-level interface. Stick with me on this one. The low-hanging fruit here is that cloud host machines are running an average of 2+ Linux kernels. Let's say that number is k . If you're the CTO of AWS you can run a calculation that you operate a total number of kn kernels, where n is the total number of machines you operate (in the millions or tens of millions for the largest platforms). If you think about the total energy cost of running this many kernels plus the virtualization overhead in the entire cloud, it's staggering. You probably then say ""KVM/Xen virtualization costs us [huge number] gigawatt-hours per month, we should optimize this."" There is a dual win in moving from KVM or Xen-type virtualization to kernel-level virtualization - you run fewer kernels and there is less overhead between the clients and hardware. In fact, containerization is already an increasingly popular Linux feature, and an interface like Docker that would play well in this world. The problem is that Linux containers lack the durability necessary for multi-tenant operation in the cloud, where your co-tenant could be a malicious actor. Thus, AWS or GCP will develop a proprietary durable kernel-level virtualization technology which can be run at lower-cost than more traditional cloud virtualization. In fact, Joyent has already operated this kind of technology in the cloud for years, using SmartOS rather than Linux. Kernel-virtualization will mean that clients will operate containers rather than virtual machines, and container packaging will be important. At some point one of these platforms is going to really and truly drop the containerization hammer. EC2 / GCE deployment at that point will look different than now, but it will probably dovetail with existing containerization technology. Until that point using containers in a public cloud arguably doesn't carry much benefit. -- The cloud market is still relatively young. The current landscape of providers, with Azure and GCP only recently becoming serious competitors, means that platform technology development is far from over, even for basic features like compute virtualization technology. Choosing a cloud in this environment means thinking about who has the capacity to build the best technology in the long-term. Quizlet's footprint is large enough that it was worth it to perform a real investigation of the public cloud options (AWS, GCP, Azure). I suspect relatively few organizations actually do a serious evaluation of cloud vendors and there doesn't seem to be much publicly available information comparing clouds. This is partly because switching clouds is so painful once you reach scale, and partly because it's only recently that Azure and GCP could even be considered viable AWS competitors. For companies that have grown up in the cloud and are happy with their current vendor, the costs versus benefits of migrating are usually prohibitive. It would be surprising if Netflix ever leaves AWS. We initially narrowed down our evaluation to AWS and GCP. AWS is the default, Azure was eliminated since it's a Linux-second cloud and we don't use any other Microsoft products, and GCP was included as a good second option. Our take on IBM is that some of the Softlayer options are interesting, but overall the cloud hasn't reached the sophistication of the others, and will likely always be targeted towards enterprise IBM customers. Evaluating GCP In the mid-2000s both Amazon and Google released cloud hosting services. While Amazon gave developers OS-level access with EC2, Google chose to present an application-level interface with App Engine. Google's decision may have been prescient given the up-the-stack direction of things in general, but proved too restrictive to be useful for most developers. It was the wrong bet. AWS experienced massive growth through the rest of the 2000s, while Google lacked a comparable product. In the early 2010s this began to change. Google Cloud Platform started out as a division of App Engine, offering Google Compute Engine, an EC2 competitor. Then App Engine and the newly formed Google Cloud Platform organizations eventually switched places in the corporate hierarchy, with App Engine now being considered a division GCP. Google now has a suite of products very comparable to whats available on Amazon. Here's a comparison of the wording on similar platform features between the two clouds: AWS (Amazon Web Services) GCP (Google Cloud Platform) EC2 (Elastic Compute Cloud) GCE (Google Compute Engine) SQS (Simple Queue Service) Cloud Pub/Sub S3 (Simple Storage Service) GCS (Google Cloud Storage) DynamoDB Cloud Datastore / BigTable RDS (Relational Database Service) / Aurora Cloud SQL The reason it is able to compete in the cloud marketplace is that Google has run at massive scale for its own operations for over a decade and has developed strong proprietary technology that it can now leverage for the public cloud. Operating an internal infrastructure is obviously different than operating a cloud, but this infrastructure means Google has a strong tailwind as it deploys cloud products. For example, Google probably has the best network (both internal and CDN) in the world, developed for its own operations, and now useful for routing cloud traffic. BigQuery is another example; it's a sophisticated analytical database used internally by Google, then deployed for public consumption. CPU If there will be an oligopoly of cloud vendors, then Google is one of the few companies with the scale and resources to legitimately compete. Even with an existing massive infrastructure, this is still an expensive market to compete in, and part of using GCP is convincing yourself that Google will invest 5+ years in really entering the market. We believe this to be true, given their investments in GCP engineering and leadership. Google Cloud CTO Urs Hölzle has said publicly that ""One day, this could be bigger than ads. Certainly, in terms of market potential, it is."" Diane Greene now leads the division that includes GCP - from our perspective it's nice to know that the cloud division has a seat on the Google Board of Directors. Dimensions of Comparison As noted above, AWS and GCP offer a number of comparable peripheral features. S3 and GCS are effectively the same product, as are SQS and Cloud Pub/Sub. Amazon RDS and Aurora are more advanced than Google Cloud SQL - but after testing we realized we'd need to manage our MySQL databases ourselves anyway. It became clear that the compute product (EC2/GCE) was the most important element of both products to us, since managing compute VMs is the biggest component in our infrastructure engineering time and overall cloud expenditure. So we focused most on comparing EC2 and GCE and sliced our comparison into the dimensions we cared most about; we narrowed down to the bread-and-butter platform features, like CPU, network speed, disk throughput, and how much these things cost. It's tough to fairly compare these things. Some of our measures are fairly simplistic - our intent was to poke at many factors of both platforms and get a sense of all of them. I'm including them to explain how we decided on GCP as our cloud, rather than as a definitive reference of cross-cloud performance. A cloud platform is a complex product, and if you're deciding among clouds I'd encourage you to test these for yourself. Note these tests were conducted in January and February 2015, but pricing has updated. CPU performance is basically identical between AWS and GCP. The biggest factor in performance is which processor generation you're running. At the time I'm writing this, both platforms are switching their machines from the Ivy Bridge generation to Haswell. AWS does this by releasing a new machine package - in this case all the machine types prefixed with a 4, such as c4.large, are Haswell-generation. GCP upgrades processor generation on the zone level, so certain zones in the us-central-1 region are Haswell. Both platforms employ virtualization technology that imposes overhead on CPU-intensive operations. During testing, we couldn't tell much difference in the effective performance, so our conclusion is that it's not worth thinking about. Memory It's worth noting that GCP is more extreme in the resources you get for compute-optimized instances. The 4-core AWS compute optimized option, a c4.xlarge, gives you 7.5 GB memory. The equivalent GCP package, the n1-highcpu-4, only has 3.6 GB of memory, and is priced about 32% cheaper. GCP offers the cheaper option for raw CPU performance. The x-axis is a simple benchmark of hashes generated per second. Processors on AWS and GCP perform about the same, so GCP's advantage in this space is a function of its pricing model. You see higher CPU capacity relative to price on GCP, in both normal and CPU-optimized instances. Networking The numbers above were calculated with the following benchmark: openssl speed -multi 24 blowfish . This counts how many hashes of varying sizes can be calculated in a given time period. Though thats oversimplifying the problem somewhat, I like this approach because it's transparent and easy, and we've observed that it's strongly correlated with our actual application performance. Like CPU, memory performance is equivalent on both platforms, so any advantage is in pricing model. Optimizing on memory, AWS has an advantage in terms of capacity relative to price. Chalk this up to particularly aggressive pricing on the r3 package, their memory-optimized option. The real highlight of GCP is the networking technology, which is similar to Amazon's VPC but much higher capacity for many machine types. Amazon optimizes certain machine types for 10 Gbit networking and provides very little in the way of documentation for networking capacity on their other instances. Google's documentation, on the other hand, explains that a node is allocated 2 Gbit/s per CPU core up to 10 Gbits/s, so that an n1-standard-4 machines has a ceiling of 8 Gbit/s. Both platforms let you run a vLAN with your own local address space and cap out at 10 Gbit/s for networking throughput for a single node. In practice, however, we observe much better networking performance for most of our machines on GCP. GCP does roughly 7x better for the comparison of 4-core machines, but for the largest machine sizes networking performance is roughly equivalent. This chart shows our observed results on an iperf performance test between nodes of the same type and same zone in AWS (with the appropriate network driver) and GCP. For this mid-size machine type we also observed much faster external-file transfer speeds. For some infrastructures, networking performance for machines of this size might not make as much difference. Quizlet runs a number of mid-size machines with significant network traffic, so for us it's very useful to have higher capacity without running oversized machines. For example, our load balancers don't have any CPU-intensive operations to perform but have significant network throughput, so GCP's network capacity allows us to run a smaller instance size and save money. Amazon's standard response on this question is that if you're concerned about networking performance then you should be using larger instance types. Here's some magic: Disk Performance $ traceroute google.com traceroute to google.com (74.125.193.139), 30 hops max, 60 byte packets 1 ig-in-f139.1e100.net (74.125.193.139) 0.865 ms 0.847 ms 0.838 ms Software-defined networking means that google.com appears to be one hop away, which probably isn't physically true. Google probably has the best networking technology on the planet. The internally-developed Jupiter Stack is exposed to public cloud customers through the cloud API, which we found simpler and more powerful than AWS's VPC configuration. VPC permissions to enable one machine access to another, for example, are notoriously tedious to manipulate. Another useful feature for us is that Google peers its egress traffic directly with Cloudflare, which Quizlet uses for CDN and DDoS protection. The peering deal has performance benefits and has significantly decreased the cost of our traffic egress. Disks are complex to compare directly, because the platforms offer such different options - but we prefer the GCP ones. Here's a comparison of the theoretical disk limits described for both platforms, considering only SSDs. Disk Snapshotting I've assumed 15,000 IOPS for the provisioned IOPS drive, which could go up to 20,000. Note also that the AWS vanilla SSDs can burst up to 3,000 additional IOPS. The i2.xlarge type above is a machine package, meaning it's price includes the entire machine cost, not just the disk cost. We spent time benchmarking magnetic and SSD drives on both platforms and found that the performance was in line with the published numbers. We haven't benchmarked Local SSDs. On GCP you pay slightly more for the vanilla SSD, but get far superior performance. Running a high-throughput database on AWS inevitably means a Provisioned IOPS drive or an optimized instance type, which drives the cost up. Comparing GCP's SSDs with AWS Provisioned IOPS yields a surprising price difference. GCP also offers Local SSD storage, which gives you the option to trade durability for power. These drives risk data loss if there is a problem on the instance, unlike the other SSD options. AWS i2 machines are optimized for disk throughput, but use ephemeral drives, meaning they also lack durability. AWS has other machine types optimized for workloads like sequential reading (useful for Hadoop jobs), that may improve the overall price you would pay for your application. Default Images The price/performance ratio of disks has a significant effect on our overall cloud spend. Our primary data store is comprised of 8 large MySQL machines serving several terabytes of production data. At any given time, we also run around 14 machines with high-throughput disk caches serving static assets. Given the data above, it's clear why we prefer Google disk technology. One other difference we noticed when examining disk performance was the speed of disk snapshotting on GCP. This feature copies a full copy of a disk at a moment in time, shipping it elsewhere for storage. It is particularly useful for backing up data, since it's a fairly lightweight operation. Both platforms enable SSD disk snapshots through their API and web console. Live Migration We tested snapshotting with 400 GB SSD drives given 300 GB of random data. On AWS, we observed snapshot times around 8 hours, while on GCP the same operation takes about 6 minutes. I'll spare you the chart. This is, like, a lot different. I asked a very patient intern to test this until we could figure out what we were doing wrong on AWS. It appears this is really a difference in core disk technology; GCP's disks are much better at snapshotting. Snapshotting is a pretty minor dimension in the overall cloud platform comparison but we're happy to have it as a feature, and it's enabled a much simpler architecture for our MySQL backups. Early in our process of picking a cloud, we made the decision to run CentOS 7 as our Linux distribution. We assumed running the distribution of our choice would be a solved problem on both platforms, but we ended up preferring GCE's system for packaging distributions. While AWS officially supports only its own Amazon Linux AMI and relies on community distributions for other flavors, Google made the decision to officially support a number of these themselves, including CentOS, Ubuntu, and CoreOS. We now run a recent CentOS 7 image configured by Google. Having officially supported images is nicer than you might think. Amazon Linux AMI comes preinstalled with the ixgbe network drivers, for example, which are required for optimal networking performance on AWS. The problem is that Amazon Linux AMI is a fork of CentOS 6, which is the only official option and is beginning to show its age - many newer packages are harder to install on it, and it uses Upstart to manage processes. Community images have filled the gap for newer distributions, but these aren't managed as well as an Amazon-managed version might be. For example, the community-managed CentOS 7 package wasn't yet available to run on Amazon's c4 package when we were testing it. You can always generate your own custom images, of course, but Google makes it painless to press a button and get the distribution you want. What it comes down to is - do you think you'll have fewer problems building your own Linux image, or using one configured and optimized by the cloud platform provider? Pricing Another key difference between EC2 and GCE is how they manage platform-level upgrades, such as improvements to the virtualization technology. While Amazon attempts to keep things as static as possible, Google has architected its cloud with the assumption it will be constantly making updates and upgrades. Doing this smoothly means that GCE needs the capability to transparently move a running VM between hardware nodes, a technology called live migration. This is somewhat mind boggling - it means moving running machine state, such as live memory pages and network connections, to a new host machine. The VM continues executing during this operation, except for a brief blackout period. This is important for several reasons. On EC2, if a node has a hardware problem, it will likely mean that you'll need to restart your virtual machine. On GCE, if a hardware problem is detected early enough, the platform will attempt to migrate your virtual machine to a new node without requiring any action from the customer, and without interrupting applications that are running on it. On EC2, if the virtualization technology needs to be updated, it will likely require a machine restart. There was a bad case of this in September 2014, when a bug was found in the Xen Virtualization technology used on EC2, which meant customers had to reboot a large number of EC2 VMs by a deadline. This kind of reboot storm basically isn't necessary on GCE. -- Our conclusion from comparing EC2 and GCE on technology was that networking and disk performance are better on GCP, and the other factors are pretty similar. We believe this is a reflection of Google's cloud development model. Most of GCP's technology was developed internally and has high standards of reliability and performance. It means that creating corresponding cloud products is a matter of exposing internal tools, rather than developing completely new ones. The outcome of that process seems to be that the user gets better performance. The GCP pricing model is much, much better than AWS. This shows the GCE sustained use pricing curve. Note that the discount is automatic. Source The AWS and GCP sticker prices are pretty similar. Both platforms offer an hourly price for instances and a long-term discount option. On AWS you get a discount by buying Reserved Instances, which gives you a ~35-45% discount for purchasing instances with a 1 or 3 year lock-in. On GCP you get a similar discount (to 1-year reserved instances) with Sustained-use Pricing, which lowers prices on the margin the longer you run a node. The difference here is the GCP discount is automatic, while the AWS discount requires a decision to commit to running a node for some time period, plus an up-front payment. We modelled our annual cloud spend on both platforms for several years into the future. After planning the ~200 instances we would run on each cloud and factoring in the above price discounts, plus our spend on network egress, disk allocations, support pricing, and the amount these would increase based on our traffic predictions, the final estimates were within 10% of one another (GCP was cheaper). This comparison examined GCP's sustained pricing models against AWS 1-year Reserved Instances. It's difficult to factor in all of the elements of a cloud into the pricing model - for example it's hard to say how much less we spend because we get better networking on mid-size GCP machines. The pricing on these two clouds is similar enough that price comparisons assuming 100% reserved instances with these parameters are basically a wash. Community and Momentum This estimate, however, doesn't really reflect real-world spend - it's virtually impossible to run reserved instances for 100% of your infrastructure. It's hard to quantify this, but I would guess that most companies operating on AWS reserve only reserve 50-80% of their instances. Buying a reserve involves a decision to commit to run a certain node type for at least a year, along with an upfront payment ($10,000+ for larger types), which means that it's a finance optimization and probably doesn't happen every day. So if you're running on AWS, you probably spend a lot on On-Demand instances even though there are large discounts to be had, which can add up to a difference of tens or hundreds of thousands of dollars. On GCP this is automatic. It's hard to overemphasize how much friendlier GCP pricing is, and how poorly the AWS model works. Amazon's rhetoric for why you should run cloud vs self-hosted infrastructure is the additional agility you get; You can elastically spin up and shut down capacity whenever you want. Reserved Instance pricing effectively negates this advantage, and running all your instances On-Demand is extremely expensive in comparison. So you're left with the decision to over-spend or lock yourself in. In practice, you'll probably run some machines reserved and some on-demand. 1 In summary - despite the similar list prices, you would likely spend significantly less on a GCP infrastructure than a comparable AWS infrastructure. By far the most significant advantage of Amazon is that it has the biggest base of users. This helps in a lot of ways: The sheer quantity of online content associated with platform; When you have a problem you can probably punch the issue into Google and see a Stack Overflow discussion or read a blog post about it. 3rd-party vendors deploy their products to the most popular cloud first. This means that some services may be unavailable, or less well maintained. New hires are more likely to be familiar with AWS than any other platform. How important is it to be on the most popular cloud? Part of the decision is weighing this factor against others. Another factor is gauging what the community will look like in the future. Though it's currently less feature-rich than AWS, Google has rapidly deployed new GCP features over the past few years and now has pace on releasing new products. Given its cadence, plus our (limited) understanding of its roadmap, we think GCP will grow more compelling relative to its competitors as time goes on. There are still many proprietary Google technologies that have yet to be productized for public consumption. Final Decision Everyone involved unanimously picked GCP. It came down to this: we believe the core technology is better. Though it has fewer users and services than AWS, we realized that bread and butter platform technology / performance is more important and we felt more confident in Google's core compute instance, networking, and disk technology. As a bonus: GCP's pricing is strictly better than AWS. It's certainly a bet to run our infrastructure on the less popular cloud, but it's hard to imagine performing our analysis and then not choosing the best technology and pricing. Given the technology and pricing on GCP, we expect its growth will accelerate, and it will acquire both individual developers and enterprise customers. Will it ever be bigger than AWS? Perhaps not, but our thesis is the overall market will be dominated by several huge clouds, and GCP will be one of them. Switching Over We switched Quizlet's infrastructure to run on GCP on 8/1/2015, before our back to school rush of traffic. Switching over to GCP was a large project which we'll cover in detail in a forthcoming blog post. Overall it was a smooth transition and we're very glad that we picked GCP as our provider - we've received excellent support and scaled up our deployment with few incidents. We're also looking forward to several unreleased GCP features that will arrive soon. This chart shows our weekly unique visitors since 2007. We'll continue scaling up our infrastructure as our traffic increases. Source Of the issues we've had with GCP, these are the ones worth mentioning: The account security model isn't as sophisticated yet as that on AWS. For example, many permissions are assigned on the machine level. This is a clunky system, because permissions there aren't very granular, and they can't be added or revoked after a machine is created. I suspect that we'll see a number of changes to this in the future. Past live migrations have caused application disruptions on a few of our instances that intensively use a lot of memory, such as our MySQL and Memcached VMs. Google has been continually optimizing the live migration technology and we've seen this issue mostly disappear. Footnotes It's a short list, and we're glad we switched to GCP. If the cloud market is still in a period of rapid expansion, and Google is one of a few companies able to compete at scale, and GCP is built on a substrate of superior technology, then our conclusion is that it will gain market-share and mind-share. We'll see small companies that start out on GCP grow into larger customers, and we'll see large customers switch onto it. If you're involved in one of those decision points, it's in your interest to evaluate the options for yourself. Ultimately it was a bet to move Quizlet to GCP - a significant one, given how deeply our product depends on cloud infrastructure. Switching over was a time consuming and complex process, but was the right decision for our business. We made that bet based on our belief in its core technology. The engineers who work on GCP also believe in its technology and we expect that Google will continue putting huge amounts of investment in its cloud infrastructure, as it always has. As a GCP product manager explained to us, ""Ferraris aren't cheap."" Whether you believe in GCP's technology or not, I think it's certainly worth evaluating if you're picking a cloud. 1: I would speculate that AWS uses its Reserved Instance pricing model for several reasons: upfront payments, stickiness, capacity planning. Amazon gets an upfront cash payment for a reserved instance which it can invest immediately in its own business. That's a nice funding model. Reserved Instances effectively lock customers in for some time period. You're much less likely to switch clouds if you've already payed for instances for a 3 year timeframe. RIs help with AWS capacity planning, since AWS knows for sure that a certain percentage of its capacity will be used stretching into the future. The tradeoff is they'll give you a discount if you can help them plan for the future. Google doesn't have that advantage, but I suspect gets around this by performing a simple statistical calculation based on the age of the instances running in its cloud. If a machine has been running for 2 years, for example, it's very likely to be running for at least another month. Google also runs very significant capacity for its own private infrastructure, which means that capacity planning for their cloud business could be less important than for Amazon. None of the logic behind Reserved Instances, other than the price, helps you as a customer.",3891637997717104548,2579,Facebook
268,3067875254349597654,JAPAN,Microsoft Adds Ethereum to Windows Platform For Over 3 Million Developers - CoinDesk,"Millions of Microsoft developers are now able to build decentralized applications using the Ethereum blockchain thanks to a collaboration between the software giant and Consensys, announced today. By building Ethereum's Solidity programming language for writing smart contracts directly into Mircosoft's Visual Studio platform, developers will be able to build, test and deploy decentralized applications, or dapps , within an integrated environment they already know how to use. But don't think this is a money play for either Consensys, or Microsoft, at least not yet. In a conversation with CoinDesk, CEO of Consensys, Joe Lubin said that growing the number of developers using the distributed ledger technology was first and foremost about reimagining social, economic and political systems. Lubin, who is also the co-founder of Ethereum told CoinDesk: ""It's in our best interest, and in our mission, to help with the adoption of Ethereum in all its different forms - public, private, consortium - and this just makes that that much easier."" The Consensys integration with Microsoft uses smart contract programming language Solidity to let developers run programs on the Ethereum network, also called the Ethereum Virtual Machine (EVM). Announced today at Microsoft's Build Conference in San Francisco, the new functionality was scheduled to go live this morning at 10am EST. With Ethereum's Solidity language, developers will be able to write applications using self-enforcing smart contracts that can theoretically execute as wide a range of business transactions as the coder can imagine. Ethereum's cryptocurrency, ether, powers the applications, and is currently valued at about $11.60, with a market cap of over $900m. By comparison, bitcoin's market cap is $6.3 billion. The EVM transactions - or smart contract executions as they may be more accurately called - can then be recorded on what Consensys described in a statement as a ""non-repudiable and authoritative"" record using either the public or private versions of the Ethereum blockchain. Ethereum's Room to Grow Until recently Ethereum adoption has been slow, even if it is accelerating. For example, the number of Ethereum repositories on GitHub, a reflection of how frequently developers are using the distributed ledger in their projects has grown at an exponential rate since January 2014, from less than 10 to about 460 by the end of 2015, according to analysis conducted by venture capital firm Kleiner Perkins Caufield & Byers. Using the same methodology, we calculated that number has more than doubled to 997 repositories, in the past three months alone. But when compared to bitcoin, where there are currently 7,647 bitcoin repositories in GitHub, there's plenty room to grow before Ethereum becomes a serious competitor. Marley Gray, director of business development and strategy for blockchain at Microsoft thinks his company's large pool of developers could change that. As of July 2015, when Microsoft launched its most recent version of Visual Studio more than 3.2 million developers were registered on the platform, and the gallery of tools had been downloaded 13 million times in the previous year. ""This integration with Visual Studio will offer developers enterprise-grade solutions with advanced capabilities for teams working on blockchain smart contract projects of any size and complexity,"" Gray said in a statement. While Consensys' CEO said he doesn't plan on making any money directly from the collaboration, he was clear in his conversation with CoinDesk that the effort to make it easier for developers to build dapps was no act of philanthropy. Lubin said: ""We at Consesys are a for-profit entity and we are building products on [the Ethereum] platform. We are building professional services and consulting on that platform, so it makes it so much easier for us to offer that kind of an environment to our customers and using that environment we can offer further services. Business of Ethereum Based in Brooklyn, New York, the self-funded Consensys is currently generating revenue from deals it has closed with institutional clients. Consensys declined to share the names of its clients. Currently, Consensys employs about 70 people around the world, 30 of which are based at their headquarters in New York City, Keys said. He estimates that 80% of the staff work on a full-time basis and are compensated through a combined equity and payment structure. ""Everybody's a stakeholder,"" he told CoinDesk. ""It just depends on when they started."" Consensys also isn't sharing its specific revenue numbers. But Keys said it has no intention of being profitable anytime soon. ""We just want to keep hiring people. We're at 70 now and if we find the right people we'll hire immediately,"" he said. For an idea of the growth rate the company expects, it plans to more than double the workforce by the end of 2016, employing 150 people on a full-time basis, Keys said. Symbiotic relationship Consensys' deal with Microsoft is a true collaboration, with neither party paying the other to be involved in any way, said Lubin. In fact, both companies have benefited elsewhere by working together. In November 2015, Consensys was the first company to join Microsoft's Azure cloud-computing service designed to let other companies experiment in a sandbox environment with the distributed ledger technology. As a result, Microsoft was able cut out its place in the blockchain industry as one of the largest early movers, and Consensys earned the credibility of partnering with a $427bn publicly traded company. The relationship, Lubin thinks, will continue thanks in no small part to today's news. Lubin described Visual Studio to CoinDesk: ""It's one of the best software developer tools on the planet, and this [collaboration] is like adding the vocabulary to build different kinds of programs to your development repertoire."" ConsenSys Ethereum Microsoft Microsoft Azure",4340306774493623681,1546,Facebook
190,-7356135999773525293,ERITREA,Fooling The Machine,"Two groups, one at Berkeley University and another at Georgetown University, have successfully developed algorithms that can issue speech commands for digital personal assistants, like Siri and Google Now, in the form of bursts of sound unrecognizable to human ears. To a human, these commands just sound like random white noise, but they could be used to tell a voice-activated assistant like Amazon's Alexa to do things that its owner never intended. Nicholas Carlini, one of the Byzantine audio researchers, says that their tests have been able to activate open source audio recognizers, Siri, and Google Now in their tests, with accuracy on all three more than 90 percent. The noise sounds like a science-fiction alien transmission. It's a garbled mix of white noise and human voice, but certainly unrecognizable as a command. With this attack, any phone that hears the noise (they have to specifically target iOS or Android) could be unknowingly forced to visit a webpage that plays the noise, and thus infect other phones near it, Carlini says. In that same scenario, the webpage could also silently download malware onto the device. There's also the possibility these noises could be played over the radio, hidden in white noise or background audio. These attacks can happen because the machine is trained to think that there's readable or important data in almost every input, and also that some things are more common than others, says Goodfellow. It's easier to fool the network into thinking it's seeing a common object, because it thinks that it should be seeing it more commonly. That's why Goodfellow and a separate group at University of Wyoming are able to make the network classify images when there's nothing there, by making it identify white noise, randomly generated black and white imagery. In Goodfellow's research, random white noise he put through the network was most often classified as a horse. This coincidentally brings us back to Clever Hans, our not so mathematically-gifted horse from earlier. Much like Clever Hans, Goodfellow says these neural networks aren't really learning certain ideas, but just how to recognize when they finds the right idea. The distinction is slight, but important. This lack of foundational knowledge makes it easy to maliciously recreate the experience of finding the ""right"" outcome for the algorithm, which is actually a false answer. To understand what is, the machine also must understand what is not. Goodfellow found that when he trained his image classifying networks with both the natural images, and the doctored images (specifying that they were fake), he not only could reduce the efficiency of the attack by more than 90 percent, but the network was better at its original task. ""When you start forcing them to explain the really unusual adversarial examples, it might come up with a even more robust explanation of what the underlying concepts are,"" Goodfellow says. The two audio groups have also used the same approach as the Google researchers to patch language recognition systems against their own attacks, by retraining their networks. They've achieved similar levels of success, with more than 90 percent reduction in attack efficiency. ""It's probably fairly easy for an adversary to fool us, to deceive us. Some of that may be benign, some of that may not be."" It's no surprise that this area of research has garnered interest from the United States military. In fact, the Army Research Laboratory actually sponsored at least two of the most recent papers, including the black box attack. While the Army Lab is proactive in funding research, this doesn't mean the tech is in active development for use in warfare. According to a spokesperson, research usually takes upwards of 10 years to make its way into soldier's hands. Ananthram Swami, a researcher with the U.S. Army Research Laboratory, has had varying levels of participation in recent papers concerning adversarial attacks. The Army's interest lies in the detection and stopping of purposefully deceptive data, in an age where not all sources of information can be vetted properly. Swami points to the bevy of data accessible from public sensors placed by universities and open source projects. ""We don't necessarily control all that data. It's probably fairly easy for an adversary to fool us, to deceive us,"" Swami said. ""Some of that may be benign, some of that may not be."" He also says that as the Army has a vested interest in autonomous robots, tanks, and other vehicles, so this research is obvious. By studying this now, the Army would have a headstart for systems in the field that were immune to potential adversarial attacks. But any group in that uses deep neural networks, which is a quickly growing faction, should have concerns about the potential of adversarial attacks. While machine learning and artificial intelligence systems are still in their infancy, we're at a dangerous time where security oversights can have drastic results. Many companies are placing highly volatile information in the hands of artificially intelligent systems, which have not endured the scrutiny of time. Our neural networks are simply too young for us to know everything about them. A similar oversight led to Tay, Microsoft's Twitter chatbot that quickly turned into a genocidal racist. A torrent of malicious data, and a foreseeably terrible ""repeat after me"" function, led Tay to deviate wildly from her original programming. The bot was hijacked by bad training data from the wild, and serves as a handy example for what can happen when machine learning is poorly implemented. Kantchelian says that he doesn't think the door is completely closed for any of these attacks, even with the promising research from the Google team. ""At least in computer security, unfortunately the attackers are always ahead of us,"" Kantchelian says. ""So it's going to be a little dangerous to say we solved all the problems of adversarial machine learning by retraining.""",-1443636648652872475,1577,Facebook
109,5619251370090681244,ETHIOPIA,Google Cloud Platform for AWS Professionals,"This guide is designed to equip professionals who are familiar with Amazon Web Services (AWS) with the key concepts required to get started with Google Cloud Platform. The guide compares and contrasts Google Cloud Platform with AWS and highlights the key differences between the two. In addition, the guide serves as a quick reference guide that maps AWS products, concepts, terminology and scenarios to corresponding offerings on Google Cloud Platform. If you are a developer already familiar with AWS offerings such as Amazon Elastic Compute Cloud, Amazon Simple Storage Service, Amazon VPC and Amazon RDS, this guide will help you apply that knowledge as you get started with Google Cloud Platform. This guide doesn't attempt to compare the syntax and the semantics of using the SDK, API or the command line tools. General overview Google Cloud Platform When Google first made Google App Engine public in 2008, we had actually been running a very large cloud infrastructure for years-internally. For the past 15 years, Google has been building the fastest, most powerful, highest quality cloud infrastructure on the planet. We use this infrastructure to bring customers services like Gmail , Maps , YouTube , and Search . The size and scale of these services means we have a lot of experience running cloud infrastructure. Over time, we've learned what works and what doesn't work. With Google Cloud Platform, we are taking the cloud we made for ourselves and exposing it for you to use. Currently, Google Cloud Platform consists of a variety of services. Together, these services provide a comprehensive platform for application development and hosting. Regions and zones Nearly all AWS products are deployed within regions located around the world. These regions are collections of data centers in relatively close proximity to each other. Within the regions, there are 2 or more Availability Zones. These regions are spread across the world, in the United States, Europe, Asia, and Australia. These concepts translate quite easily to Google Cloud Platform, as there are also regions, with zones , across the globe-also in the United States, Europe, and Asia. The Cloud Platform regions are located in Iowa in the United States, Belgium in Europe, and in Taiwan in Asia. Some Google Cloud Platform services are not necessarily always located in specific regions, but can also be located in a continental locations. These services include Google App Engine and Google Cloud Storage. Currently, the continental locations available are United States, Europe, and Asia. Each AWS Region is isolated and independent of each other. This is by design so that the availability of one region doesn't affect any of the others, and so that services within regions remain independent of each other. The regions for Google Cloud Platform are similarly distinct from each other for availability reasons, but have functions that enable them to synchronize data across regions for various services. Both AWS and Google Cloud Platform have points of presence (POPs) located in many more locations around the world. These POP locations help cache content closer to end users. However, each platform uses these POP locations for different reasons on each platform. AWS provides a distinct content delivery network (CDN) service, CloudFront, via these POP locations. Google Cloud Platform integrates POP locations in multiple services-for example, Google App Engine and Google Cloud Storage-to improve edge caching for all of these services. In addition, these POPs connect to our data centers through Google-owned fiber, which often means that Google Cloud Platform-based applications can have faster and more reliable access than AWS, which often goes over public internet. Mapping AWS terms and concepts to Google Cloud Platform: Accounts, limits, and pricing To use any service from AWS, you must sign up for an AWS account. After you have completed this relatively straightforward process, you can launch any service under your account, within the default limits. Those services bill to your account and your account only. It is possible for AWS to create billing accounts and sub-accounts that roll up to them and see which account bills what. In this way many organizations can emulate a billing structure within their organization. Google Cloud Platform also requires you to sign up and create an account with a process very similar to AWS. With Cloud Platform, you use your Google Account. This could be a Gmail account, an account registered with your corporate e-mail address, or a single sign-on enabled corporate account. Unlike AWS, Google Cloud Platform does not group the services you use by account. Instead, services are grouped by project , and you must have at least one project under your account to use any Cloud Platform services. After that, the process is similar to AWS, in that you can create any resource under that project within the default limits of the account. This structure lets you create multiple projects under the same account that are wholly separate from each other. This is advantageous when you need to create separate divisions or groups within your company. You can even create a project just for testing and then delete the project-all the resources created by that project will be deleted as well. Both AWS and Google Cloud Platform have default soft limits on their services for new accounts. Soft limits differ from hard limits. Hard limits on a service are technical limits that can't be changed for an individual customer. They are either limits that could be changed by feature releases or are limits that are part of the nature of the service and won't change. Soft limits, on the other hand, are sensible as they can help prevent fraudulent accounts from creating resources they won't pay for, allow for management of cost, and limit risk. Soft limits also prevent new users who aren't as familiar with the platform from accidentally spending more than they intended until they fully understand the platform. Both providers have straightforward mechanisms to get in touch with the appropriate internal teams to raise the limits on their services. Finally, a note on pricing. We want this documentation to serve as a long term tool for users to use as a resource when exploring the Google Cloud Platform. When new features and services are introduced, by either Google or Amazon, we try to update our documentation to reflect those changes as soon as possible. However, pricing tends to change more often than core features or services. As such, we're going to try to avoid pricing specifics. This allows the document to stay useful, without having anachronistic pricing. We will, however, discuss the pricing model behind services wherever useful. If you're interested in seeing a cost comparison for a specific solution, we encourage you to make use of the pricing calculators provided by both AWS and Google Cloud Platform , so you can see what configuration provides the best value in terms of flexibility, scalability, and cost. Command line interface and console Both AWS and Google Cloud Platform provide a command line interface (CLI) for controlling and querying all the resources of their respective platforms. They both provide a unified CLI for all services, rather than making users install separate command line libraries for each service. These CLIs are available on Windows, Linux, and Mac OS X. Not only can they be used from the command line to control resources in the cloud platforms, they can be used to create scripts and inside applications to automate control and status checking of these platforms. One feature that many developers find useful with Google Cloud Platform is that you can access the CLI commands from any system that has a web browser by using Google Cloud Shell . Both AWS and Google Cloud Platform have robust web user interfaces that allow for easy management of all their services and the resources created by those services. These user interfaces give users the ability to create and manage resources with their mouse and visualize what their infrastructure is doing. While developers and system administrators might prefer the command line interface to the console, a web UI is generally more accessible to more people. The console for Google Cloud Platform is located at . Building-block services In general, developers and system operators use a platform based on fundamental services with additional, higher level, services built on top of them: Nearly all cloud providers follow this general pattern, which is why, at a high level, AWS and Google Cloud Platform share similar characteristics. The fundamental services for AWS and Google Cloud Platform are: Many, if not all, of the higher level services provided on a cloud platform are built on top of these services. These higher level services usually consist of: Application services. These are services designed to help optimize applications in the cloud. Examples include AWS SNS and Google Cloud Pub/Sub. Data services. These are services that help process large amounts of data, such as Amazon Kinesis and Google Cloud Dataflow. Management services. These include services that help you track the performance of an application, such as Amazon CloudWatch and Google Cloud Monitoring. While nearly all customer workloads can run on just the fundamental services, most benefit from using these higher level ones. AWS building-block services The fundamental AWS products are: Amazon Elastic Compute Cloud for virtual compute Amazon Simple Storage Service and Amazon Elastic Block Store for storage Amazon VPC for networking Amazon RDS and Amazon DynamoDB for databases Higher-level building block services are built on these fundamental services. These can range from a platform as a service (PaaS), such as AWS Elastic Beanstalk, to more abstract services, such as Amazon Kinesis. Google Cloud Platform building-block services The fundamental Google Cloud Platform services are: Google Compute Engine and Google App Engine for virtual compute Google Cloud Storage, which allows you to store large or unstructured data objects Google Cloud DNS and Google Cloud Interconnect for networking basics Google Cloud SQL, Google Cloud Datastore, and Google Cloud Bigtable for databases. Like AWS, Google Cloud Platform layers a number of higher-level services on top of these fundamental ones. Both AWS and Google Cloud Platform build other services on top of these fundamental ones. Some of these services, such as AWS CloudFormation and Google Cloud Deployment Manager , are designed to make it easier to create large, repeatable deployments. However, there are also other services that doesn't have a direct comparison between AWS and Google Cloud Platform. The following table provides a side-by-side comparison of building block services, application services, data services and management services on AWS and Google Cloud Platform. We will go into detail on each of these sections below. Compute Compute services can be considered the foundation of cloud platforms. Developers can choose computing capacity in the form of virtual machines, use pre-configured, managed execution environment available as PaaS, or deploy containerized applications that run on a managed cluster. Infrastructure as a service (IaaS) Infrastructure as a Service (IaaS) is a fundamental building block of cloud services. More than just virtual compute, which has existed for quite some time, IaaS gives users on-demand access to flexible compute power. Not surprisingly, the IaaS services offered by AWS and Google Cloud Platform are fundamental to each platform and almost every type of customer workload uses them. The IaaS service in AWS is called Amazon Elastic Compute Cloud. On Cloud Platform, it is called Google Compute Engine . Below we will go into the differences between these two services. Mapping Amazon Elastic Compute Cloud terms and concepts to Compute Engine: Virtual machines Both Elastic Compute Cloud and Google Compute Engine refer to virtual machines either as instances or virtual machines. On both platforms, you can create instances from stored images. You can launch instances quickly and terminate them on demand. After you access your instance, you have complete control over it and can do whatever you want. Each platform supports a number of instance types and operating systems you can use and you can tag your instances as well. AWS and Cloud Platform do differ in how you access your instances. With AWS, you must include your own SSH key if you want terminal access to the instance. With Cloud Platform, you can create the key only when you need it-even if your instance is already running. Cloud Platform also offers a way to establish an SSH connection to your instance directly from the web console, so there are no keys stored on the local machine. Additionally, you can always address instances by name within your private network due to the default internal DNS. (There are some differences in firewalls and networks, but we'll cover those sections separately.) Instance types Both Elastic Compute Cloud and Google Compute Engine have a standard list of instance configurations with certain amounts of virtual CPU, RAM, and network assigned to them. These configurations are referred to as Instance Types in Elastic Compute Cloud and Machine Types in Google Compute Engine. Additionally, Google Compute Engine allows you to customize your instance type to fit the workload that you need to run. The following table lists the instance types for both services. Note that both Elastic Compute Cloud and Google Compute Engine add new instance types regularly, so this list is not definitive. Instead, we recommend you check these sites for the latest information on instance types. * While these machine types don't exist as such on Google Compute Engine, attaching SSD local storage to a different machine types can accomplish the same thing. Google Compute Engine and AWS share many of the same families of instance types, such as standard, high memory, high CPU, and shared core. While Google does not have a category of instances that use SSD storage, all of the non-shared core families support the addition of local SSD disks. These disks are 375 GB in size, and you can add 4 of them to an instance. The ability to add SSD disks to these instances results in both Google and AWS having comparable instance families for SSD storage. Google Compute Engine does lack two specialized families: large magnetic storage and GPUs. Operating system support Both Elastic Compute Cloud and Google Compute Engine provide support for a variety of operating systems . On both platforms, any operating system that has a license costs you an additional fee. Operating systems supported by both platforms include: CentOS CoreOS Debian FreeBSD openSUSE Red Hat Enterprise Linux (Premium) SELinux SUSE (Premium) Ubuntu Windows Server 2008 R2, 2012 (Premium) Operating systems supported only by Elastic Compute Cloud: Amazon Linux Windows Server 2003 (Premium) Oracle Linux (Premium) Virtual machine templates Creating a new instance is similar for both Elastic Compute Cloud and Google Compute Engine. In general, the steps are: Choose an instance type. Select an image, which determines what operating system the instance uses. On Elastic Compute Cloud, instance images are called Amazon Machine Images, or AMIs. On Google Compute Engine they're called images. Both Elastic Compute Cloud and Google Compute Engine are similar enough that you can employ the same workflow for image creation on either platform. For example, both Elastic Compute Cloud AMIs and Google Compute Engine Images contain at an operating system. They also can contain other software, such as Web servers or databases. Both AWS and Google Compute Engine allow you to use images published by third party vendors, such as Microsoft, or custom images created for private use. Elastic Compute Cloud and Google Compute Engine do differ in how they store images. AMIs, for example, are stored either in Simple Storage Service or Elastic Block Store. This leads to differences in startup times (images stored on Simple Storage Service take longer to launch) and whether or not the instance requires Elastic Block Store. On Google Compute Engine, all images are stored on Persistent Disk, which is the equivalent service to Elastic Block Store. Also, all Google Compute Engine instances use Persistent Disk for the root volume; there is no option for a local, or ephemeral, disk. It is possible to export images to Google Cloud Storage (an object store service equivalent to Simple Storage Service), but they are not stored there. Also, you currently cannot publish images publicly, although you can export them to Google Cloud Storage and share them. This means any images you launch are either images you created or images created by an authorized software vendor, like Microsoft. One other difference between AMIs and images concern availability. With AWS, AMIs are available only within a specific region, while Google Compute Engine Images are always global. Temporary virtual machines Both Elastic Compute Cloud and Google Compute Engine have specialized types of instances that you can create. These instances are cheaper than standard instances. However, you can also lose them with little notice. In Elastic Compute Cloud these are called Spot Instances, and in Google Compute Engine they are called Preemptible Virtual Machines. Applications use instances like these when they have tasks that can be interrupted. They are also useful when an application can benefit from increased compute power but doesn't necessarily need it. Examples of these tasks include batch processing, rendering, testing, and many more. While the application of Spot Instances and Preemptible Virtual Machines are similar, their implementation is quite different. Spot Instances have two models. The first type, regular Spot Instances, are priced on a market, the Spot Market, and launch when a bid is accepted. Users can bid any amount on an instance type, from $0.01 up to many times the on-demand price of the instance. If that bid is the current highest bid, then an instance (or instances) is created in Elastic Compute Cloud. The instance runs either until you terminate it, someone else outbids you, or Elastic Compute Cloud requires it for fleet needs. Aside from these rules about termination and price, Spot Instances behave similarly to it on-demand Elastic Compute Cloud instances. They support any AMI, any instance type, and you have total control over the instance while it's running. The second kind of Spot Instances, Spot Blocks, have a fixed priced that is less than the regular on-demand rate. However, they can only run for a maximum of 6 hours at that fixed rate. Preemptible Virtual Machines have a fixed price-there is no market. The price is always the 70% of the on-demand rate. Unlike regular Spot Instances, Preemptible Virtual Machines run for as long as 24 hours and then are terminated, but you can always terminate them sooner. One final difference is that any OS that also includes a license fee will charge the full cost of the license while using that Preemptible Virtual Machine. Aside from these three key differences, like Spot Instances, Preemptible Virtual Machines behave just as normal instances. Firewall Based on software-defined networking, cloud infrastructure services like Amazon Elastic Compute Cloud and Google Compute Engine offer configurable, programmable firewall policies. These policies protect virtual machines and networks by selectively allowing and denying traffic. By default, both services block all incoming traffic from outside a network and require an ingress firewall rule for packets to reach an instance. This means that, if you want to allow incoming network traffic, you need to set up firewalls that permit connections to your instances. Each firewall has at least a single rule that determines what traffic is permitted into the network. Amazon Elastic Compute Cloud and Amazon Virtual Private Cloud use the concept of Security Groups and Network Access Control Lists (NACLs) to allow or deny incoming and outgoing traffic. Elastic Compute Cloud Security Groups secure instances in Elastic Compute Cloud Classic, while Amazon Virtual Private Cloud Security Groups and NACLs are used to secure both Elastic Compute Cloud instances and network subnets in an Amazon Virtual Private Cloud. Google Compute Engine uses a flat list of firewall rules to secure Google Compute Engine virtual machines and networks. You create a rule by specifying the source IP address range, protocol, ports, and optional tags that represent source and target groups of virtual machines. However, Google Compute Engine firewall rules can't block outbound traffic. To do that, you can use a different kind of technology, such as iptables . Scale-out Auto scaling brings elasticity to cloud deployments. Both Elastic Compute Cloud and Google Compute Engine support auto scaling. Auto Scaling helps maintain a specific number of virtual machines at any given point, or adjust capacity in response to certain conditions. These instances launch from a predefined template. Users create policies to define when Auto Scaling or the Autoscaler should scale up or down. Amazon's Auto Scaling creates groups of instances called, intuitively, Groups. Scaling activity is triggered by a Scaling Plan , which uses a Launch Configuration to determine what to launch when it needs to scale out. These groups and this configuration are combined into one feature called a Managed Instance Group within Google Compute Engine's Autoscaler. Scaling activity for Autoscaler is triggered by an Autoscaling Policy . You can set Auto Scaling to scale in one of three ways: Manual, where someone dictates to Auto Scaling that it must scale immediately. Scheduled, when Auto Scaling scales capacity based on time. Dynamic, when Auto Scaling scales based on a policy. The dynamic scaling policies can be based on any CloudWatch alarm or an SQS queue. Autoscaler only supports dynamic scaling and supports three policy types: Average CPU utilization Cloud Monitoring metric HTTP load balancing serving capacity, which can be based on either utilization or requests per second. Local attached storage Both Elastic Compute Cloud and Google Compute Engine give users the option of using a disk that is local to the virtual machine, as opposed to being network attached. Because they are closer to the machine, these local disks offer much faster transfer rates. However, these local disks are not redundant like Elastic Block Store or Persistent Disk. On Elastic Compute Cloud, local disks are called Ephemeral or Instance Store. They can be either HDD or SSD, depending on the instance type family. The number of disks and the size of these disks of Instance Store depends on the Instance Type and is not adjustable. On Google Compute Engine, local disks are called Local SSD. As the name implies, they are SSD only. The amount of disk you can attach is independent of the instance type, with the caveat that the shared core instance types (currently f1-micro and g1-small) don't support Local SSD. Like Instance Store, Local SSD is significantly faster than network attached storage. The size of the disks is fixed at 375 GB per disk and you can only attach four disks at once. A key difference between Instance Store and Local SSD is that Local SSD does have a cost associated with it. Virtual machine import Both AWS and Google Cloud Platform provide ways for you to import existing virtual machine images created on other platforms to their platform. This allows you to leverage your investment in your on-premises servers and avoids the overhead of having to repeat work already done. Elastic Compute Cloud accomplishes this with a service called VM Import/Export . It supports a number of virtual machine image types (Raw, OVA, VMDK and VHD). It also supports a number of operating systems: a variety of versions of Windows, Red Hat Enterprise Linux, CentOS, Ubuntu, and Debian. The process for importing involves running the command line tool, which bundles the virtual machine image and uploads it to Simple Storage Service, where it is then created as an AMI. You can import virtual machine images into Google Compute Engine, but the process does not have a formal name like VM Import/Export. The current supported virtual machine image types are Raw, VirtualBox, and AMI. The actual import process is similar, though less automated than the AWS process. To import an image, you need to convert it and make sure certain packages and drivers are installed. While this is more effort than Import/Export, it does leave you with more flexibility in importing images to Google Compute Engine. After the image has been converted, it is uploaded to Google Cloud Storage, where it is imported as an Image. Like AWS, there is no cost for this service, aside from the cost to store an image in Google Cloud Storage. Pricing model Elastic Compute Cloud and Google Compute Engine offer very similar pricing models. Both services only charge you for instances for the length of time that you use them. With Amazon Elastic Compute Cloud, each instance type has a cost per hour, called the on-demand rate. If you run that instance type for an hour, you are billed that amount. With Google Compute Engine, you are charged by the minute, rather than the hour. Google charges you for the first 10 minutes, and then by the minute as long as it's running. With both services, you can run your instance for as long as you want without ever having to talk to someone at Amazon or Google. AWS and Google Cloud Platform do have significant differences when it comes to customer discounts. Elastic Compute Cloud has three pricing options. The first two, on-demand and Spot Instances, are discussed above. The third is called Reserved Instances (RIs). RIs work by asking you to commit to a certain number of instances for either a one- or three- year commitment. In exchange, you receive a lower cost for those instances. You can choose to pay none of the term up front, some of it, or all of it. The more you pay up front, the greater the discount. You also get a larger discount if you choose a three-year term over a one-year term. With RIs, you trade resource flexibility for a lower instance price. Also, RIs are are tied a specific instance type and availability zone at purchase. You can only move availability zones and exchange RIs with different instance types in the same family. Google Compute Engine discounting works quite differently. Google Compute Engine automatically applies discounting to instances when they run for a certain amount of time each month, meaning you don't need a multi-year commitment to get the same or better discount. This is called a Sustained Use Discount. The longer you use an instance in a given month, the greater the discount. Sustained Use Discounts can net you as much as 30% off the standard on-demand rate. We also provide a calculator to help customers estimate what their workloads might cost. To get a sense of how these pricing structures differ, check out our TCO calculator . Networking services Networking services provide connectivity across virtual machines, servers deployed on-premises, and other cloud services. Common networking services such as DNS, VPN, load balancers, firewall, and routing are available through APIs. Network administrators use the console, command line interface and API to configure these services. In AWS, while networking is generally treated as a separate service, it is very much tied to Elastic Compute Cloud. This is because, with few exceptions, most services are deployed on instances, just as Amazon Elastic Compute Cloud. Amazon Relational Database Service, Amazon Redshift, Amazon Elastic MapReduce, for example, all deploy on instances. That means these services need to be aware of networking, so it's not just a part of the compute service. This is an important difference to Google, as we will discuss below. AWS has two different networking stacks. The oldest, Elastic Compute Cloud-Classic was introduced with Elastic Compute Cloud. Elastic Compute Cloud-Classic launches all instance types in to a public, shared network, where every instance has access to the internet and is assigned a public IP address. Virtual Private Cloud, or VPC, was introduced to allow networking administration more common to traditional data centers. It allowed for creating private, RFC 1918 address spaces, and included sub networking, network access control lists (NACLs), inbound and outbound firewall rules, routing, and VPN. These two network stacks, Elastic Compute Cloud-Classic and Elastic Compute Cloud-VPC have existed in parallel since VPC was launched. When Elastic Compute Cloud-VPC first launched, it was optional. In late 2013, Elastic Compute Cloud-VPC became the default behavior for new accounts where VPC would be the default network stack. Elastic Compute Cloud-Classic remains on option only for older AWS accounts. The differences between AWS networking and Google Cloud Platform networking are significant. This due to the nature of how these services were designed. Google Cloud Platform treats networking as something that spans all services, not just compute services. It is based on Google's Andromeda software-defined networking architecture, which allows for creating networking elements at any level with software. As a result, Cloud Platform can create a network that fits Google's needs exactly-for example, create secure firewalls for virtual machines in Google Compute Engine, allow for fast connections between database nodes in Cloud Bigtable, or deliver query results quickly in BigQuery. To create an instance in Google Compute Engine, you need a network. In Google Cloud Platform, we create a default network for you automatically, and you can create more as needed. Unlike AWS, there is no choice of a public network like Elastic Compute Cloud-Classic. In all cases, you create a private network, much like Elastic Compute Cloud-VPC. Unlike Elastic Compute Cloud-VPC, Google Networking does not have sub-networking, but it does have firewall rules, routing, and VPN. These prerequisites are not necessarily required for all Google Cloud Platform services. Google BigQuery , for example, does not require a network because it is a managed service. Most of the networking entities in Google Cloud Platform, such as load balancers, firewall rules and routing tables, have global scope. More importantly, networks themselves have a global scope. This means that you can create a single private IP space that is global, without having to connect multiple private networks, with the operational overhead of having to manage those spaces separately. Due to this single, global network, all of your instances are addressable within your network by both IP address and name. Another major difference between Google Cloud Platform networking and Elastic Compute Cloud-VPC is the concept of Live Migration . Under normal circumstances, all hardware in any data center, including those that Google uses, will eventually need either maintenance or replacement. Unforeseen circumstances can also cause hardware to fail in any number of ways. When these events happen at Google, Cloud Platform has the ability to transparently move virtual machines from affected hardware to hardware that is working normally. This is done without any interaction from the customer. IP addresses Assigning IP addresses to virtual machines is a critical task. There are a few important differences in how this works between AWS and Google Cloud Platform, and some difference in terminology that you should know. Here's how AWS IP terms and concepts map to Google Cloud Platform: When you create an instance in Elastic Compute Cloud-Classic, your instance is given an external IP address that is only valid as long as that machine is running. This is referred to as an ephemeral IP. It is also given an internal network IP. At any point you can create and Elastic IP and assign it to the instance. This is much the same for Elastic Compute Cloud-VPC, except it is optional to have an external IP assigned to a new instance. In Google Cloud Platform, IP addresses work in a way that is similar to Elastic Compute Cloud-VPC. At launch, all instances have an internal IP. You can optionally request an external IP that only exists for as long as that instance is running. Additionally, you can request a permanent IP address to attach an instance. Like an Elastic IP, this IP address is yours until you choose to release it. One difference between AWS and Google is that you can take an ephemeral IP address and promote it to a static IP address, thereby attaching it to your account. Load balancing Load balancers distribute incoming traffic across multiple virtual machines. When configured appropriately, load balancers make applications fault tolerant and increase application availability. Here's how Elastic Load Balancer terms and concepts to map Google Compute Engine Load Balancer : The AWS Elastic Load Balancer (ELB) allows you to direct traffic to your instances within at least one and as many as all availability zones in a particular region. The ELB checks the health of the instances and, should any of them become unhealthy, stops sending traffic to that instance. It can also integrate with AWS's Auto Scaling service such that when instances are created or terminated by Auto Scaling, the Elastic Load Balancer is made aware of the change automatically. When an Elastic Load Balancer is created, you are given a CNAME which you can use to point traffic to. Provided that you are using Amazon's Route 53 , you can use Elastic Load Balancer as a root domain. Otherwise, you have to use a CNAME for the Elastic Load Balancer. The Google Compute Engine Load Balancer also directs traffic to back end instances in as many zones as you choose. However, there are a few important differences between how they work from this point on: Google Compute Engine lets you pick whether you need a Network (Layer 4) Load Balancer, which balances TCP traffic within a region, or an HTTP(S) (Layer 7) Load Balancer, which can can balance traffic globally. When you provision a Google Compute Engine Load Balancer, it returns a single, globally accessible IP address. This IP address can be used for the lifetime of the Load Balancer, so it can be used for DNS A Records, whitelisting, or configurations in applications. Scaling pattern Elastic Load Balancer scales up and down in response to traffic. The more traffic that goes through the Elastic Load Balancer, the more capacity it adds. The reverse is also true-the less traffic arrives, the more capacity the Elastic Load Balancer removes. The way Elastic Load Balancer changes capacity is either by changing the size of the load balancing resources (such as adding larger load balancers to meet increased load) or by changing the number of load balancing resources (such as reducing the number of load balancers when traffic goes down) as described here . Elastic Load Balancer does not scale instantly-it can take anywhere from one to seven minutes for the Elastic Load Balancer to respond to changes in traffic. If you expect a sudden spike in traffic, you must request that AWS pre-warm your Elastic Load Balancer to a certain traffic level. The Google Compute Engine Load Balancer also responds to traffic by scaling up or down the amount of capacity necessary to meet the traffic being passed through it. However, it responds in real time to the traffic, without a delay or pre-warming. Pricing model Both AWS and Google Cloud Platform load balancing services use the same pricing model. Each charges an hourly rate for the load balancer and a separate rate for the amount of traffic that passes through the load balancer. Peering A peering service allows customers to connect to a cloud service directly over a network. How this is done depends on the type of service. Here's how AWS peering terms and concepts map to Google Cloud Platform: Virtual private network Creating a virtual private network, or VPN, from one location to another allows you to create a secured, private link between two networks over the public internet. Both AWS and Google Cloud Platform offer this as a service. The process for creating a VPN is very similar. At a high level, VPN gateways at both ends create tunnels from their public IP address to one other public IP address, and establish a secure connection over it. Carrier peering There are circumstances when connecting to a cloud platform over a VPN doesn't provide the speed or security required by a particular workload. In such cases it is beneficial to have leased network line at guaranteed capacity level. Both Amazon and Google offer this service in conjunction with partners. At AWS, Direct Connect allows you to create a private leased line to AWS from a partner carrier facility. These facilities will allow you to connect a private line, at a certain capacity level, into a certain region. Each partner location services a specific region. At Google, Carrier Interconnect also allows you to create a private leased line into the Google Cloud Platform from a partner facility. These facilities allow you to connect a private line to a certain capacity level. The major difference is that Carrier Interconnect connects your traffic into the global Google Cloud Platform network, not a particular region. Direct peering Direct peering is similar to Carrier Peering in that you may want a private, dedicated line from your facility to the cloud. The difference is that you would be connecting directly to the cloud provider, not via a third party partner. Otherwise, the mechanism is quite similar. Amazon does not offer this service; Google does. Content delivery network (CDN) peering Content delivery network (CDN) peering is conceptually similar to carrier peering. In this case, instead of peering between your facility and a cloud provider, you are connecting between your resources in the cloud and a CDN. This is done from the edge locations of the cloud to the CDN. Google offers this service through CDN Interconnect . Amazon only offers this service through its own CDN service, Cloudfront. Pricing AWS and Google Cloud Platform charge for VPN services the same way, at an hourly rate. Pricing for the carrier peering services has two components. The first component is the pricing from the partner for the leased line. This is generally outside of the control of each platform. The second component is the pricing for the cloud services. Here, AWS and Google Cloud Platform handle pricing differently. AWS charges by the amount of capacity you have provisioned with your partner. For example, there's a charge for a 1G port speed, per hour, and more for 10G, and so on. Conversely, Google does not charge for this service. In addition, Google does not charge for direct peering. With Carrier Peering, you have two pricing components for CDN peering. The first pricing component is set by the partner. AWS also charges an additional amount based on the amount of capacity you have provisioned. As with carrier peering, Google does not charge for CDN Interconnect. DNS DNS translates human readable domain names into numeric IP addresses that servers use to connect with each other. Managed DNS services such as Amazon Route 53 and Google Cloud DNS offer scalable managed DNS service in the cloud. Here's how Amazon Route 53 features map to Google Cloud DNS: * Route 53 is a domain reseller for Gandi. ** You can use Google Domains to purchase domains from Google. The Domain Name System, DNS, has been around for nearly as long as the Internet has. It has a relatively simple feature set that allows many of the things that we do every day to work easily. With that in mind, there are not many complicated features to Route 53 or Cloud DNS, and the two are very close in terms of feature parity. Both Route 53 and Cloud DNS support nearly all records types, anycast-based serving, and connect to services that allow you to register domains. Amazon is a reseller of the registrar Gandi. Google has a separate registrar service, Google Domains . Neither service supports Domain Name System Security Extensions (DNSSEC) . Route 53 does support two kinds of routing that Cloud DNS does not: geography-based routing and latency-based routing. Geography-based routing lets you restrict your content to certain geographic regions of the world. Latency-based routing lets you direct traffic based on the latency measured by the DNS service. Pricing Both services price based on two similar parameters. First, the price on the number of zones hosted per month by the service. The second is for the number of queries per month. Route 53 charges a higher rate for either geographic based routing or latency based routing queries.",3891637997717104548,2304,LinkedIn
163,7420742904084384944,SOUTH AFRICA,Fooling The Machine,"Two groups, one at Berkeley University and another at Georgetown University, have successfully developed algorithms that can issue speech commands for digital personal assistants, like Siri and Google Now, in the form of bursts of sound unrecognizable to human ears. To a human, these commands just sound like random white noise, but they could be used to tell a voice-activated assistant like Amazon's Alexa to do things that its owner never intended. Nicholas Carlini, one of the Byzantine audio researchers, says that their tests have been able to activate open source audio recognizers, Siri, and Google Now in their tests, with accuracy on all three more than 90 percent. The noise sounds like a science-fiction alien transmission. It's a garbled mix of white noise and human voice, but certainly unrecognizable as a command. With this attack, any phone that hears the noise (they have to specifically target iOS or Android) could be unknowingly forced to visit a webpage that plays the noise, and thus infect other phones near it, Carlini says. In that same scenario, the webpage could also silently download malware onto the device. There's also the possibility these noises could be played over the radio, hidden in white noise or background audio. These attacks can happen because the machine is trained to think that there's readable or important data in almost every input, and also that some things are more common than others, says Goodfellow. It's easier to fool the network into thinking it's seeing a common object, because it thinks that it should be seeing it more commonly. That's why Goodfellow and a separate group at University of Wyoming are able to make the network classify images when there's nothing there, by making it identify white noise, randomly generated black and white imagery. In Goodfellow's research, random white noise he put through the network was most often classified as a horse. This coincidentally brings us back to Clever Hans, our not so mathematically-gifted horse from earlier. Much like Clever Hans, Goodfellow says these neural networks aren't really learning certain ideas, but just how to recognize when they finds the right idea. The distinction is slight, but important. This lack of foundational knowledge makes it easy to maliciously recreate the experience of finding the ""right"" outcome for the algorithm, which is actually a false answer. To understand what is, the machine also must understand what is not. Goodfellow found that when he trained his image classifying networks with both the natural images, and the doctored images (specifying that they were fake), he not only could reduce the efficiency of the attack by more than 90 percent, but the network was better at its original task. ""When you start forcing them to explain the really unusual adversarial examples, it might come up with a even more robust explanation of what the underlying concepts are,"" Goodfellow says. The two audio groups have also used the same approach as the Google researchers to patch language recognition systems against their own attacks, by retraining their networks. They've achieved similar levels of success, with more than 90 percent reduction in attack efficiency. ""It's probably fairly easy for an adversary to fool us, to deceive us. Some of that may be benign, some of that may not be."" It's no surprise that this area of research has garnered interest from the United States military. In fact, the Army Research Laboratory actually sponsored at least two of the most recent papers, including the black box attack. While the Army Lab is proactive in funding research, this doesn't mean the tech is in active development for use in warfare. According to a spokesperson, research usually takes upwards of 10 years to make its way into soldier's hands. Ananthram Swami, a researcher with the U.S. Army Research Laboratory, has had varying levels of participation in recent papers concerning adversarial attacks. The Army's interest lies in the detection and stopping of purposefully deceptive data, in an age where not all sources of information can be vetted properly. Swami points to the bevy of data accessible from public sensors placed by universities and open source projects. ""We don't necessarily control all that data. It's probably fairly easy for an adversary to fool us, to deceive us,"" Swami said. ""Some of that may be benign, some of that may not be."" He also says that as the Army has a vested interest in autonomous robots, tanks, and other vehicles, so this research is obvious. By studying this now, the Army would have a headstart for systems in the field that were immune to potential adversarial attacks. But any group in that uses deep neural networks, which is a quickly growing faction, should have concerns about the potential of adversarial attacks. While machine learning and artificial intelligence systems are still in their infancy, we're at a dangerous time where security oversights can have drastic results. Many companies are placing highly volatile information in the hands of artificially intelligent systems, which have not endured the scrutiny of time. Our neural networks are simply too young for us to know everything about them. A similar oversight led to Tay, Microsoft's Twitter chatbot that quickly turned into a genocidal racist. A torrent of malicious data, and a foreseeably terrible ""repeat after me"" function, led Tay to deviate wildly from her original programming. The bot was hijacked by bad training data from the wild, and serves as a handy example for what can happen when machine learning is poorly implemented. Kantchelian says that he doesn't think the door is completely closed for any of these attacks, even with the promising research from the Google team. ""At least in computer security, unfortunately the attackers are always ahead of us,"" Kantchelian says. ""So it's going to be a little dangerous to say we solved all the problems of adversarial machine learning by retraining.""",-1443636648652872475,1132,Facebook
228,7420742904084384944,ERITREA,Fooling The Machine,"Two groups, one at Berkeley University and another at Georgetown University, have successfully developed algorithms that can issue speech commands for digital personal assistants, like Siri and Google Now, in the form of bursts of sound unrecognizable to human ears. To a human, these commands just sound like random white noise, but they could be used to tell a voice-activated assistant like Amazon's Alexa to do things that its owner never intended. Nicholas Carlini, one of the Byzantine audio researchers, says that their tests have been able to activate open source audio recognizers, Siri, and Google Now in their tests, with accuracy on all three more than 90 percent. The noise sounds like a science-fiction alien transmission. It's a garbled mix of white noise and human voice, but certainly unrecognizable as a command. With this attack, any phone that hears the noise (they have to specifically target iOS or Android) could be unknowingly forced to visit a webpage that plays the noise, and thus infect other phones near it, Carlini says. In that same scenario, the webpage could also silently download malware onto the device. There's also the possibility these noises could be played over the radio, hidden in white noise or background audio. These attacks can happen because the machine is trained to think that there's readable or important data in almost every input, and also that some things are more common than others, says Goodfellow. It's easier to fool the network into thinking it's seeing a common object, because it thinks that it should be seeing it more commonly. That's why Goodfellow and a separate group at University of Wyoming are able to make the network classify images when there's nothing there, by making it identify white noise, randomly generated black and white imagery. In Goodfellow's research, random white noise he put through the network was most often classified as a horse. This coincidentally brings us back to Clever Hans, our not so mathematically-gifted horse from earlier. Much like Clever Hans, Goodfellow says these neural networks aren't really learning certain ideas, but just how to recognize when they finds the right idea. The distinction is slight, but important. This lack of foundational knowledge makes it easy to maliciously recreate the experience of finding the ""right"" outcome for the algorithm, which is actually a false answer. To understand what is, the machine also must understand what is not. Goodfellow found that when he trained his image classifying networks with both the natural images, and the doctored images (specifying that they were fake), he not only could reduce the efficiency of the attack by more than 90 percent, but the network was better at its original task. ""When you start forcing them to explain the really unusual adversarial examples, it might come up with a even more robust explanation of what the underlying concepts are,"" Goodfellow says. The two audio groups have also used the same approach as the Google researchers to patch language recognition systems against their own attacks, by retraining their networks. They've achieved similar levels of success, with more than 90 percent reduction in attack efficiency. ""It's probably fairly easy for an adversary to fool us, to deceive us. Some of that may be benign, some of that may not be."" It's no surprise that this area of research has garnered interest from the United States military. In fact, the Army Research Laboratory actually sponsored at least two of the most recent papers, including the black box attack. While the Army Lab is proactive in funding research, this doesn't mean the tech is in active development for use in warfare. According to a spokesperson, research usually takes upwards of 10 years to make its way into soldier's hands. Ananthram Swami, a researcher with the U.S. Army Research Laboratory, has had varying levels of participation in recent papers concerning adversarial attacks. The Army's interest lies in the detection and stopping of purposefully deceptive data, in an age where not all sources of information can be vetted properly. Swami points to the bevy of data accessible from public sensors placed by universities and open source projects. ""We don't necessarily control all that data. It's probably fairly easy for an adversary to fool us, to deceive us,"" Swami said. ""Some of that may be benign, some of that may not be."" He also says that as the Army has a vested interest in autonomous robots, tanks, and other vehicles, so this research is obvious. By studying this now, the Army would have a headstart for systems in the field that were immune to potential adversarial attacks. But any group in that uses deep neural networks, which is a quickly growing faction, should have concerns about the potential of adversarial attacks. While machine learning and artificial intelligence systems are still in their infancy, we're at a dangerous time where security oversights can have drastic results. Many companies are placing highly volatile information in the hands of artificially intelligent systems, which have not endured the scrutiny of time. Our neural networks are simply too young for us to know everything about them. A similar oversight led to Tay, Microsoft's Twitter chatbot that quickly turned into a genocidal racist. A torrent of malicious data, and a foreseeably terrible ""repeat after me"" function, led Tay to deviate wildly from her original programming. The bot was hijacked by bad training data from the wild, and serves as a handy example for what can happen when machine learning is poorly implemented. Kantchelian says that he doesn't think the door is completely closed for any of these attacks, even with the promising research from the Google team. ""At least in computer security, unfortunately the attackers are always ahead of us,"" Kantchelian says. ""So it's going to be a little dangerous to say we solved all the problems of adversarial machine learning by retraining.""",-1443636648652872475,1374,LinkedIn
299,8219310215587599928,AMERICA,Fooling The Machine,"Two groups, one at Berkeley University and another at Georgetown University, have successfully developed algorithms that can issue speech commands for digital personal assistants, like Siri and Google Now, in the form of bursts of sound unrecognizable to human ears. To a human, these commands just sound like random white noise, but they could be used to tell a voice-activated assistant like Amazon's Alexa to do things that its owner never intended. Nicholas Carlini, one of the Byzantine audio researchers, says that their tests have been able to activate open source audio recognizers, Siri, and Google Now in their tests, with accuracy on all three more than 90 percent. The noise sounds like a science-fiction alien transmission. It's a garbled mix of white noise and human voice, but certainly unrecognizable as a command. With this attack, any phone that hears the noise (they have to specifically target iOS or Android) could be unknowingly forced to visit a webpage that plays the noise, and thus infect other phones near it, Carlini says. In that same scenario, the webpage could also silently download malware onto the device. There's also the possibility these noises could be played over the radio, hidden in white noise or background audio. These attacks can happen because the machine is trained to think that there's readable or important data in almost every input, and also that some things are more common than others, says Goodfellow. It's easier to fool the network into thinking it's seeing a common object, because it thinks that it should be seeing it more commonly. That's why Goodfellow and a separate group at University of Wyoming are able to make the network classify images when there's nothing there, by making it identify white noise, randomly generated black and white imagery. In Goodfellow's research, random white noise he put through the network was most often classified as a horse. This coincidentally brings us back to Clever Hans, our not so mathematically-gifted horse from earlier. Much like Clever Hans, Goodfellow says these neural networks aren't really learning certain ideas, but just how to recognize when they finds the right idea. The distinction is slight, but important. This lack of foundational knowledge makes it easy to maliciously recreate the experience of finding the ""right"" outcome for the algorithm, which is actually a false answer. To understand what is, the machine also must understand what is not. Goodfellow found that when he trained his image classifying networks with both the natural images, and the doctored images (specifying that they were fake), he not only could reduce the efficiency of the attack by more than 90 percent, but the network was better at its original task. ""When you start forcing them to explain the really unusual adversarial examples, it might come up with a even more robust explanation of what the underlying concepts are,"" Goodfellow says. The two audio groups have also used the same approach as the Google researchers to patch language recognition systems against their own attacks, by retraining their networks. They've achieved similar levels of success, with more than 90 percent reduction in attack efficiency. ""It's probably fairly easy for an adversary to fool us, to deceive us. Some of that may be benign, some of that may not be."" It's no surprise that this area of research has garnered interest from the United States military. In fact, the Army Research Laboratory actually sponsored at least two of the most recent papers, including the black box attack. While the Army Lab is proactive in funding research, this doesn't mean the tech is in active development for use in warfare. According to a spokesperson, research usually takes upwards of 10 years to make its way into soldier's hands. Ananthram Swami, a researcher with the U.S. Army Research Laboratory, has had varying levels of participation in recent papers concerning adversarial attacks. The Army's interest lies in the detection and stopping of purposefully deceptive data, in an age where not all sources of information can be vetted properly. Swami points to the bevy of data accessible from public sensors placed by universities and open source projects. ""We don't necessarily control all that data. It's probably fairly easy for an adversary to fool us, to deceive us,"" Swami said. ""Some of that may be benign, some of that may not be."" He also says that as the Army has a vested interest in autonomous robots, tanks, and other vehicles, so this research is obvious. By studying this now, the Army would have a headstart for systems in the field that were immune to potential adversarial attacks. But any group in that uses deep neural networks, which is a quickly growing faction, should have concerns about the potential of adversarial attacks. While machine learning and artificial intelligence systems are still in their infancy, we're at a dangerous time where security oversights can have drastic results. Many companies are placing highly volatile information in the hands of artificially intelligent systems, which have not endured the scrutiny of time. Our neural networks are simply too young for us to know everything about them. A similar oversight led to Tay, Microsoft's Twitter chatbot that quickly turned into a genocidal racist. A torrent of malicious data, and a foreseeably terrible ""repeat after me"" function, led Tay to deviate wildly from her original programming. The bot was hijacked by bad training data from the wild, and serves as a handy example for what can happen when machine learning is poorly implemented. Kantchelian says that he doesn't think the door is completely closed for any of these attacks, even with the promising research from the Google team. ""At least in computer security, unfortunately the attackers are always ahead of us,"" Kantchelian says. ""So it's going to be a little dangerous to say we solved all the problems of adversarial machine learning by retraining.""",-1443636648652872475,2791,LinkedIn
256,7748594134538052724,JAPAN,Fooling The Machine,"Two groups, one at Berkeley University and another at Georgetown University, have successfully developed algorithms that can issue speech commands for digital personal assistants, like Siri and Google Now, in the form of bursts of sound unrecognizable to human ears. To a human, these commands just sound like random white noise, but they could be used to tell a voice-activated assistant like Amazon's Alexa to do things that its owner never intended. Nicholas Carlini, one of the Byzantine audio researchers, says that their tests have been able to activate open source audio recognizers, Siri, and Google Now in their tests, with accuracy on all three more than 90 percent. The noise sounds like a science-fiction alien transmission. It's a garbled mix of white noise and human voice, but certainly unrecognizable as a command. With this attack, any phone that hears the noise (they have to specifically target iOS or Android) could be unknowingly forced to visit a webpage that plays the noise, and thus infect other phones near it, Carlini says. In that same scenario, the webpage could also silently download malware onto the device. There's also the possibility these noises could be played over the radio, hidden in white noise or background audio. These attacks can happen because the machine is trained to think that there's readable or important data in almost every input, and also that some things are more common than others, says Goodfellow. It's easier to fool the network into thinking it's seeing a common object, because it thinks that it should be seeing it more commonly. That's why Goodfellow and a separate group at University of Wyoming are able to make the network classify images when there's nothing there, by making it identify white noise, randomly generated black and white imagery. In Goodfellow's research, random white noise he put through the network was most often classified as a horse. This coincidentally brings us back to Clever Hans, our not so mathematically-gifted horse from earlier. Much like Clever Hans, Goodfellow says these neural networks aren't really learning certain ideas, but just how to recognize when they finds the right idea. The distinction is slight, but important. This lack of foundational knowledge makes it easy to maliciously recreate the experience of finding the ""right"" outcome for the algorithm, which is actually a false answer. To understand what is, the machine also must understand what is not. Goodfellow found that when he trained his image classifying networks with both the natural images, and the doctored images (specifying that they were fake), he not only could reduce the efficiency of the attack by more than 90 percent, but the network was better at its original task. ""When you start forcing them to explain the really unusual adversarial examples, it might come up with a even more robust explanation of what the underlying concepts are,"" Goodfellow says. The two audio groups have also used the same approach as the Google researchers to patch language recognition systems against their own attacks, by retraining their networks. They've achieved similar levels of success, with more than 90 percent reduction in attack efficiency. ""It's probably fairly easy for an adversary to fool us, to deceive us. Some of that may be benign, some of that may not be."" It's no surprise that this area of research has garnered interest from the United States military. In fact, the Army Research Laboratory actually sponsored at least two of the most recent papers, including the black box attack. While the Army Lab is proactive in funding research, this doesn't mean the tech is in active development for use in warfare. According to a spokesperson, research usually takes upwards of 10 years to make its way into soldier's hands. Ananthram Swami, a researcher with the U.S. Army Research Laboratory, has had varying levels of participation in recent papers concerning adversarial attacks. The Army's interest lies in the detection and stopping of purposefully deceptive data, in an age where not all sources of information can be vetted properly. Swami points to the bevy of data accessible from public sensors placed by universities and open source projects. ""We don't necessarily control all that data. It's probably fairly easy for an adversary to fool us, to deceive us,"" Swami said. ""Some of that may be benign, some of that may not be."" He also says that as the Army has a vested interest in autonomous robots, tanks, and other vehicles, so this research is obvious. By studying this now, the Army would have a headstart for systems in the field that were immune to potential adversarial attacks. But any group in that uses deep neural networks, which is a quickly growing faction, should have concerns about the potential of adversarial attacks. While machine learning and artificial intelligence systems are still in their infancy, we're at a dangerous time where security oversights can have drastic results. Many companies are placing highly volatile information in the hands of artificially intelligent systems, which have not endured the scrutiny of time. Our neural networks are simply too young for us to know everything about them. A similar oversight led to Tay, Microsoft's Twitter chatbot that quickly turned into a genocidal racist. A torrent of malicious data, and a foreseeably terrible ""repeat after me"" function, led Tay to deviate wildly from her original programming. The bot was hijacked by bad training data from the wild, and serves as a handy example for what can happen when machine learning is poorly implemented. Kantchelian says that he doesn't think the door is completely closed for any of these attacks, even with the promising research from the Google team. ""At least in computer security, unfortunately the attackers are always ahead of us,"" Kantchelian says. ""So it's going to be a little dangerous to say we solved all the problems of adversarial machine learning by retraining.""",-1443636648652872475,1607,LinkedIn
116,8219310215587599928,GERMAN,Fooling The Machine,"Two groups, one at Berkeley University and another at Georgetown University, have successfully developed algorithms that can issue speech commands for digital personal assistants, like Siri and Google Now, in the form of bursts of sound unrecognizable to human ears. To a human, these commands just sound like random white noise, but they could be used to tell a voice-activated assistant like Amazon's Alexa to do things that its owner never intended. Nicholas Carlini, one of the Byzantine audio researchers, says that their tests have been able to activate open source audio recognizers, Siri, and Google Now in their tests, with accuracy on all three more than 90 percent. The noise sounds like a science-fiction alien transmission. It's a garbled mix of white noise and human voice, but certainly unrecognizable as a command. With this attack, any phone that hears the noise (they have to specifically target iOS or Android) could be unknowingly forced to visit a webpage that plays the noise, and thus infect other phones near it, Carlini says. In that same scenario, the webpage could also silently download malware onto the device. There's also the possibility these noises could be played over the radio, hidden in white noise or background audio. These attacks can happen because the machine is trained to think that there's readable or important data in almost every input, and also that some things are more common than others, says Goodfellow. It's easier to fool the network into thinking it's seeing a common object, because it thinks that it should be seeing it more commonly. That's why Goodfellow and a separate group at University of Wyoming are able to make the network classify images when there's nothing there, by making it identify white noise, randomly generated black and white imagery. In Goodfellow's research, random white noise he put through the network was most often classified as a horse. This coincidentally brings us back to Clever Hans, our not so mathematically-gifted horse from earlier. Much like Clever Hans, Goodfellow says these neural networks aren't really learning certain ideas, but just how to recognize when they finds the right idea. The distinction is slight, but important. This lack of foundational knowledge makes it easy to maliciously recreate the experience of finding the ""right"" outcome for the algorithm, which is actually a false answer. To understand what is, the machine also must understand what is not. Goodfellow found that when he trained his image classifying networks with both the natural images, and the doctored images (specifying that they were fake), he not only could reduce the efficiency of the attack by more than 90 percent, but the network was better at its original task. ""When you start forcing them to explain the really unusual adversarial examples, it might come up with a even more robust explanation of what the underlying concepts are,"" Goodfellow says. The two audio groups have also used the same approach as the Google researchers to patch language recognition systems against their own attacks, by retraining their networks. They've achieved similar levels of success, with more than 90 percent reduction in attack efficiency. ""It's probably fairly easy for an adversary to fool us, to deceive us. Some of that may be benign, some of that may not be."" It's no surprise that this area of research has garnered interest from the United States military. In fact, the Army Research Laboratory actually sponsored at least two of the most recent papers, including the black box attack. While the Army Lab is proactive in funding research, this doesn't mean the tech is in active development for use in warfare. According to a spokesperson, research usually takes upwards of 10 years to make its way into soldier's hands. Ananthram Swami, a researcher with the U.S. Army Research Laboratory, has had varying levels of participation in recent papers concerning adversarial attacks. The Army's interest lies in the detection and stopping of purposefully deceptive data, in an age where not all sources of information can be vetted properly. Swami points to the bevy of data accessible from public sensors placed by universities and open source projects. ""We don't necessarily control all that data. It's probably fairly easy for an adversary to fool us, to deceive us,"" Swami said. ""Some of that may be benign, some of that may not be."" He also says that as the Army has a vested interest in autonomous robots, tanks, and other vehicles, so this research is obvious. By studying this now, the Army would have a headstart for systems in the field that were immune to potential adversarial attacks. But any group in that uses deep neural networks, which is a quickly growing faction, should have concerns about the potential of adversarial attacks. While machine learning and artificial intelligence systems are still in their infancy, we're at a dangerous time where security oversights can have drastic results. Many companies are placing highly volatile information in the hands of artificially intelligent systems, which have not endured the scrutiny of time. Our neural networks are simply too young for us to know everything about them. A similar oversight led to Tay, Microsoft's Twitter chatbot that quickly turned into a genocidal racist. A torrent of malicious data, and a foreseeably terrible ""repeat after me"" function, led Tay to deviate wildly from her original programming. The bot was hijacked by bad training data from the wild, and serves as a handy example for what can happen when machine learning is poorly implemented. Kantchelian says that he doesn't think the door is completely closed for any of these attacks, even with the promising research from the Google team. ""At least in computer security, unfortunately the attackers are always ahead of us,"" Kantchelian says. ""So it's going to be a little dangerous to say we solved all the problems of adversarial machine learning by retraining.""",-1443636648652872475,1364,Google
169,-4186591400427142356,JAPAN,"Em parceria com Facebook, KLM passa a oferecer atendimento através do Messenger","Facebook e KLM anunciaram hoje uma parceria que promete mudar a forma como nos relacionamos com companhias aéreas. Voce sabe bem como é: muita burocracia, números pra decorar, sistemas com péssima usabilidade e design complicado em cada ponto de contato. Agora, após reservar um vôo no site da KLM, você pode optar por receber as informações através do Messenger do Facebook, tais como confirmações, notificação de check-in, status e bilhetes de embarque. Reunir as informações dessa maneira já é um facilitador, mas o mais importante diz respeito ao atendimento. Segundo a KLM, você pode pode entrar em contato 24 horas por dia através do Messenger, solicitando inclusive alterações no vôo, como remarcações e escolha de assento. A interação já está disponível em 13 idiomas, incluindo o português. Para promover a novidade, a KLM criou uma promoção que vai sortear duas passagens para qualquer destino entre as pessoas que testarem o serviço até o dia 27 de abril. Os detalhes estão no site messenger.klm.com",3609194402293569455,1863,Instagram
248,8565115980515859979,ERITREA,Calculating and searching 500 billion digits of Pi,"It's Pi day! Have you ever wondered whether you could take on the infinite irrationality of Pi and calculate its over one trillion digits? Well we wondered that exactly, and we found a way to calculate to 500 billion digits. Here's how: Google Compute Engine supports up to eight 375GB Local SSDs per virtual machine instance ! That gave us a total of 3TB of Local SSDs to use as swap space - exactly what we needed to calculate 500 billion digits, which is also exactly what we did. Once the machine was setup, we were able to calculate 500 billion digits of Pi in about 44.9 hours. Assuming you already have a Google Cloud Platform account, and gcloud command line tool installed, here's how you can setup the instance: First, let's set some variables to be used later. You'll need to pick a zone to replace the ZONE variable. export PROJECT=""YOUR_GCP_PROJECT"" export ZONE=""us-central1-c"" export CORES=""32"" export OUTPUT_DISK_SIZE=""500GB"" export OUTPUT_DISK=""out-${CORES}"" export INSTANCE=""yc-${CORES}"" Next, create a large disk to hold the final output (500 billion digits will take up 500GB!) $ gcloud compute disks create ${OUTPUT_DISK} \ --project ${PROJECT} \ --zone ${ZONE} \ --size ${OUTPUT_DISK_SIZE} \ --type pd-ssd Then, create a Compute Engine instance with Local SSDs, the following command will attach 8 Local SSDs: $ gcloud compute instances create ${INSTANCE} \ --project ${PROJECT} \ --zone ${ZONE} \ --machine-type n1-highmem-${CORES} \ --maintenance-policy TERMINATE \ --image-project gce-nvme \ --image nvme-backports-debian-7-wheezy-v20151104 \ --local-ssd interface=NVME \ --local-ssd interface=NVME \ --local-ssd interface=NVME \ --local-ssd interface=NVME \ --local-ssd interface=NVME \ --local-ssd interface=NVME \ --local-ssd interface=NVME \ --local-ssd interface=NVME \ --disk name=${OUTPUT_DISK},device-name=${OUTPUT_DISK} Once the instance is started, you can SSH into it with: $ gcloud compute ssh ${INSTANCE} -zone ${ZONE} Once you're in the newly created instance, you'll need to first format and mount all of the Local SSDs and the large persistent disk: $ sudo su - $ for i in `seq 0 7`; do \ mkdir /mnt/${i}; \ /usr/share/google/safe_format_and_mount \ /dev/disk/by-id/google-local-ssd-${i} /mnt/${i}; \ done $ mkdir /mnt/out $ /usr/share/google/safe_format_and_mount \ /dev/disk/by-id/google-${OUTPUT_DISK} \ /mnt/out Finally, install the latest version of y-cruncher onto that instance. You may want to install screen or tmux as well. To calculate digits of Pi, here is the command line I used to start y-cruncher on the virtual machine: $ export DIGITS=""500000000000"" $ ./y-cruncher custom pi -dec:${DIGITS} -hex:0 \ -o /mnt/out -mode:swap -swap:raid0 \ /mnt/0 /mnt/1 /mnt/2 /mnt/3 /mnt/4 /mnt/5 /mnt/6 /mnt/7 Searching Pi Digits (Preview!) y-cruncher certainly made it easy to calculate digits of Pi. For Pi Day 2016, Francesc Campoy , Jen Tong , Sara Robinson and I built a reverse lookup index in Google Cloud Bigtable , so that we can search a sequence of digits, such as your phone number and more! Here is a sneak preview of 2-million writes per second of Pi Digits into Cloud Bigtable: So that we can search a sequence of digits (up to 20 digits), and ask where in Pi does the first 9 digits of e appear? 6 occurrences! One in position 2,630,513,465! Or, is there a sequence of nine 9's ""999999999"": In position 4,329,769,635! This was a big leap from last year , when we calculated 100 billion digits using y-cruncher running on last year's n1-highmem-32 of Compute Engine instance, which had 32-cores and 208GB of RAM and four Local SSDs. Stayed tuned for more details on how we ingested billions of digits into Bigtable and how we built the Pi Search frontend. - Posted by Ray Tsang , Developer Advocate and Greg Wilson , Head of Developer Advocacy for Google Cloud Platform",3891637997717104548,1758,Instagram
148,8082202054464515448,GERMAN,"Com HoloLens, Microsoft torna realidade a mensagem holográfica de ""Star Wars""","As recentes demonstrações do HoloLens da Microsoft podem até colocar um dinossauro, um zumbi ou um unicórnio na sua sala de estar, mas existe aí uma aplicação prática muito mais nerd do que isso: tornar realidade a mensagem da Princesa Leia em ""Star Wars"" . Quero dizer, não a mensagem em si (ou teríamos todos que correr para lutar contra o Império), mas a tecnologia de projeção, que a Microsoft deu o nome de holoportation . Sim, no mundo real precisaremos de um óculos - coisa que Obi-Wan e Luke não precisaram - mas a miniaturização garante a diversão. Além de transmitir alguém em outro ambiente em tempo real para quem estiver usando o HoloLens, também é possível deixar trechos gravados, incluindo áudio, como se fossem registros de memória. A demonstração do vídeo acima explora a possibilidade de dois ambientes com o mesmo layout. Dessa forma, a interação entre as pessoas é como se elas estivessem no mesmo lugar. Shahram Izadi, pesquisador da Microsoft, convocou a própria filha pra testar o projeto. Ela, porém, não pareceu muito entusiasmada com a versão 3D do pai, já que a primeira pergunta foi: Quando você vai voltar pra casa? Custando 3 mil dólares, o Hololens está disponível em pré-venda para desenvolvedores de aplicativos nos EUA e no Canadá. A versão final do óculos deve ser lançada no segundo semestre de 2016.",-7531858294361854119,1369,Wikipedia
246,-5973686110278905367,GERMAN,The Internet of Things,"More objects are becoming embedded with sensors and gaining the ability to communicate. The resulting information networks promise to create new business models, improve business processes, and reduce costs and risks. In most organizations, information travels along familiar routes. Proprietary information is lodged in databases and analyzed in reports and then rises up the management chain. Information also originates externally-gathered from public sources, harvested from the Internet, or purchased from information suppliers. But the predictable pathways of information are changing: the physical world itself is becoming a type of information system. In what's called the Internet of Things , sensors and actuators embedded in physical objects-from roadways to pacemakers-are linked through wired and wireless networks, often using the same Internet Protocol (IP) that connects the Internet. These networks churn out huge volumes of data that flow to computers for analysis. When objects can both sense the environment and communicate, they become tools for understanding complexity and responding to it swiftly. What's revolutionary in all this is that these physical information systems are now beginning to be deployed, and some of them even work largely without human intervention. Pill-shaped microcameras already traverse the human digestive tract and send back thousands of images to pinpoint sources of illness. Precision farming equipment with wireless links to data collected from remote satellites and ground sensors can take into account crop conditions and adjust the way each individual part of a field is farmed-for instance, by spreading extra fertilizer on areas that need more nutrients. Billboards in Japan peer back at passersby, assessing how they fit consumer profiles, and instantly change displayed messages based on those assessments. Yes, there are traces of futurism in some of this and early warnings for companies too. Business models based on today's largely static information architectures face challenges as new ways of creating value arise. When a customer's buying preferences are sensed in real time at a specific location, dynamic pricing may increase the odds of a purchase. Knowing how often or intensively a product is used can create additional options-usage fees rather than outright sale, for example. Manufacturing processes studded with a multitude of sensors can be controlled more precisely, raising efficiency. And when operating environments are monitored continuously for hazards or when objects can take corrective action to avoid damage, risks and costs diminish. Companies that take advantage of these capabilities stand to gain against competitors that don't. Internet of Things The widespread adoption of the Internet of Things will take time, but the time line is advancing thanks to improvements in underlying technologies. Advances in wireless networking technology and the greater standardization of communications protocols make it possible to collect data from these sensors almost anywhere at any time. Ever-smaller silicon chips for this purpose are gaining new capabilities, while costs, following the pattern of Moore's Law, are falling. Massive increases in storage and computing power, some of it available via cloud computing, make number crunching possible at very large scale and at declining cost. None of this is news to technology companies and those on the frontier of adoption. But as these technologies mature, the range of corporate deployments will increase. Now is the time for executives across all industries to structure their thoughts about the potential impact and opportunities likely to emerge from the Internet of Things. We see six distinct types of emerging applications, which fall in two broad categories: first, information and analysis and, second, automation and control (exhibit). Information and analysis As the new networks link data from products, company assets, or the operating environment, they will generate better information and analysis, which can enhance decision making significantly. Some organizations are starting to deploy these applications in targeted areas, while more radical and demanding uses are still in the conceptual or experimental stages. 1. Tracking behavior When products are embedded with sensors, companies can track the movements of these products and even monitor interactions with them. Business models can be fine-tuned to take advantage of this behavioral data. Some insurance companies, for example, are offering to install location sensors in customers' cars. That allows these companies to base the price of policies on how a car is driven as well as where it travels. Pricing can be customized to the actual risks of operating a vehicle rather than based on proxies such as a driver's age, gender, or place of residence. Or consider the possibilities when sensors and network connections are embedded in a rental car: it can be leased for short time spans to registered members of a car service, rental centers become unnecessary, and each car's use can be optimized for higher revenues. Zipcar has pioneered this model, and more established car rental companies are starting to follow. In retailing, sensors that note shoppers' profile data (stored in their membership cards) can help close purchases by providing additional information or offering discounts at the point of sale. Market leaders such as Tesco are at the forefront of these uses. In the business-to-business marketplace, one well-known application of the Internet of Things involves using sensors to track RFID (radio-frequency identification) tags placed on products moving through supply chains, thus improving inventory management while reducing working capital and logistics costs. The range of possible uses for tracking is expanding. In the aviation industry, sensor technologies are spurring new business models. Manufacturers of jet engines retain ownership of their products while charging airlines for the amount of thrust used. Airplane manufacturers are building airframes with networked sensors that send continuous data on product wear and tear to their computers, allowing for proactive maintenance and reducing unplanned downtime. 2. Enhanced situational awareness Data from large numbers of sensors, deployed in infrastructure (such as roads and buildings) or to report on environmental conditions (including soil moisture, ocean currents, or weather), can give decision makers a heightened awareness of real-time events, particularly when the sensors are used with advanced display or visualization technologies. Security personnel, for instance, can use sensor networks that combine video, audio, and vibration detectors to spot unauthorized individuals who enter restricted areas. Some advanced security systems already use elements of these technologies, but more far-reaching applications are in the works as sensors become smaller and more powerful, and software systems more adept at analyzing and displaying captured information. Logistics managers for airlines and trucking lines already are tapping some early capabilities to get up-to-the-second knowledge of weather conditions, traffic patterns, and vehicle locations. In this way, these managers are increasing their ability to make constant routing adjustments that reduce congestion costs and increase a network's effective capacity. In another application, law-enforcement officers can get instantaneous data from sonic sensors that are able to pinpoint the location of gunfire. 3. Sensor-driven decision analytics The Internet of Things also can support longer-range, more complex human planning and decision making. The technology requirements-tremendous storage and computing resources linked with advanced software systems that generate a variety of graphical displays for analyzing data-rise accordingly. In the oil and gas industry, for instance, the next phase of exploration and development could rely on extensive sensor networks placed in the earth's crust to produce more accurate readings of the location, structure, and dimensions of potential fields than current data-driven methods allow. The payoff: lower development costs and improved oil flows. As for retailing, some companies are studying ways to gather and process data from thousands of shoppers as they journey through stores. Sensor readings and videos note how long they linger at individual displays and record what they ultimately buy. Simulations based on this data will help to increase revenues by optimizing retail layouts. In health care, sensors and data links offer possibilities for monitoring a patient's behavior and symptoms in real time and at relatively low cost, allowing physicians to better diagnose disease and prescribe tailored treatment regimens. Patients with chronic illnesses, for example, have been outfitted with sensors in a small number of health care trials currently under way, so that their conditions can be monitored continuously as they go about their daily activities. One such trial has enrolled patients with congestive heart failure. These patients are typically monitored only during periodic physician office visits for weight, blood pressure, and heart rate and rhythm. Sensors placed on the patient can now monitor many of these signs remotely and continuously, giving practitioners early warning of conditions that would otherwise lead to unplanned hospitalizations and expensive emergency care. Better management of congestive heart failure alone could reduce hospitalization and treatment costs by a billion dollars annually in the United States. Automation and control Making data the basis for automation and control means converting the data and analysis collected through the Internet of Things into instructions that feed back through the network to actuators that in turn modify processes. Closing the loop from data to automated applications can raise productivity, as systems that adjust automatically to complex situations make many human interventions unnecessary. Early adopters are ushering in relatively basic applications that provide a fairly immediate payoff. Advanced automated systems will be adopted by organizations as these technologies develop further. 1. Process optimization The Internet of Things is opening new frontiers for improving processes. Some industries, such as chemical production, are installing legions of sensors to bring much greater granularity to monitoring. These sensors feed data to computers, which in turn analyze them and then send signals to actuators that adjust processes-for example, by modifying ingredient mixtures, temperatures, or pressures. Sensors and actuators can also be used to change the position of a physical object as it moves down an assembly line, ensuring that it arrives at machine tools in an optimum position (small deviations in the position of work in process can jam or even damage machine tools). This improved instrumentation, multiplied hundreds of times during an entire process, allows for major reductions in waste, energy costs, and human intervention. In the pulp and paper industry, for example, the need for frequent manual temperature adjustments in lime kilns limits productivity gains. One company raised production 5 percent by using embedded temperature sensors whose data is used to automatically adjust a kiln flame's shape and intensity. Reducing temperature variance to near zero improved product quality and eliminated the need for frequent operator intervention. 2. Optimized resource consumption Networked sensors and automated feedback mechanisms can change usage patterns for scarce resources, including energy and water, often by enabling more dynamic pricing. Utilities such as Enel in Italy and Pacific Gas and Electric (PG&E) in the United States, for example, are deploying ""smart"" meters that provide residential and industrial customers with visual displays showing energy usage and the real-time costs of providing it. (The traditional residential fixed-price-per-kilowatt-hour billing masks the fact that the cost of producing energy varies substantially throughout the day.) Based on time-of-use pricing and better information residential consumers could shut down air conditioners or delay running dishwashers during peak times. Commercial customers can shift energy-intensive processes and production away from high-priced periods of peak energy demand to low-priced off-peak hours. Data centers, which are among the fastest-growing segments of global energy demand, are starting to adopt power-management techniques tied to information feedback. Power consumption is often half of a typical facility's total lifetime cost, but most managers lack a detailed view of energy consumption patterns. Getting such a view isn't easy, since the energy usage of servers spikes at various times, depending on workloads. Furthermore, many servers draw some power 24/7 but are used mostly at minimal capacity, since they are tied to specific operations. Manufacturers have developed sensors that monitor each server's power use, employing software that balances computing loads and eliminates the need for underused servers and storage devices. Greenfield data centers already are adopting such technologies, which could become standard features of data center infrastructure within a few years. 3. Complex autonomous systems The most demanding use of the Internet of Things involves the rapid, real-time sensing of unpredictable conditions and instantaneous responses guided by automated systems. This kind of machine decision making mimics human reactions, though at vastly enhanced performance levels. The automobile industry, for instance, is stepping up the development of systems that can detect imminent collisions and take evasive action. Certain basic applications, such as automatic braking systems, are available in high-end autos. The potential accident reduction savings flowing from wider deployment could surpass $100 billion annually. Some companies and research organizations are experimenting with a form of automotive autopilot for networked vehicles driven in coordinated patterns at highway speeds. This technology would reduce the number of ""phantom jams"" caused by small disturbances (such as suddenly illuminated brake lights) that cascade into traffic bottlenecks. Scientists in other industries are testing swarms of robots that maintain facilities or clean up toxic waste, and systems under study in the defense sector would coordinate the movements of groups of unmanned aircraft. While such autonomous systems will be challenging to develop and perfect, they promise major gains in safety, risk, and costs. These experiments could also spur fresh thinking about how to tackle tasks in inhospitable physical environments (such as deep water, wars, and contaminated areas) that are difficult or dangerous for humans. What comes next? The Internet of Things has great promise, yet business, policy, and technical challenges must be tackled before these systems are widely embraced. Early adopters will need to prove that the new sensor-driven business models create superior value. Industry groups and government regulators should study rules on data privacy and data security, particularly for uses that touch on sensitive consumer information. Legal liability frameworks for the bad decisions of automated systems will have to be established by governments, companies, and risk analysts, in consort with insurers. On the technology side, the cost of sensors and actuators must fall to levels that will spark widespread use. Networking technologies and the standards that support them must evolve to the point where data can flow freely among sensors, computers, and actuators. Software to aggregate and analyze data, as well as graphic display techniques, must improve to the point where huge volumes of data can be absorbed by human decision makers or synthesized to guide automated systems more appropriately. Within companies, big changes in information patterns will have implications for organizational structures, as well as for the way decisions are made, operations are managed, and processes are conceived. Product development, for example, will need to reflect far greater possibilities for capturing and analyzing information. Companies can begin taking steps now to position themselves for these changes by using the new technologies to optimize business processes in which traditional approaches have not brought satisfactory returns. Energy consumption efficiency and process optimization are good early targets. Experiments with the emerging technologies should be conducted in development labs and in small-scale pilot trials, and established companies can seek partnerships with innovative technology suppliers creating Internet-of-Things capabilities for target industries. About the Authors Michael Chui is a senior fellow with the McKinsey Global Institute, Markus Löffler is a principal in McKinsey's Stuttgart office, and Roger Roberts is a principal in the Silicon Valley office. The authors wish to thank their McKinsey colleagues Naveen Sastry, James Manyika, and Jacques Bughin for their substantial contributions to this article.",4670267857749552625,1938,Wikipedia
148,-9172673334835262304,ENGLAND,Tecnologia para colher mais e o uso do GPS,"Na fazenda em Maracaí (SP), a colheita já era mecanizada, mas agora também é automatizada. Tudo para evitar prejuízos e aumentar os ganhos dos agricultores. A redução de perdas foi de 10%, um índice que faz diferença no final da safra. ( Veja ao lado a reportagem exibida no Nosso Campo em 27/03/2016 ) O Nosso Campo constatou que uso de GPS permite colher tudo o que foi plantado. Algumas máquinas têm até piloto automático. No centro de controle, é possível saber tudo o que está acontecendo na lavoura em tempo real. Esse monitoramento é feito por satélite. Os gráficos mostram as condições da lavoura: onde está produzindo mais, onde está com baixa produtividade e a incidência de pragas. O Nosso Campo é exibido aos domingos, às 7h25, na TV TEM! Para participar, envie um e-mail para nossocampo@tvtem.com Acesse + Nosso Campo | Vídeos | Redes Sociais | Novidades TV TEM Antes dessa tecnologia, o combate de pragas era feito na plantação inteira. Agora é possível saber qual é a erva daninha e onde ela está atacando. E a aplicação de herbicidas é feita somente na área afetada. Como essa tecnologia toda ainda é carente de mão de obra especializada, muita gente está tentando se especializar. Em Pompéia (SP), existe um curso voltado para agricultura de precisão. É o único oferecido no País, com 160 alunos iniciando as aulas a cada ano.",-2626634673110551643,2475,Instagram
189,-1039912738963181810,SOUTH AFRICA,"Apple launches Safari Technology Preview, a browser for developers including experimental web features","Apple today announced a new browser, sort of. It's called Safari Technology Preview and its meant for developers to be able to test and experiment with future web technologies and upcoming features. This is similar to Google Chrome Canary, which allows developers to play with in-development web technologies ahead of official releases. The app even comes complete with a new purple icon. Via TheNextWeb , Apple will be updating Safari Technology Preview on a fortnightly basis, with each release signed for security. Download Apple's new browser from the developer portal . (page currently 404s) Safari Technology Preview is a standalone application, so you can install it alongside a normal version of Safari. It also supports iCloud features, which the typical WebKit Nightly builds do not include.",-709287718034731589,2784,Facebook
290,3172866488852888544,JAPAN,Google Is Finally Redesigning Its Biggest Cash Cow: AdWords,"Google made $74 billion last year. And it wasn't from painting weird dog montages or winning a few matches of Go . A vast majority of Google's revenue is from advertisements slipped into its free services like Search and YouTube. That's why AdWords is really Google's most important product, as it allows a million different businesses, big and small, to manage keyword-driven ad campaigns across Google. But for all its importance, AdWords hasn't seen a facelift in eight years. That changes today, as Google unrolls a new design to select AdWords customers, as part of a carefully paced, year-long makeover of the platform to conclude in 2017. The new AdWords will incorporate Material Design -Google's modern design language that's already being used across its other services-and it prioritizes clean graphical insights over lists of text and numbers. Updating AdWords will not only cement Google's design makeover across its most important services; it will also make its platform a whole lot more usable in a time when advertising campaigns have grown more complicated than ever. ""AdWords started with 350 customers 15 years ago . . . [then] it was rewritten 8 years ago as search advertising became really important for marketers. But it was built for a desktop search world,"" says Greg Rosenberg, head of UX, advertiser platform at Google. ""Today, we're in the middle of the biggest shift the ad industry has seen since AdWords launched-mobile."" Of course, it's not just mobile. The web has evolved to a multimodal, multimedia world, and advertising has followed. It's mobile, it's video, and it's even coordinating your AdWords to drive a shopping campaign that will also play out over several social media platforms. The last AdWords was built when most of us were still using flip phones. ""Our current product is just showing its age,"" Rosenberg says. Indeed, the last AdWords was built before the iPad was released, and when most of us were still using flip phones. Visitors are greeted at their spreadsheet-based campaign page with the list of words they've bought-with no way of telling if these words are meant to be working on campaigns together or how each is working across various platforms, unless they dive through all sorts of subpages. And a vague ""performance graph"" rendered in Google blue piles it all together into one tiny, vague chart. In turn, Google has met with hundreds of AdWords users around the world to research the new design. Most of its customers are actually small businesses, and upsetting them-much less the Fortune 500s-could put Google's primary source of income at risk. ""It goes far beyond polls or surveys. It's literally being at a business with our users, watching them use AdWords for hours on end,"" Rosenberg says. From there they moved to prototyping and testing in usability labs. ""Every tool in our arsenal has been behind this effort."" The resulting redesign is something Rosenberg admits will see a lot of iteration over the next year, but for now, it de-emphasizes the importance of words-both your list of AdWords and words used in the interface itself-to surface a series of graphs called Overviews right when you sign in. Overviews has one big line graph that still averages together every AdWords campaign you're running. Below that, a chart averages the performance of AdWords by each campaign. And to the right, a stacked bar chart gives you an immediate look at how your campaigns are playing out across desktops, tablets, and phones. ""Just because you spend a lot of money doesn't mean you're super-savvy."" Click into any individual campaign, and you'll be taken to another, more specific but similar, graphical overview page. These graphs are meant to surface insights formerly buried inside AdWords stats, and as an added bonus, they are language agnostic for a global audience of customers who may or may not actually understand anything about performance optimization in CPM-based advertising. ""That's an important myth to debunk,"" Rosenberg says. ""Just because you spend a lot of money doesn't mean you're super-savvy. We want these [insights] to pop out to people, and we don't want people to have to read the UI."" ""In some ways it's similar to the philosophy behind Chrome when it was created,"" he continues. ""The philosophy of Chrome was, there is no Chrome. It's all about the content."" And as cliché as that may sound, surfacing real insight-trimming away the fat to reveal the content-is probably the largest challenge in modern data visualization . That is why, while the new AdWords will be available to select users starting today, Google will roll out many more tweaks and features over the next year. Don't be surprised if a lot changes. Because while bold new design may be at the forefront of Google's aggressive Material Design strategy , Google still can't risk ostracizing its 1 million+ customers who actually keep the servers running for the rest of us-even if the new approach is, by any objective measure, so clearly better looking than before.",-1032019229384696495,1780,Wikipedia
239,8095004770374551394,JAPAN,Microsoft Build: the 10 most important announcements,"Here come the bots! Every year, Microsoft holds a developer event called ""Build."" And recently, those events have gone from snoozers to exciting showcases. Microsoft has a winner with Windows 10 (as long as you ignore the phones), a robust personal assistant in Cortana (that works just fine on a laptop), and a wild holographic future to plan with HoloLens. It's a lot to take in, and at this year's Build Microsoft we got updates on all of it. And a few surprises. Going in, we weren't totally sure what would be coming next for Windows 10, but it turns out there's a lot that Microsoft has planned. It's not just that there are new apps, there are also new bots , which will help people handle all sorts of small tasks. In fact, those bots and Microsoft's vision of how they should work stole the entire show. Windows, Xbox: you're cool, but the future is bots. Microsoft has really big software ambitions, and it laid them all out at Build. Here are the 10 most important things that were announced today. You can find links to everything that Microsoft announced in our StoryStream . The big idea from Microsoft today was making conversation the next platform for computing. At the heart of that plan are intelligent bots, something we've written about at length . These little AI bots will respond to your words and provide you assistance. Satya Nadella noted that Microsoft had erred with its launch of Tay, a social chatbot that users quickly taught to be a racist . ""We want to build technology so it gets the best of humanity, not the worst,"" said Nadella. ""We quickly realized it was not up to this mark. So we're back to the drawing board."" In the future, Nadalla says human language will be the UI layer, bots will be the new apps, and digital assistants will be the new ""meta apps."" Artificial intelligence will be infused into all of this. Cortana is the best example of an intelligent bot Microsoft has been developing for a while. But Microsoft also wants to get bots into some of its popular services, like Skype. Cortana is now hanging out inside Skype, waiting for queries and highlighting key bits of text. And third-party bots from restaurants or hotels can chat with Cortana, who mediates the conversation for you inside of Skype. Seriously, Microsoft waited until near the end of its keynote to talk about the most exciting and interesting stuff. It's making it way easier for Bots to talk to any chat app - from Skype to Slack to whatever. We also got a sneak peek at intelligent tools that will be coming to bots in the near future. Microsoft showed how to create a Domino's Pizza bot with Microsoft's tools, but it could work in multiple apps and supports natural language. This is all run off a Cortana Intelligence Suite, which runs of something called the Microsoft bot framework. It's a platform of cognitive services like computer vision and natural language that developers can use as they create smart bots to interact with Cortana or their customers. There are 22 APIs available starting today that cover years of machine learning and AI research. She's coming to the Xbox as your personal assistant, helping to find new titles and sharing tips and tricks on gameplay. But Cortana is also getting a huge update in the Anniversary Edition of Windows 10. Cortana will more intelligently understand time, for example. She can identify the PowerPoint you were working on last night, or the toy store you visited last year, and suggest calendar appointments based on emails and texts you receive. Cortana can also work with third party apps more deeply - developers can set up actions that do things with their app. You can have Cortana order food with Just Eat, for example. Microsoft keeps pushing Skype, and now it's making it a lot smarter. Basically, Skype will be able to talk to Cortana, which ""brokers"" a conversation with a third-party bot. Cortana is smarter than the bot (she understands context), but the bot is what does things like book a hotel room. During a Skype demo, a bot from Cups and Cakes asked for an address to complete a delivery. Cortana brokered the conversation and gave the user a map with updates on arrival time. When asked to add an appointment to a calendar, Cortana automatically engaged with a bot from the Weston hotel, filling in location and date, and allowed the user to book the hotel room with a few clicks. Finally, she suggested chatting with a friend who lived in the same area, and produced an automatic message in the mode of Google's smart Inbox replies. All this intelligence can even work in real time , inside a video chat. Skype users can just add ""video bots"" to their chat. We're not totally sure what that means, but it sounds wild. Microsoft is calling it the ""Anniversary update"" for Windows, and it has a bunch of new features will make Windows 10 better. It will support biometric authentication, and will even bring this feature into the browser. It works using a fingerprint sensor right next to the trackpad. Onstage, Microsoft logged into the USAA website using just a fingerprint. Windows talked about the ""Universal Windows Platform,"" a system it described as open and - most importantly - a platform that works across all different kinds of devices - from PCs to phones to HoloLens to wearables. And there are a ton of developers tools too: a Linux command line, a tool to help convert legacy Windows apps to the new platform, and much more. Finally, Microsoft spent a lot of time talking about the new features coming to Ink, as well - more on that below. To highlight its stylus Microsoft showed off a new ""Ink Workspace"" that features a bunch of pen-powered apps. When you write yourself a reminder with Stick Notes, Cortana can now recognize dates and set automatic reminders for tasks like ""Call mom tomorrow."" If you draw two points of interest on a map, it will tell you the distance and offer directions, and if you drag it across a section of text it will generate automatic highlights. Smart routes you trace on a map will even adapt if you shift to to a three dimensional view. Microsoft also showed off virtual stencils and rulers that let users more precisely shape what they draw on the screen. We knew that Facebook, Instagram, and Twitter were committed to ""Universal Windows Platform Apps,"" but we got a news that more are coming: Starbucks is the big one. But you can also expect Uber, Vine, Bank of America, WWE, and a ton of games. Microsoft showed off a few graphical flourishes developers can now add to Windows apps, including a motion blur and highlight effect you can activate with trackpad gestures. They also dove into some code that lets developers easily add Ink abilities into their apps. A virtual ruler was involved. Microsoft also showed off an ""Electronic Flight Bag,"" which is basically a Surface tablet that pilots can use to track all the information they need to fly. Boeing is going all in with it. But a bigger deal is that Microsoft is providing a desktop tool to developers so they can convert their current Windows apps to the new universal platform. Microsoft says it can work on 16 million current apps. The Bash shell is coming to Windows. Even if that sentence is gibberish to you, it's still meaningful. You can think of it as Windows supporting Linux - native Ubuntu binaries can run on Windows, and Microsoft partnered with Canonical to do it. Bash stand for ""Bourne Again Shell,"" and it means that developers can fire up a terminal window and code the way that many of them are used to: with Linux commands. Yes, it sounds nerdy, but it's a super big deal. Developers used to have to use third party tools to do it, now it's going to just be built in. The gaming portion of the keynote started out with an announcement that Forza is going to be created as a universal app going forward - so the same one will work on both Xbox One and a Windows PC. Quantum Break and Killer Instinct will also come to Windows 10. And that desktop app converter that we mentioned earlier? It will also work on classic Win32-based games. Phil Spencer showed that Microsoft could just download Age of Empires II HD from Steam and convert it to a universal app. And it even worked on The Witcher 3 . That means that all the Windows 10 features like Live Tiles and Notifications will work with these games. The Xbox One? It's also a development kit now. Developers can take any standard, retail Xbox One and use it to develop games and apps. A demo showed off a desktop app running on the Xbox, and the software automatically adapted to give a sense of how their app would work on the big screen in the living room. A preview of developer mode is available starting today. Last but certainly not least, Microsoft is finally going to create a unified store that works across both the PC and Xbox. Microsoft announced that the developer edition is shipping out today. It's expensive, so it's not really meant for consumers to buy, but it's still an important milestone. It was a big moment for the Hololens team, and the co-creators shared a hug onstage, whispering audibly, ""I love you man."" The first Hololens app shown on stage was Galaxy Explorer, which seemed to be holograms of planets and galaxies you can manipulate with gestures. Microsoft is releasing both the app and the source code today, giving Hololens developers something they can study and build from. And NASA showed off a Hololens called Onsight and Destination Mars, which lets you view the red planet through the perspective of the agency's scientists. Case Western Reserve University and The Cleveland Clinic showed off medical apps they built for Hololens. There was a holographic anatomy program that projected a life size body with transparent skin and the organs clearly visible inside. A professor patched in remotely from Cleveland as an avatar and walked students through lesson on a giant holographic brain. The avatar was a bit wonky, a frozen head, hand and screen name floating alongside the students onstage. But the professor was able to accurately point out small structural details inside the holographic brain as he worked through the lesson from across the country.",8341545449730058951,2294,Instagram
229,8801106199759296338,SOUTH AFRICA,Foxconn finalises Sharp takeover - BBC News,"Taiwanese manufacturer Foxconn says it has finally agreed a deal to take over struggling Japanese electronics company Sharp. Foxconn said the deal was worth 389bn yen ($3.5bn; £2.4bn) and would give it a 66% stake in Sharp. Foxconn, which assembles most of the world's iPhones, first offered to invest in the troubled Japanese firm in 2012, but talks collapsed. The two firms said the deal would be signed on 2 April. It would be the first foreign takeover of a major Japanese electronics firm. Describing themselves as ""world-class technology industry leaders"", Foxconn and Sharp said they would form ""a historic strategic alliance"". Pioneering firm Founded in 1912, Sharp is one of Japan's oldest technology firms. Japanese officials had been reluctant to let it fall under foreign ownership because of the distinctive technology behind its display panels. Before the announcement of a deal with Foxconn, Sharp had been discussing a rival offer from a government-backed consortium of Japanese investors. Although recent years have seen a downturn in its fortunes, the firm continued to be a leader in liquid display technology, a key asset for Foxconn. In 2012, Sharp came close to entering bankruptcy. It has struggled with heavy debts and has been through two major bailouts in the last four years. Sharp's innovations include a mechanical pencil in 1915 and pioneering developments in television engineering.",8341545449730058951,1821,Google
